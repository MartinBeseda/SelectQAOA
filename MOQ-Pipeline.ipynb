{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8df92c7f16f61fcc",
   "metadata": {},
   "source": [
    "# SelectQAOA: Regression Test Case Selection Using QAOAs\n",
    "Regression testing is an important part of the software development process in software engineering. It is a practice aimed at identifying any regression, which is the emergence of new defects or issues in a software application following changes, enhancements, or updates made to the source code. In other words, regression testing focuses on how changes made to the software can affect the correct behavior of existing features. Regression testing is particularly important in agile software development environments, where changes are made frequently and rapidly. This practice helps ensure that the software remains stable and reliable as it evolves over time. Ideal regression testing would re-run all the available test cases of a given software system. However, in addition to being potentially very costly, this could even be impractical in some case. In this scenario, test case selection is one of the most widely investigated test suite optimization approaches.\n",
    "Test case selection focuses on selecting a subset from an initial test suite to test software changes, i.e., to test whether unmodified parts of a program continue to work correctly after changes involving other parts. Furthermore, while in Test Case Minimization, not selecting a test case means permanently removing it; in Test Case Selection, the identified subset of the starting test suite is bounded to a specific execution of regression testing. Various techniques, such as Integer Programming, symbolic execution, data flow analysis, dependence graph-based methods, and flow graph-based approaches, can be employed to identify the modified portions of the software. Once test cases covering the unchanged program segments are pinpointed using a specific technique, an optimization algorithm (e.g., additional greedy, DIV-GA,\n",
    "SelectQA, BootQA or SelectQAOA) can select a minimal set of these test cases based on certain testing criteria (e.g., branch coverage). The ultimate aim is to reduce the expenses associated with regression testing.\n",
    "\n",
    "## Quantum Approximate Optimization Algorithm (QAOA)\n",
    "The Quantum Approximate Optimization Algorithm (QAOA) is a quantum algorithm designed to tackle combinatorial optimization problems by combining classical and quantum computing techniques. It encodes the optimization problem into a Quantum Unconstrained Binary Optimization (QUBO) format and constructs a parameterized quantum circuit that alternates between applying the problem Hamiltonian, which represents the optimization goal, and a mixing Hamiltonian, which promotes exploration of the solution space. The parameters of this circuit are optimized using classical optimization methods to maximize the likelihood of measuring the optimal solution. Once the parameters are tuned, the circuit is executed on a quantum device or simulator to get candidate solutions, making QAOA particularly suitable for NP-hard problems like graph partitioning and maximum cut. This hybrid approach highlights the potential of quantum computing to provide advantages in solving complex optimization challenges."
   ]
  },
  {
   "cell_type": "code",
   "id": "26cfee6323f1164",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:23:55.723432Z",
     "start_time": "2025-05-13T16:23:52.608241Z"
    }
   },
   "source": [
    "#this cell contains all the imports needed by the pipeline\n",
    "#to run it on the browser: jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import statistics\n",
    "import ast\n",
    "import csv\n",
    "from scipy.stats import mannwhitneyu, shapiro\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from qiskit_optimization import QuadraticProgram\n",
    "from qiskit_optimization.algorithms import MinimumEigenOptimizer\n",
    "from qiskit_algorithms.optimizers import COBYLA\n",
    "from qiskit.primitives import BackendSampler\n",
    "from qiskit_algorithms import QAOA\n",
    "from qiskit_algorithms.utils import algorithm_globals\n",
    "from qiskit_aer import Aer, AerSimulator\n",
    "from qiskit_aer.noise import NoiseModel, depolarizing_error\n",
    "from qiskit_ibm_runtime.fake_provider import FakeVigoV2\n",
    "from qiskit_optimization.converters import QuadraticProgramToQubo\n",
    "from qiskit.circuit.library import QAOAAnsatz\n",
    "\n",
    "from mitiq import zne\n",
    "from mitiq.zne.inference import RichardsonFactory\n",
    "from mitiq.zne.scaling import fold_global\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from collections import defaultdict"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "f36c35765771b1a0",
   "metadata": {},
   "source": [
    "## Multi-Objective SelectQAOA\n",
    "The first part of this work aims to compare SelectQAOA to the Multi-Objective DIV-GA, Additional Greedy, and SelectQA in terms of effectiveness and efficiency.\n",
    "So, we will first focus on the implementation of a Multi-Objective version of SelectQAOA.\n",
    "\n",
    "### The dataset\n",
    "To experiment the performance of the proposed solution by this work and to compare its results to those of the classical state-of-the-art solutions, four public programs have been downloaded from the SIR website. SIR is a repository of software-related artifacts meant to support rigorous controlled experimentation with program analysis and software testing techniques, and education in controlled experimentation. \n",
    "\n",
    "### Chosen SIR Programs\n",
    "The programs that will be used for experimentation have all been written in C and are:\n",
    "- flex (a program that generates a lexical analysis program, based on regular expressions and C statement contained in one or more input files);\n",
    "- grep (a useful program to search for matching patterns in a file);\n",
    "- gzip (a program that substitutes a file, generally text files or web pages, with their compressed version)\n",
    "- sed (a powerful program for stream text editing).\n",
    "\n",
    "### Necessary information\n",
    "The information needed by the quantum algorithm to work on every one of the four programs is:\n",
    "- a fault matrix: it indicates whether a precise test case already found, during previous execution, a bug in the source code or not;\n",
    "- execution cost: it indicates the execution cost of any test case of the suite;\n",
    "- statement coverage: it indicates statement coverage information for every test case. \n",
    "\n",
    "All this information has been gathered through previous experimentation on the four programs mentioned above and written in files organized in the SIR_Programs folder. So, the first goal of the project will be gathering data from these files for computational purposes."
   ]
  },
  {
   "cell_type": "code",
   "id": "7c35ee998a723ebe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:23:55.732799Z",
     "start_time": "2025-05-13T16:23:55.729932Z"
    }
   },
   "source": [
    "#this cell contains all variable definitions that will be useful throughout the entire project\n",
    "sir_programs = [\"flex\",\"grep\",\"gzip\",\"sed\"]\n",
    "sir_programs_tests_number = {\"flex\":567,\"grep\":806,\"gzip\":214,\"sed\":360}\n",
    "sir_programs_end_lines = {\"flex\":14192,\"grep\":13281,\"gzip\":6701,\"sed\":7118}\n",
    "sir_programs_rep_values = {\"flex\":1,\"grep\":16,\"gzip\":1,\"sed\":1}\n",
    "alpha = 0.5\n",
    "experiments = 10"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:23:56.397673Z",
     "start_time": "2025-05-13T16:23:55.775551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def json_keys_to_int(d):\n",
    "    \"\"\"This method correctly converts the data\"\"\"\n",
    "    if isinstance(d, dict):\n",
    "        return {int(k) if k.isdigit() else k: json_keys_to_int(v) for k, v in d.items()}\n",
    "    elif isinstance(d, list):\n",
    "        return [json_keys_to_int(i) for i in d]\n",
    "    else:\n",
    "        return d\n",
    "\n",
    "\n",
    "with open(\"datasets/sir_programs/executed_lines_test_by_test.json\", \"r\") as file:\n",
    "    #dictionary that, for each sir program, associates at each LINE of that program the LIST of TESTS COVERING it\n",
    "    executed_lines_test_by_test = json_keys_to_int(json.load(file)) #{program1:{line:[tci,tcj,...,tck],line2:...}\n",
    "with open(\"datasets/sir_programs/faults_dictionary.json\", \"r\") as file:\n",
    "    #dictionary that associates at each SIR PROGRAM the LIST of PAST FAULT COVERAGE VALUES ORDERED BY TEST \n",
    "    faults_dictionary = json.load(file) #{program1:[fault_cov_tc1,fault_cov_tc2,...,fault_cov_tcn],program2:...}\n",
    "with open(\"datasets/sir_programs/test_coverage_line_by_line.json\", \"r\") as file:\n",
    "    #dictionary that, for each sir program, associates at each TEST of that program the LIST of LINES COVERED by it\n",
    "    test_coverage_line_by_line = json_keys_to_int(json.load(file)) #{program1:{tc1:[linei,linej,...,linek],tc2:...}\n",
    "with open(\"datasets/sir_programs/test_cases_costs.json\", \"r\") as file:\n",
    "    #dictionary that, for each sir program, associates at each TEST its EXECUTION COST\n",
    "    test_cases_costs = json_keys_to_int(json.load(file)) #{program1:{tc1:ex_cost1,tc2:ex_cost2,...,tcn:ex_costn},program2:...}\n",
    "with open(\"datasets/sir_programs/total_program_lines.json\", \"r\") as file:\n",
    "    #dictionary which associates at each SIR PROGRAM its size in terms of the NUMBER OF ITS LINES\n",
    "    total_program_lines = json.load(file) #{program1:tot_lines_program1,program2:tot_lines_program2,program3:...}"
   ],
   "id": "7ef8cfdb01630b6b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Additional Greedy\n",
    "One of the classical strategies generally used to solve the test case selection problem is the \"Additional Greedy\" approach. This strategy is applied using the standard weighted sum approach to conflate all the objectives to optimize the weighted sum of code coverage per unit time and fault coverage per unit time. This way, a list of non dominated solutions is built incrementally."
   ],
   "id": "11645e30da1cf78d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:23:29.328736Z",
     "start_time": "2025-05-13T16:23:29.243680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def lines_to_cover(sir_program):\n",
    "    \"\"\"This function is needed to know which are the lines that have to be covered.\"\"\"\n",
    "    lines_to_cover = set()\n",
    "    \n",
    "    for covered_lines in test_coverage_line_by_line[sir_program].values():\n",
    "        for covered_line in covered_lines:\n",
    "            lines_to_cover.add(covered_line)\n",
    "            \n",
    "    return lines_to_cover\n",
    "\n",
    "def additional_greedy(sir_program):\n",
    "    \"\"\"This functions implements the Additional Greedy algorithm. It incrementally builds a set of non dominated solutions, choosing each time the test case that minimizes the function described earlier.\"\"\"\n",
    "    c = set()\n",
    "    p = lines_to_cover(sir_program)\n",
    "    s = []\n",
    "    pareto_front = []\n",
    "    test_cases_already_selected = []\n",
    "    while len(c) < len(p):\n",
    "        weighted_sum = []\n",
    "        for test_case in range(0,sir_programs_tests_number[sir_program]):\n",
    "            if test_case not in test_cases_already_selected and test_case in test_coverage_line_by_line[sir_program].keys():\n",
    "                max_cost = max(test_cases_costs[sir_program].values())\n",
    "                added_coverage = [line for line in test_coverage_line_by_line[sir_program][test_case] if line not in c]\n",
    "                try:\n",
    "                    weighted_sum[test_case]=((0.5 * (len(added_coverage)/len(test_coverage_line_by_line[sir_program][test_case]))) + (0.5 * faults_dictionary[sir_program][test_case]))/(test_cases_costs[sir_program][test_case]/max_cost)\n",
    "                except:\n",
    "                    pass\n",
    "        # take the test case that minimizes the function\n",
    "        best_test_case = min(weighted_sum,key=weighted_sum.get)\n",
    "        test_cases_already_selected.append(best_test_case)\n",
    "        for covered_line in test_coverage_line_by_line[sir_program][best_test_case]:\n",
    "            # update the count of covered lines\n",
    "            c.add(covered_line)\n",
    "        s.append(best_test_case)\n",
    "        # incremental building of the pareto frontier\n",
    "        pareto_front.append(s.copy())\n",
    "    \n",
    "    return pareto_front\n",
    "\n",
    "# running the greedy algorithm and saving the resulting solutions with execution times into .json files \n",
    "for sir_program in sir_programs:\n",
    "    with open(f\"results/add-greedy/{sir_program}_data.json\", \"w\") as file:\n",
    "        json_data = []\n",
    "        start = time.time()\n",
    "        json_data[\"pareto_front\"] = additional_greedy(sir_program)\n",
    "        end = time.time()\n",
    "        json_data[\"resolution_time(ms)\"] = (end-start)*1000\n",
    "        json.dump(json_data,file)\n"
   ],
   "id": "56c92cfcbe1041df",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 45\u001B[0m\n\u001B[1;32m     43\u001B[0m json_data \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     44\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m---> 45\u001B[0m json_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpareto_front\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m additional_greedy(sir_program)\n\u001B[1;32m     46\u001B[0m end \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m     47\u001B[0m json_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresolution_time(ms)\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m (end\u001B[38;5;241m-\u001B[39mstart)\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m1000\u001B[39m\n",
      "Cell \u001B[0;32mIn[13], line 29\u001B[0m, in \u001B[0;36madditional_greedy\u001B[0;34m(sir_program)\u001B[0m\n\u001B[1;32m     27\u001B[0m             \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# take the test case that minimizes the function\u001B[39;00m\n\u001B[0;32m---> 29\u001B[0m best_test_case \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m(weighted_sum,key\u001B[38;5;241m=\u001B[39mweighted_sum\u001B[38;5;241m.\u001B[39mget)\n\u001B[1;32m     30\u001B[0m test_cases_already_selected\u001B[38;5;241m.\u001B[39mappend(best_test_case)\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m covered_line \u001B[38;5;129;01min\u001B[39;00m test_coverage_line_by_line[sir_program][best_test_case]:\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;66;03m# update the count of covered lines\u001B[39;00m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'list' object has no attribute 'get'"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "ec65b50547d46586",
   "metadata": {},
   "source": [
    "## Problem Decomposition with Clustering\n",
    "The qubit availability is the most significant limitation of the currently available quantum computers.\n",
    "Therefore, the test suites to optimize are decomposed into smaller sub-suites solvable by QAOA simulators using clustering techniques. Applying clustering allows for preserving the similarities between test cases in sub-suites, and since similar test cases tend to have redundant coverage, the optimization process will facilitate useless duplications. Also, the similarities between test cases of the same clusters imply a diversified representation of the initial suite, facilitating targeted balancing of the goal while building a diversified final sub-suite.\n",
    "\n",
    "## QUBO Problems\n",
    "A Quadratic Unconstrained Binary Optimization (QUBO) problem is a type of mathematical problem where we seek to find the best combination of binary values (0 or 1) for a set of variables to minimize or maximize an objective function. In other words, we are looking for the optimal solution among all possible variable combinations that satisfies certain constraints and makes the objective function as small as possible.\n",
    "\n",
    "Weights for constraints (or penalty coefficients) are used in QUBO problems to assign a numerical value to the constraints and influence the optimization process. These weights are important because they allow for managing the priority and relative importance of constraints within the optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "id": "9224a4794535ffef",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-05-13T16:24:09.089645Z",
     "start_time": "2025-05-13T16:24:09.060810Z"
    }
   },
   "source": [
    "def num_of_covered_lines(sir_program,test_cases):\n",
    "    \"\"\"This method returns the number of covered lines (no redundancy)\"\"\"\n",
    "    covered_lines = set()\n",
    "    \n",
    "    for test_case in test_cases:\n",
    "        try:\n",
    "            for covered_line in test_coverage_line_by_line[sir_program][test_case]:\n",
    "                covered_lines.add(covered_line)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return len(covered_lines)\n",
    "\n",
    "clusters_dictionary = dict()\n",
    "\n",
    "for sir_program in sir_programs:\n",
    "    tot_test_cases = sir_programs_tests_number[sir_program]\n",
    "    \n",
    "    # from {..., test_case_i : [cov_stmts], ...} to [..., #_stmt_cov_i, ...]\n",
    "    test_cases_stmt_cov = []\n",
    "    for test_case in test_coverage_line_by_line[sir_program].keys():\n",
    "        test_cases_stmt_cov.append(len(test_coverage_line_by_line[sir_program][test_case]))\n",
    "    suite_stmt_cov = sum(test_cases_stmt_cov)\n",
    "    \n",
    "    # Normalize data\n",
    "    data = np.column_stack((list(test_cases_costs[sir_program].values()),faults_dictionary[sir_program],test_cases_stmt_cov))\n",
    "    scaler = StandardScaler()\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "    num_clusters = 50\n",
    "        \n",
    "    max_cluster_dim = 5\n",
    "    \n",
    "    # Step 2: Perform K-Means Clustering\n",
    "    start = time.time()\n",
    "    linkage_matrix = linkage(normalized_data, method='ward')\n",
    "    clusters = fcluster(linkage_matrix, t=num_clusters, criterion='maxclust')\n",
    "    \n",
    "    # Organize test cases by cluster\n",
    "    clustered_data = defaultdict(list)\n",
    "    for idx, cluster_id in enumerate(clusters):\n",
    "        clustered_data[cluster_id].append(idx)\n",
    "    \n",
    "    # Process clusters to ensure none exceed max_cluster_dim\n",
    "    new_cluster_id = max(clustered_data.keys()) + 1  # Start new IDs after existing ones\n",
    "    to_add = []  # Collect new smaller clusters\n",
    "    \n",
    "    for cluster_id, elements in list(clustered_data.items()):  # Avoid modifying dict during iteration\n",
    "        if len(elements) > max_cluster_dim:\n",
    "            num_splits = -(-len(elements) // max_cluster_dim)  # Ceiling division to get the required number of splits\n",
    "            split_size = -(-len(elements) // num_splits)  # Recalculate to distribute elements evenly\n",
    "            \n",
    "            # Split while keeping sizes balanced\n",
    "            parts = [elements[i:i + split_size] for i in range(0, len(elements), split_size)]\n",
    "    \n",
    "            # Ensure all new clusters are within max_cluster_dim\n",
    "            for part in parts:\n",
    "                if len(part) > max_cluster_dim:\n",
    "                    raise ValueError(f\"A split cluster still exceeds max_cluster_dim ({len(part)} > {max_cluster_dim})!\")\n",
    "    \n",
    "            # Add new parts to the new clusters\n",
    "            to_add.extend(parts)\n",
    "    \n",
    "            # Remove original large cluster\n",
    "            del clustered_data[cluster_id]\n",
    "    \n",
    "    # Assign new IDs to split parts\n",
    "    for part in to_add:\n",
    "        if part:  # Only add if the part is non-empty\n",
    "            clustered_data[new_cluster_id] = part\n",
    "            new_cluster_id += 1\n",
    "    end = time.time()\n",
    "    print(\"SelectQAOA Decomposition Time(ms): \" + str((end-start)*1000))\n",
    "    \n",
    "    clusters_dictionary[sir_program] = clustered_data\n",
    "        \n",
    "    # Step 3: Calculate the metrics for each cluster and validate\n",
    "    cluster_metrics = {}\n",
    "    for cluster_id in clustered_data.keys():\n",
    "        tot_cluster_exec_cost = 0\n",
    "        tot_cluster_past_fault_cov = 0\n",
    "        tot_cluster_stmt_cov = 0\n",
    "        for test_case in clustered_data[cluster_id]:\n",
    "            tot_cluster_exec_cost += test_cases_costs[sir_program][test_case]\n",
    "            tot_cluster_past_fault_cov += faults_dictionary[sir_program][test_case]\n",
    "        tot_cluster_past_fault_cov = tot_cluster_past_fault_cov/tot_test_cases\n",
    "        tot_cluster_stmt_cov = num_of_covered_lines(sir_program,clustered_data[cluster_id])/total_program_lines[sir_program]\n",
    "        cluster_metrics[cluster_id] = {\n",
    "            \"tot_exec_cost\": tot_cluster_exec_cost,\n",
    "            \"tot_past_fault_cov\": tot_cluster_past_fault_cov,\n",
    "            \"tot_stmt_cov\": tot_cluster_stmt_cov  # Avg stmt coverage per test case in cluster\n",
    "        }\n",
    "        print(f\"Cluster {cluster_id + 1} metrics:\")\n",
    "        print(f\"Test Cases: {clustered_data[cluster_id]}\")\n",
    "        print(f\" - Num. Test Cases: {len(clustered_data[cluster_id]):.2f}\")\n",
    "        print(f\" - Execution Cost: {tot_cluster_exec_cost:.2f}\")\n",
    "        print(f\" - Past Fault Coverage (%): {tot_cluster_past_fault_cov}\")\n",
    "        print(f\" - Statement Coverage (%): {tot_cluster_stmt_cov:.2f}\\n\")\n",
    "    \n",
    "    for cluster_id in clustered_data.keys():\n",
    "        print(\"Test cases of cluster \" + str(cluster_id) + \": \" + str(len(clustered_data[cluster_id])))\n",
    "    \n",
    "    print(\"======================================================================================\")\n",
    "    \n",
    "    print(\"Program Name: \" + sir_program)\n",
    "    \n",
    "    for cluster_id in clustered_data.keys():\n",
    "        if len(clustered_data[cluster_id]) > max_cluster_dim:\n",
    "            print(\"Test cases of cluster \" + str(cluster_id) + \": \" + str(len(clustered_data[cluster_id])))\n",
    "    \n",
    "    # Plotting the clusters in 3D space\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Extracting data for plotting\n",
    "    exec_costs = np.array(list(test_cases_costs[sir_program].values()))\n",
    "    fault_covs = np.array(faults_dictionary[sir_program])\n",
    "    stmt_covs = np.array(test_cases_stmt_cov)\n",
    "    \n",
    "    # Plot each cluster with a different color\n",
    "    colors = plt.cm.get_cmap(\"tab10\", num_clusters)  # A colormap with as many colors as clusters\n",
    "    for cluster_id in clustered_data.keys():\n",
    "        cluster_indices = clustered_data[cluster_id]\n",
    "        \n",
    "        # Plot each cluster's points\n",
    "        ax.scatter(\n",
    "            exec_costs[cluster_indices], \n",
    "            fault_covs[cluster_indices], \n",
    "            stmt_covs[cluster_indices], \n",
    "            color=colors(cluster_id), \n",
    "            label=f\"Cluster {cluster_id + 1}\"\n",
    "        )\n",
    "    \n",
    "    # Label the axes\n",
    "    ax.set_xlabel(\"Execution Cost\")\n",
    "    ax.set_ylabel(\"Past Fault Coverage\")\n",
    "    ax.set_zlabel(\"Statement Coverage\")\n",
    "    ax.legend()\n",
    "    ax.set_title(\"Test Case Clustering Visualization\")\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "    \n",
    "print(clusters_dictionary)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectQAOA Decomposition Time(ms): 3.2248497009277344\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 88\u001B[0m\n\u001B[1;32m     86\u001B[0m tot_cluster_past_fault_cov \u001B[38;5;241m=\u001B[39m tot_cluster_past_fault_cov\u001B[38;5;241m/\u001B[39mtot_test_cases\n\u001B[1;32m     87\u001B[0m tot_cluster_stmt_cov \u001B[38;5;241m=\u001B[39m num_of_covered_lines(sir_program,clustered_data[cluster_id])\u001B[38;5;241m/\u001B[39mtotal_program_lines[sir_program]\n\u001B[0;32m---> 88\u001B[0m cluster_metrics[cluster_id] \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     89\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtot_exec_cost\u001B[39m\u001B[38;5;124m\"\u001B[39m: tot_cluster_exec_cost,\n\u001B[1;32m     90\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtot_past_fault_cov\u001B[39m\u001B[38;5;124m\"\u001B[39m: tot_cluster_past_fault_cov,\n\u001B[1;32m     91\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtot_stmt_cov\u001B[39m\u001B[38;5;124m\"\u001B[39m: tot_cluster_stmt_cov  \u001B[38;5;66;03m# Avg stmt coverage per test case in cluster\u001B[39;00m\n\u001B[1;32m     92\u001B[0m }\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCluster \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcluster_id\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m metrics:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     94\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest Cases: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mclustered_data[cluster_id]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mIndexError\u001B[0m: list assignment index out of range"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "97b42e5ea26ae8a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T16:24:09.893148Z",
     "start_time": "2025-05-13T16:24:09.886214Z"
    }
   },
   "source": [
    "def make_linear_terms(sir_program, cluster_test_cases, alpha):\n",
    "    \"\"\"Making the linear terms of QUBO\"\"\"\n",
    "    max_cost = max(test_cases_costs[sir_program].values())\n",
    "    \n",
    "    estimated_costs = []\n",
    "\n",
    "    #linear coefficients, that are the diagonal of the matrix encoding the QUBO\n",
    "    for test_case in cluster_test_cases:\n",
    "        estimated_costs.append((alpha * (test_cases_costs[sir_program][test_case]/max_cost)) - (1 - alpha) * faults_dictionary[sir_program][test_case])\n",
    "    \n",
    "    return np.array(estimated_costs)\n",
    "\n",
    "def make_quadratic_terms(sir_program, variables, cluster_test_cases, linear_terms, penalty):\n",
    "    \"\"\"Making the quadratic terms of QUBO\"\"\"\n",
    "    quadratic_terms = {}\n",
    "    \n",
    "    #k is a stmt\n",
    "    for k in executed_lines_test_by_test[sir_program].keys():\n",
    "        #k_test_cases is the list of test cases covering k\n",
    "        k_test_cases = executed_lines_test_by_test[sir_program][k]\n",
    "        for i in k_test_cases:\n",
    "            if i not in cluster_test_cases or i not in variables:\n",
    "                continue\n",
    "            for j in k_test_cases:\n",
    "                if j not in cluster_test_cases or j not in variables:\n",
    "                    continue\n",
    "                if i < j:\n",
    "                    linear_terms[variables.index(i)] -= penalty\n",
    "                    try:\n",
    "                        quadratic_terms[variables.index(i),variables.index(j)] += 2 * penalty\n",
    "                    except:\n",
    "                        quadratic_terms[variables.index(i),variables.index(j)] = 2 * penalty\n",
    "    \n",
    "    return quadratic_terms"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "abead0754d567232",
   "metadata": {},
   "source": [
    "def create_QUBO_problem(linear_terms, quadratic_terms):\n",
    "    \"\"\"This function is the one that has to encode the QUBO problem that QAOA will have to solve. The QUBO problem specifies the optimization to solve and a quadratic binary unconstrained problem\"\"\"\n",
    "    qubo = QuadraticProgram()\n",
    "    \n",
    "    for i in range(0,len(linear_terms)):\n",
    "        qubo.binary_var('x%s' % (i))\n",
    "\n",
    "    qubo.minimize(linear=linear_terms,quadratic=quadratic_terms)\n",
    "\n",
    "    return qubo\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8ab8c0bce740db9c",
   "metadata": {},
   "source": [
    "penalties_dictionary = {\"flex\":None,\"grep\":None,\"gzip\":None,\"sed\":None}\n",
    "\n",
    "#to get a QUBO problem from a quadratic problem with constraints, we have to insert those constraints into the Hamiltonian to solve (which is the one encoded by the QUBO problem). When we insert constraint into the Hamiltonian, we have to specify also penalties\n",
    "for sir_program in sir_programs:\n",
    "    max_penalty = 0\n",
    "    max_cost = max(test_cases_costs[sir_program].values())\n",
    "    for i in range(sir_programs_tests_number[sir_program]):\n",
    "        cost = (alpha * (test_cases_costs[sir_program][i]/max_cost)) - ((1 - alpha) * faults_dictionary[sir_program][i])\n",
    "        if cost > max_penalty:\n",
    "            max_penalty = cost\n",
    "    penalties_dictionary[sir_program] = max_penalty + 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3c7f468b9deadab3",
   "metadata": {},
   "source": [
    "qubos_dictionary = {\"flex\":[],\"grep\":[],\"gzip\":[],\"sed\":[]}\n",
    "#make a dictionary that saves, for each program, the correspondent QUBO\n",
    "for sir_program in sir_programs:\n",
    "    print(\"SIR Program:\\n\")\n",
    "    for cluster_id in clusters_dictionary[sir_program]:\n",
    "        print(\"Cluster ID: \" + str(cluster_id))\n",
    "        variables = []\n",
    "        for idx in range(0, len(clusters_dictionary[sir_program][cluster_id])):\n",
    "            variables.append(idx)\n",
    "        linear_terms = make_linear_terms(sir_program, clusters_dictionary[sir_program][cluster_id], alpha)\n",
    "        quadratic_terms = make_quadratic_terms(sir_program, variables, clusters_dictionary[sir_program][cluster_id], linear_terms, penalties_dictionary[sir_program])\n",
    "        qubo = create_QUBO_problem(linear_terms, quadratic_terms)\n",
    "        qubos_dictionary[sir_program].append(qubo)\n",
    "        print(qubo)\n",
    "        print(\"/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/\")\n",
    "    print(\"======================================================================================\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "abaf9693911723af",
   "metadata": {},
   "source": [
    "def covered_lines(sir_program,test_cases_list):\n",
    "    \"\"\"Number of covered lines (no redundancy)\"\"\"\n",
    "    covered_lines = set()\n",
    "    \n",
    "    for test_case in test_cases_list:\n",
    "        try:\n",
    "            for covered_line in test_coverage_line_by_line[sir_program][test_case]:\n",
    "                covered_lines.add(covered_line)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return len(covered_lines)\n",
    "\n",
    "def build_pareto_front(sir_program,selected_tests):\n",
    "    \"\"\"This method builds the pareto front additionally from a sub test suite solution\"\"\"\n",
    "    pareto_front = []\n",
    "    max_fault_coverage = 0\n",
    "    max_stmt_coverage = 0\n",
    "    \n",
    "    for index in range(1,len(selected_tests)+1):\n",
    "        #exract the first index selected tests\n",
    "        candidate_solution = selected_tests[:index]\n",
    "        candidate_solution_fault_coverage = 0\n",
    "        candidate_solution_stmt_coverage = 0\n",
    "        for selected_test in candidate_solution:\n",
    "            candidate_solution_fault_coverage += faults_dictionary[sir_program][selected_test]\n",
    "            candidate_solution_stmt_coverage += covered_lines(sir_program,candidate_solution)\n",
    "        #if the actual pareto front dominates the candidate solution, get to the next candidate\n",
    "        if max_fault_coverage >= candidate_solution_fault_coverage and max_stmt_coverage >= candidate_solution_stmt_coverage:\n",
    "            continue\n",
    "        #eventually update the pareto front information\n",
    "        if candidate_solution_stmt_coverage > max_stmt_coverage:\n",
    "            max_stmt_coverage = candidate_solution_stmt_coverage\n",
    "        if candidate_solution_fault_coverage > max_fault_coverage:\n",
    "            max_fault_coverage = candidate_solution_fault_coverage\n",
    "        #add the candidate solution to the pareto front\n",
    "        pareto_front.append(candidate_solution)\n",
    "    \n",
    "    return pareto_front"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Optimal Multi-Objective SelectQAOA Depth\n",
    "\n",
    "First, we want to analyze whether the depth of the circuit of QAOA affects SelectQAOA. To do this, we have executed 10 independent times SelectQAOA on an IDEAL QAOA simulator for different depth values (1, 2, 4, 8, and 16) for each of the examined SIR programs. Then, we applied the Kruskal-Wallis H-test, to see if SelectQAOA behaves differently with different depth values. Finally, we used the Vargha-Delaney's $\\hat{A}_{12}$ effect size to estimate the differences between the observations and identify the best configuration for the depth parameter.\n",
    "\n",
    "\n",
    "## Statevector Simulator\n",
    "The **Statevector Simulator** in Qiskit Aer is a high-precision quantum circuit simulator that represents the full quantum state of a system. In particular, it is a literal state vector or density matrix in a linear algebra sense, to obtain an exact simulation with no involved noise that applies the necessary rounding when performing floating-point operations. It is useful for **simulating ideal quantum circuits** without noise, decoherence, or measurement effects.\n"
   ],
   "id": "ca96f2f7ff88fdec"
  },
  {
   "metadata": {
    "scrolled": true
   },
   "cell_type": "code",
   "source": [
    "algorithm_globals.random_seed = 10598\n",
    "backend = Aer.get_backend(\"statevector_simulator\")\n",
    "\n",
    "#I want to run the sampler 30 times to get different results for each sir program\n",
    "for sir_program in sir_programs:\n",
    "    qaoa_mes = QAOA(sampler=BackendSampler(backend=backend), optimizer=COBYLA(100), reps=sir_programs_rep_values[sir_program])\n",
    "    qaoa = MinimumEigenOptimizer(qaoa_mes)  # using QAOA\n",
    "    #the fronts will be saved into files\n",
    "    print(\"SIR Program: \" + sir_program)\n",
    "    file_path = \"results/selectqaoa/statevector_sim/\" + sir_program + \"-data.json\"\n",
    "    json_data = []\n",
    "    response = None\n",
    "    qpu_run_times = []\n",
    "    pareto_fronts_building_times = []\n",
    "    for i in range(experiments):\n",
    "        final_selected_tests = []\n",
    "        cluster_dict_index = 0\n",
    "        for qubo in qubos_dictionary[sir_program]:\n",
    "            print(\"QUBO Problem: \" + str(qubo) + \"\\n Cluster Number: \" + str(cluster_dict_index))\n",
    "            print(\"Cluster's Test Cases: \" +str(list(clusters_dictionary[sir_program].values())[cluster_dict_index]))\n",
    "            #for each iteration get the result\n",
    "            s = time.time()\n",
    "            qaoa_result = qaoa.solve(qubo)\n",
    "            print(\"RESULTS: \" + str(qaoa_result))\n",
    "            e = time.time()\n",
    "            qpu_run_times.append((e - s) * 1000)\n",
    "            #let's extract the selected tests\n",
    "            variable_values = qaoa_result.x\n",
    "            indexes_selected_tests = [index for index, value in enumerate(variable_values) if value == 1]\n",
    "            print(\"Indexes of selected tests to convert. \" + str(indexes_selected_tests))\n",
    "            selected_tests = []\n",
    "            for index in indexes_selected_tests:\n",
    "                selected_tests.append(list(clusters_dictionary[sir_program].values())[cluster_dict_index][index])\n",
    "            print(\"Selected tests: \" + str(selected_tests))\n",
    "            print(\"Experiment Number: \" + str(i))\n",
    "            cluster_dict_index += 1\n",
    "            for selected_test in selected_tests:\n",
    "                if selected_test not in final_selected_tests:\n",
    "                    final_selected_tests.append(selected_test)\n",
    "        i+=1\n",
    "        #now we have to build the pareto front\n",
    "        print(\"Final Selected Test Cases: \" + str(final_selected_tests))\n",
    "        print(\"Length of the final list of selected test cases: \" + str(len(final_selected_tests)))\n",
    "        start = time.time()\n",
    "        pareto_front = build_pareto_front(sir_program, final_selected_tests)\n",
    "        end = time.time()\n",
    "        json_data[\"pareto_front_\" + str(i)] = pareto_front\n",
    "        pareto_front_building_time = (end - start) * 1000\n",
    "        pareto_fronts_building_times.append(pareto_front_building_time)\n",
    "\n",
    "    #compute the average time needed for the construction of a pareto frontier and run time\n",
    "    mean_qpu_run_time = statistics.mean(qpu_run_times)\n",
    "    mean_pareto_fronts_building_time = statistics.mean(pareto_fronts_building_times)\n",
    "    json_data[\"mean_qpu_run_time(ms)\"] = mean_qpu_run_time\n",
    "    json_data[\"stdev_qpu_run_time(ms)\"] = statistics.stdev(qpu_run_times)\n",
    "    json_data[\"all_qpu_run_times(ms)\"] = qpu_run_times\n",
    "    json_data[\"mean_pareto_fronts_building_time(ms)\"] = mean_pareto_fronts_building_time\n",
    "\n",
    "    with open(file_path, \"w\") as file:\n",
    "        json.dump(json_data, file)\n"
   ],
   "id": "306aa159e2a51733",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Kruskal-Wallis H Test\n",
    "\n",
    "The **Kruskal-Wallis H test** is a **non-parametric** statistical test used to determine whether **three or more independent groups** come from the same population. It is an extension of the **Mann-Whitney U test** for multiple groups and serves as a non-parametric alternative to **one-way ANOVA**.\n",
    "\n",
    "- **Null Hypothesis (\\(H_0\\))**: The samples come from the same distribution (no significant difference between groups).\n",
    "- **Alternative Hypothesis (\\(H_A\\))**: At least one group differs significantly from the others.\n",
    "\n",
    "- A **p-value** is computed to assess statistical significance:\n",
    "  - **If \\( p > 0.05 \\)** → Fail to reject \\( H_0 \\) → Groups may come from the same distribution.\n",
    "  - **If \\( p < 0.05 \\)** → Reject \\( H_0 \\) → At least one group differs significantly.\n",
    "\n",
    "## Vargha-Delaney's $\\hat{A}_{12}$ Effect Size\n",
    "\n",
    "The **Vargha-Delaney $\\hat{A}_{12}$ effect size** is a non-parametric measure used to compare two distributions. It quantifies the probability that a randomly chosen value from one group ( X ) is **greater** than a randomly chosen value from another group ( Y ). It is particularly useful for analyzing differences in performance between two methods, conditions, or datasets.\n",
    "- **$\\hat{A}_{12}$ = 0.5**: Both groups have similar distributions.\n",
    "- **$\\hat{A}_{12}$ > 0.5**: Values in \\( X \\) tend to be larger than those in \\( Y \\).\n",
    "- **$\\hat{A}_{12}$ < 0.5**: Values in \\( Y \\) tend to be larger than those in \\( X \\)."
   ],
   "id": "f54605203bb31517"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def total_cost(sir_program,solution):\n",
    "    \"\"\"Total execution cost of the sub suite\"\"\"\n",
    "    solution_cost = 0\n",
    "    \n",
    "    for test_case in solution:\n",
    "        try:\n",
    "            solution_cost += test_cases_costs[sir_program][test_case]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return solution_cost\n",
    "        \n",
    "def total_coverage(sir_program,solution):\n",
    "    \"\"\"Number of covered lines (no redundancy)\"\"\"\n",
    "    covered_lines = set()\n",
    "    \n",
    "    for test_case in solution:\n",
    "        try:\n",
    "            for covered_line in test_coverage_line_by_line[sir_program][test_case]:\n",
    "                covered_lines.add(covered_line)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return len(covered_lines)\n",
    "    \n",
    "def total_faults(sir_program,solution):\n",
    "    \"\"\"Number of covered faults\"\"\"\n",
    "    covered_faults = 0\n",
    "    \n",
    "    for test_case in solution:\n",
    "        try:\n",
    "            covered_faults += faults_dictionary[sir_program][test_case]\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return covered_faults\n",
    "\n",
    "def pareto_dominance(tuple1, tuple2):\n",
    "    \"\"\"This method returns if tuple1 dominates tuple2\"\"\"\n",
    "    # Check if all conditions are satisfied\n",
    "    dominates = (\n",
    "        (tuple2[0] <= tuple1[0]) and \n",
    "        (tuple2[1] >= tuple1[1]) and \n",
    "        (tuple2[2] >= tuple1[2])\n",
    "    )\n",
    "    \n",
    "    # Check if at least one condition is strict\n",
    "    strict = (\n",
    "        (tuple2[0] < tuple1[0]) or \n",
    "        (tuple2[1] > tuple1[1]) or \n",
    "        (tuple2[2] > tuple1[2])\n",
    "    )\n",
    "    \n",
    "    # Return 1 if the second tuple dominates the first, otherwise 0\n",
    "    return 1 if dominates and strict else 0"
   ],
   "id": "c9ada78e44c97be4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#here we make the same but for each of the fronts built by SelectQA and DIV-GA\n",
    "\n",
    "#the following 2 lists will contain much tuples as solutions in each of the 10 pareto fronts\n",
    "#each tuple represent the fitness value for each solution\n",
    "json_data = {\"flex\":{\"statevector_sim_1\":[],\"statevector_sim_2\":[],\"statevector_sim_4\":[],\"statevector_sim_8\":[],\"statevector_sim_16\":[]},\"grep\":{\"statevector_sim_1\":[],\"statevector_sim_2\":[],\"statevector_sim_4\":[],\"statevector_sim_8\":[],\"statevector_sim_16\":[]},\"gzip\":{\"statevector_sim_1\":[],\"statevector_sim_2\":[],\"statevector_sim_4\":[],\"statevector_sim_8\":[],\"statevector_sim_16\":[]},\"sed\":{\"statevector_sim_1\":[],\"statevector_sim_2\":[],\"statevector_sim_4\":[],\"statevector_sim_8\":[],\"statevector_sim_16\":[]}}\n",
    "normal_dist = True\n",
    "\n",
    "for sir_program in sir_programs:\n",
    "    qaoa_statevector_pareto_vectors_1 = []\n",
    "    qaoa_statevector_pareto_vectors_2 = []\n",
    "    qaoa_statevector_pareto_vectors_4 = []\n",
    "    qaoa_statevector_pareto_vectors_8 = []\n",
    "    qaoa_statevector_pareto_vectors_16 = []\n",
    "    \n",
    "    for index in range(0,30):\n",
    "        # Load the JSON file\n",
    "        with open('results/selectqaoa/statevector_sim/' + sir_program + '-data-rep-1.json', 'r') as file:\n",
    "            pareto_fronts_json = json.load(file)\n",
    "        \n",
    "        qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "        #a single solution is a subset of the initial test suite\n",
    "        for front_solution in qaoa_pareto_front:\n",
    "            qaoa_statevector_pareto_vectors_1.append((total_cost(sir_program,front_solution),total_coverage(sir_program,front_solution),total_faults(sir_program,front_solution)))\n",
    "        #print(len(qaoa_statevector_pareto_vectors_1))\n",
    "        \n",
    "        with open('results/selectqaoa/statevector_sim/' + sir_program + '-data-rep-2.json', 'r') as file:\n",
    "            pareto_fronts_json = json.load(file)\n",
    "        \n",
    "        qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "        #a single solution is a subset of the initial test suite\n",
    "        for front_solution in qaoa_pareto_front:\n",
    "            qaoa_statevector_pareto_vectors_2.append((total_cost(sir_program,front_solution),total_coverage(sir_program,front_solution),total_faults(sir_program,front_solution)))\n",
    "        #print(len(qaoa_statevector_pareto_vectors_2))\n",
    "        \n",
    "        with open('results/selectqaoa/statevector_sim/' + sir_program + '-data-rep-4.json', 'r') as file:\n",
    "            pareto_fronts_json = json.load(file)\n",
    "        \n",
    "        qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "        #a single solution is a subset of the initial test suite\n",
    "        for front_solution in qaoa_pareto_front:\n",
    "            qaoa_statevector_pareto_vectors_4.append((total_cost(sir_program,front_solution),total_coverage(sir_program,front_solution),total_faults(sir_program,front_solution)))\n",
    "        #print(len(qaoa_statevector_pareto_vectors_4))\n",
    "        \n",
    "        with open('results/selectqaoa/statevector_sim/' + sir_program + '-data-rep-8.json', 'r') as file:\n",
    "            pareto_fronts_json = json.load(file)\n",
    "        \n",
    "        qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "        #a single solution is a subset of the initial test suite\n",
    "        for front_solution in qaoa_pareto_front:\n",
    "            qaoa_statevector_pareto_vectors_8.append((total_cost(sir_program,front_solution),total_coverage(sir_program,front_solution),total_faults(sir_program,front_solution)))\n",
    "        #print(len(qaoa_statevector_pareto_vectors_8))\n",
    "        \n",
    "        with open('results/selectqaoa/statevector_sim/' + sir_program + '-data-rep-16.json', 'r') as file:\n",
    "            pareto_fronts_json = json.load(file)\n",
    "        \n",
    "        qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "        #a single solution is a subset of the initial test suite\n",
    "        for front_solution in qaoa_pareto_front:\n",
    "            qaoa_statevector_pareto_vectors_16.append((total_cost(sir_program,front_solution),total_coverage(sir_program,front_solution),total_faults(sir_program,front_solution)))\n",
    "        #print(len(qaoa_noise2_pareto_vectors))\n",
    "    \n",
    "    #once we have the pareto vectors from each pareto front obtained by each run, we extract \n",
    "    #just the solution not dominated by anyone else\n",
    "    \n",
    "    total_fronts = [qaoa_statevector_pareto_vectors_1,qaoa_statevector_pareto_vectors_2,qaoa_statevector_pareto_vectors_4,qaoa_statevector_pareto_vectors_8,qaoa_statevector_pareto_vectors_16]\n",
    "    reference_pareto = []\n",
    "    portions = [0,0,0,0,0]\n",
    "    \n",
    "    # get the reference frontier\n",
    "    for index, front1 in enumerate(total_fronts):\n",
    "        for front_solution1 in front1:\n",
    "            is_dominated = 0\n",
    "            other_fronts = total_fronts[:index] + total_fronts[index+1:]\n",
    "            for front2 in other_fronts:\n",
    "                for front_solution2 in front2:\n",
    "                    if pareto_dominance(front_solution1,front_solution2):\n",
    "                        is_dominated = 1\n",
    "                        break\n",
    "                if is_dominated:\n",
    "                    break\n",
    "            if not is_dominated:\n",
    "                reference_pareto.append(front_solution1)\n",
    "                portions[index] = portions[index] + 1\n",
    "    \n",
    "    print(portions)\n",
    "    # For SelectQA and DIV-GA we want to compute, for each of the 10 iterations, how many of the solutions of the i-th pareto front were selected by the reference front\n",
    "    \n",
    "    qaoa_statevector_non_dominated_values_1 = []\n",
    "    qaoa_statevector_non_dominated_values_2 = []\n",
    "    qaoa_statevector_non_dominated_values_4 = []\n",
    "    qaoa_statevector_non_dominated_values_8 = []\n",
    "    qaoa_statevector_non_dominated_values_16 = []\n",
    "    \n",
    "    for index in range(0,30):\n",
    "        #print(\"Iteratrion: \" + str(index))\n",
    "        \n",
    "        qaoa_statevector_non_dominated_1 = 0\n",
    "        qaoa_statevector_non_dominated_2 = 0\n",
    "        qaoa_statevector_non_dominated_4 = 0\n",
    "        qaoa_statevector_non_dominated_8 = 0\n",
    "        qaoa_statevector_non_dominated_16 = 0\n",
    "        \n",
    "        # Load the JSON file\n",
    "        with open('results/selectqaoa/statevector_sim/' + sir_program + '-data-rep-1.json', 'r') as file:\n",
    "            pareto_fronts_json = json.load(file)\n",
    "        \n",
    "        qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "        for front_solution in qaoa_pareto_front:\n",
    "            if (total_cost(sir_program,front_solution),total_coverage(sir_program,front_solution),total_faults(sir_program,front_solution)) in reference_pareto:\n",
    "                qaoa_statevector_non_dominated_1 += 1\n",
    "        \n",
    "        qaoa_statevector_non_dominated_values_1.append(qaoa_statevector_non_dominated_1)\n",
    "        #print(qaoa_statevector_non_dominated_1)\n",
    "        \n",
    "        with open('results/selectqaoa/statevector_sim/' + sir_program + '-data-rep-2.json', 'r') as file:\n",
    "            pareto_fronts_json = json.load(file)\n",
    "        \n",
    "        qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "        for front_solution in qaoa_pareto_front:\n",
    "            if (total_cost(sir_program,front_solution),total_coverage(sir_program,front_solution),total_faults(sir_program,front_solution)) in reference_pareto:\n",
    "                qaoa_statevector_non_dominated_2 += 1\n",
    "        \n",
    "        qaoa_statevector_non_dominated_values_2.append(qaoa_statevector_non_dominated_2)\n",
    "        #print(qaoa_statevector_non_dominated_1)\n",
    "        \n",
    "        with open('results/selectqaoa/statevector_sim/' + sir_program + '-data-rep-4.json', 'r') as file:\n",
    "            pareto_fronts_json = json.load(file)\n",
    "        \n",
    "        qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "        for front_solution in qaoa_pareto_front:\n",
    "            if (total_cost(sir_program,front_solution),total_coverage(sir_program,front_solution),total_faults(sir_program,front_solution)) in reference_pareto:\n",
    "                qaoa_statevector_non_dominated_4 += 1\n",
    "        \n",
    "        qaoa_statevector_non_dominated_values_4.append(qaoa_statevector_non_dominated_4)\n",
    "        #print(qaoa_statevector_non_dominated_4)\n",
    "        \n",
    "        with open('results/selectqaoa/statevector_sim/' + sir_program + '-data-rep-8.json', 'r') as file:\n",
    "            pareto_fronts_json = json.load(file)\n",
    "        \n",
    "        qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "        for front_solution in qaoa_pareto_front:\n",
    "            if (total_cost(sir_program,front_solution),total_coverage(sir_program,front_solution),total_faults(sir_program,front_solution)) in reference_pareto:\n",
    "                qaoa_statevector_non_dominated_8 += 1\n",
    "        \n",
    "        qaoa_statevector_non_dominated_values_8.append(qaoa_statevector_non_dominated_8)\n",
    "        #print(qaoa_statevector_non_dominated_4)\n",
    "        \n",
    "        with open('results/selectqaoa/statevector_sim/' + sir_program + '-data-rep-16.json', 'r') as file:\n",
    "            pareto_fronts_json = json.load(file)\n",
    "        \n",
    "        qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "        for front_solution in qaoa_pareto_front:\n",
    "            if (total_cost(sir_program,front_solution),total_coverage(sir_program,front_solution),total_faults(sir_program,front_solution)) in reference_pareto:\n",
    "                qaoa_statevector_non_dominated_16 += 1\n",
    "        \n",
    "        qaoa_statevector_non_dominated_values_16.append(qaoa_statevector_non_dominated_16)\n",
    "        #print(qaoa_statevector_non_dominated_8)\n",
    "    \n",
    "    print(\"SelectQAOA Non Dominated Values (Rep 1)\")\n",
    "    print(qaoa_statevector_non_dominated_values_1)\n",
    "    print(\"SelectQAOA Non Dominated Mean (Rep 1)\")\n",
    "    print(statistics.mean(qaoa_statevector_non_dominated_values_1))\n",
    "    print(\"SelectQAOA Non Dominated StDev (Rep 1)\")\n",
    "    print(statistics.stdev(qaoa_statevector_non_dominated_values_1))\n",
    "    json_data[sir_program][\"statevector_sim_1\"] = qaoa_statevector_non_dominated_values_1\n",
    "    \n",
    "    print(\"SelectQAOA Non Dominated Values (Rep 2)\")\n",
    "    print(qaoa_statevector_non_dominated_values_2)\n",
    "    print(\"SelectQAOA Non Dominated Mean (Rep 2)\")\n",
    "    print(statistics.mean(qaoa_statevector_non_dominated_values_2))\n",
    "    print(\"SelectQAOA Non Dominated StDev (Rep 2)\")\n",
    "    print(statistics.stdev(qaoa_statevector_non_dominated_values_2))\n",
    "    json_data[sir_program][\"statevector_sim_2\"] = qaoa_statevector_non_dominated_values_2\n",
    "    \n",
    "    print(\"SelectQAOA Non Dominated Values (Rep 4)\")\n",
    "    print(qaoa_statevector_non_dominated_values_4)\n",
    "    print(\"SelectQAOA Non Dominated Mean (Rep 4)\")\n",
    "    print(statistics.mean(qaoa_statevector_non_dominated_values_4))\n",
    "    print(\"SelectQAOA Non Dominated StDev (Rep 4)\")\n",
    "    print(statistics.stdev(qaoa_statevector_non_dominated_values_4))\n",
    "    json_data[sir_program][\"statevector_sim_4\"] = qaoa_statevector_non_dominated_values_4\n",
    "    \n",
    "    print(\"SelectQAOA Non Dominated Values (Rep 8)\")\n",
    "    print(qaoa_statevector_non_dominated_values_8)\n",
    "    print(\"SelectQAOA Non Dominated Mean (Rep 8)\")\n",
    "    print(statistics.mean(qaoa_statevector_non_dominated_values_8))\n",
    "    print(\"SelectQAOA Non Dominated StDev (Rep 8)\")\n",
    "    print(statistics.stdev(qaoa_statevector_non_dominated_values_8))\n",
    "    json_data[sir_program][\"statevector_sim_18\"] = qaoa_statevector_non_dominated_values_8\n",
    "    \n",
    "    print(\"SelectQAOA Non Dominated Values (Rep 16)\")\n",
    "    print(qaoa_statevector_non_dominated_values_16)\n",
    "    print(\"SelectQAOA Non Dominated Mean (Rep 16)\")\n",
    "    print(statistics.mean(qaoa_statevector_non_dominated_values_16))\n",
    "    print(\"SelectQAOA Non Dominated StDev (Rep 16)\")\n",
    "    print(statistics.stdev(qaoa_statevector_non_dominated_values_16))\n",
    "    json_data[sir_program][\"statevector_sim_16\"] = qaoa_statevector_non_dominated_values_16\n",
    "    \n",
    "    print(json_data)\n",
    "        \n",
    "    if normal_dist:\n",
    "        lists = [qaoa_statevector_non_dominated_values_1,qaoa_statevector_non_dominated_values_2,qaoa_statevector_non_dominated_values_4,qaoa_statevector_non_dominated_values_8,qaoa_statevector_non_dominated_values_16]\n",
    "\n",
    "        # we are interested in knowing if the obtained sequences are normally distributed to decide what statistical test we should apply\n",
    "        for i, list in enumerate(lists, start=1):\n",
    "            stat, p_value = shapiro(list)\n",
    "            #print(f\"List {i}: Statistic = {stat:.5f}, P-value = {p_value:.5f}\")\n",
    "            \n",
    "            # check if the sequence is normally distributed (using 0.05 as threshold)\n",
    "            if p_value <= 0.05:\n",
    "                #print(f\"List {i} NOT seems to be normally distributed (p-value = {p_value:.5f})\")\n",
    "                normal_dist = False\n",
    "                \n",
    "if normal_dist:\n",
    "    print(\"Values all normally distributed\")\n",
    "else:\n",
    "    print(\"Values not normally distributed\")\n",
    "\n",
    "with open(\"statevector_multi_obj_frontiers_eval.json\", \"w\") as file:\n",
    "    json.dump(json_data, file)"
   ],
   "id": "d3b1f0a2b0795805",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Non dominated solutions comparing aer_sim executions\n",
    "\n",
    "Flex (rep = 4 BEST) old: 1\n",
    "list_r1 = [22, 1, 50, 49, 1, 51, 78, 1, 0, 344] m=59.7 stdev=103.69\n",
    "list_r2 = [0, 13, 79, 1, 58, 132, 91, 110, 4, 9] m=49.7 stdev=50.52\n",
    "list_r4 = [1, 128, 19, 64, 0, 184, 182, 104, 240, 220] m=114.2 stdev=90.81\n",
    "list_r8 = [275, 0, 172, 99, 4, 48, 115, 4, 37, 52] m=80.6 stdev=87.99\n",
    "list_r16 = [117, 22, 5, 0, 10, 111, 354, 31, 34, 4] m=68.8 stdev=108.90\n",
    "H_test p_value > 0.05 -> no differences: BEST rep=1\n",
    "\n",
    "Grep (rep = 16 BEST) old: 16\n",
    "1. [7, 0, 1, 13, 4, 0, 4, 0, 1, 0] m=3 stdev=4.24\n",
    "2. [25, 0, 37, 56, 0, 64, 1, 0, 2, 3] m=18.8 stdev=25.17\n",
    "4. [125, 141, 2, 236, 102, 80, 1, 2, 0, 0] m=68.9 stdev=82.01\n",
    "8. [16, 139, 0, 39, 0, 0, 75, 94, 0, 21] m=38.4 stdev=48.59\n",
    "16. [414, 17, 329, 278, 207, 338, 9, 427, 137, 293] m=244.9 stdev=149.65\n",
    "H_test p_value < 0.05 -> differences: BEST rep = 16 \n",
    "\n",
    "Gzip (rep = 16 BEST) old: 1\n",
    "1. [21, 0, 29, 8, 38, 28, 12, 0, 10, 25] m=17.1 stdev=12.99\n",
    "2. [14, 0, 0, 12, 20, 2, 2, 1, 15, 87] m=15.3 stdev=26.25\n",
    "4. [0, 0, 53, 4, 10, 5, 1, 2, 34, 1] m=11 stdev=17.95\n",
    "8. [5, 53, 6, 6, 7, 10, 2, 9, 0, 6] m=10.4 stdev=15.25\n",
    "16. [8, 17, 0, 125, 126, 1, 31, 88, 124, 1] m=52.1 stdev=56.56\n",
    "H_test p_value > 0.05 -> no differences: BEST rep=1\n",
    "\n",
    "Sed (rep = 16 BEST) old: 1\n",
    "1. [3, 14, 45, 5, 8, 13, 13, 22, 127, 4] m=25.4 stdev=37.76\n",
    "2. [35, 26, 4, 4, 18, 13, 25, 42, 3, 139] m=30.9 stdev=40.27\n",
    "4. [6, 176, 6, 26, 42, 15, 5, 5, 191, 25] m=49.7 stdev=71.62\n",
    "8. [3, 52, 8, 17, 5, 2, 65, 67, 4, 3] m=22.6 stdev=27.33\n",
    "16. [28, 193, 161, 29, 36, 39, 68, 54, 64, 120] m=79.2 stdev=58.61\n",
    "H_test p_value > 0.05 -> no differences: BEST rep=1\n",
    "\"\"\""
   ],
   "id": "964e7f9965b110d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Multi-Objective SelectQAOA Execution\n",
    "Once we know the optimal configuration of SelectQAOA for each depth value, we can make the executions of all the simulators with their optimal configurations.\n",
    "\n",
    "## Aer Simulator\n",
    "The Aer Simulator applies sampling noise to simulate the natural stochasticity property of quantum mechanics. No other perturbations are allowed, but the stochasticity itself is a significant obstacle for many optimization methods."
   ],
   "id": "8c86dc11a019880b"
  },
  {
   "metadata": {
    "scrolled": true
   },
   "cell_type": "code",
   "source": [
    "#I want to run the sampler 30 times to get different results for each sir program\n",
    "for sir_program in sir_programs:\n",
    "    aer_sim = AerSimulator()\n",
    "    algorithm_globals.random_seed = 10598\n",
    "    qaoa_mes = QAOA(sampler=BackendSampler(backend=aer_sim), optimizer=COBYLA(100), reps=sir_programs_rep_values[sir_program])\n",
    "    qaoa = MinimumEigenOptimizer(qaoa_mes)  # using QAOA\n",
    "    #the fronts will be saved into files\n",
    "    print(\"SIR Program: \" + sir_program)\n",
    "    file_path = \"results/selectqaoa/aer_sim/\" + sir_program + \"-data.json\"\n",
    "    json_data = []\n",
    "    response = None\n",
    "    qpu_run_times = []\n",
    "    pareto_fronts_building_times = []\n",
    "    for i in range(experiments):\n",
    "        final_selected_tests = []\n",
    "        cluster_dict_index = 0\n",
    "        for qubo in qubos_dictionary[sir_program]:\n",
    "            print(\"QUBO Problem: \" + str(qubo) + \"\\n Cluster Number: \" + str(cluster_dict_index))\n",
    "            print(\"Cluster's Test Cases: \" +str(list(clusters_dictionary[sir_program].values())[cluster_dict_index]))\n",
    "            #for each iteration get the result\n",
    "            s = time.time()\n",
    "            qaoa_result = qaoa.solve(qubo)\n",
    "            print(\"RESULTS: \" + str(qaoa_result))\n",
    "            e = time.time()\n",
    "            qpu_run_times.append((e - s) * 1000)\n",
    "            #let's extract the selected tests\n",
    "            variable_values = qaoa_result.x\n",
    "            indexes_selected_tests = [index for index, value in enumerate(variable_values) if value == 1]\n",
    "            print(\"Indexes of selected tests to convert. \" + str(indexes_selected_tests))\n",
    "            selected_tests = []\n",
    "            for index in indexes_selected_tests:\n",
    "                selected_tests.append(list(clusters_dictionary[sir_program].values())[cluster_dict_index][index])\n",
    "            print(\"Selected tests: \" + str(selected_tests))\n",
    "            print(\"Experiment Number: \" + str(i))\n",
    "            cluster_dict_index += 1\n",
    "            for selected_test in selected_tests:\n",
    "                if selected_test not in final_selected_tests:\n",
    "                    final_selected_tests.append(selected_test)\n",
    "        i+=1\n",
    "        #now we have to build the pareto front\n",
    "        print(\"Final Selected Test Cases: \" + str(final_selected_tests))\n",
    "        print(\"Length of the final list of selected test cases: \" + str(len(final_selected_tests)))\n",
    "        start = time.time()\n",
    "        pareto_front = build_pareto_front(sir_program, final_selected_tests)\n",
    "        end = time.time()\n",
    "        json_data[\"pareto_front_\" + str(i)] = pareto_front\n",
    "        pareto_front_building_time = (end - start) * 1000\n",
    "        pareto_fronts_building_times.append(pareto_front_building_time)\n",
    "\n",
    "    #compute the average time needed for the construction of a pareto frontier and run time\n",
    "    mean_qpu_run_time = statistics.mean(qpu_run_times)\n",
    "    mean_pareto_fronts_building_time = statistics.mean(pareto_fronts_building_times)\n",
    "    json_data[\"mean_qpu_run_time(ms)\"] = mean_qpu_run_time\n",
    "    json_data[\"stdev_qpu_run_time(ms)\"] = statistics.stdev(qpu_run_times)\n",
    "    json_data[\"all_qpu_run_times(ms)\"] = qpu_run_times\n",
    "    json_data[\"mean_pareto_fronts_building_time(ms)\"] = mean_pareto_fronts_building_time\n",
    "\n",
    "    with open(file_path, \"w\") as file:\n",
    "        json.dump(json_data, file)\n"
   ],
   "id": "8b4789c23259bf42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fake Vigo Noise Simulator\n",
    "The **Fake Vigo backend** in Qiskit Aer is a simulated version of IBM Quantum's **real 5-qubit Vigo quantum processor**. It allows users to test quantum circuits in a **realistic noisy environment** introducing **decoherence**."
   ],
   "id": "e63c7b626dc9cd90"
  },
  {
   "metadata": {
    "scrolled": true
   },
   "cell_type": "code",
   "source": [
    "device_backend = FakeVigoV2()\n",
    "\n",
    "algorithm_globals.random_seed = 10598\n",
    "\n",
    "#I want to run the sampler 30 times to obtain different results for each sir program\n",
    "for sir_program in sir_programs:\n",
    "    qaoa_mes = QAOA(sampler=BackendSampler(backend=device_backend), optimizer=COBYLA(100), reps=sir_programs_rep_values[sir_program])\n",
    "    qaoa = MinimumEigenOptimizer(qaoa_mes)  # using QAOA\n",
    "    #the fronts will be saved into files\n",
    "    print(sir_program)\n",
    "    file_path = \"results/selectqaoa/fake_vigo/\" + sir_program + \"-data.json\"\n",
    "    json_data = []\n",
    "    response = None\n",
    "    qpu_run_times = []\n",
    "    pareto_fronts_building_times = []\n",
    "    for i in range(experiments):\n",
    "        final_selected_tests = []\n",
    "        cluster_dict_index = 0\n",
    "        for qubo in qubos_dictionary[sir_program]:\n",
    "            print(\"Experiment Number: \" + str(i))\n",
    "            print(\"QUBO Problem: \" + str(qubo) + \"\\nNumber: \" + str(cluster_dict_index))\n",
    "            print(\"Cluster's Test Cases: \" +str(list(clusters_dictionary[sir_program].values())[cluster_dict_index]))\n",
    "            #for each iteration get the result\n",
    "            s = time.time()\n",
    "            qaoa_result = qaoa.solve(qubo)\n",
    "            print(\"RESULTS: \" + str(qaoa_result))\n",
    "            e = time.time()\n",
    "            qpu_run_times.append((e - s) * 1000)\n",
    "            #let's extract the selected tests\n",
    "            variable_values = qaoa_result.x\n",
    "            indexes_selected_tests = [index for index, value in enumerate(variable_values) if value == 1]\n",
    "            print(\"Indexes of selected tests to convert. \" + str(indexes_selected_tests))\n",
    "            selected_tests = []\n",
    "            for index in indexes_selected_tests:\n",
    "                selected_tests.append(list(clusters_dictionary[sir_program].values())[cluster_dict_index][index])\n",
    "            print(\"Selected tests: \" + str(selected_tests))\n",
    "            cluster_dict_index += 1\n",
    "            for selected_test in selected_tests:\n",
    "                if selected_test not in final_selected_tests:\n",
    "                    final_selected_tests.append(selected_test)\n",
    "        i+=1\n",
    "        #now we have to build the pareto front\n",
    "        print(\"Final Selected Test Cases: \" + str(final_selected_tests))\n",
    "        print(len(final_selected_tests))\n",
    "        start = time.time()\n",
    "        pareto_front = build_pareto_front(sir_program, final_selected_tests)\n",
    "        end = time.time()\n",
    "        json_data[\"pareto_front_\" + str(i)] = pareto_front\n",
    "        pareto_front_building_time = (end - start) * 1000\n",
    "        pareto_fronts_building_times.append(pareto_front_building_time)\n",
    "\n",
    "    #compute the average time needed for the construction of a pareto frontier and run time\n",
    "    mean_qpu_run_time = statistics.mean(qpu_run_times)\n",
    "    mean_pareto_fronts_building_time = statistics.mean(pareto_fronts_building_times)\n",
    "    json_data[\"mean_qpu_run_time(ms)\"] = mean_qpu_run_time\n",
    "    json_data[\"stdev_qpu_run_time(ms)\"] = statistics.stdev(qpu_run_times)\n",
    "    json_data[\"all_qpu_run_times(ms)\"] = qpu_run_times\n",
    "    json_data[\"mean_pareto_fronts_building_time(ms)\"] = mean_pareto_fronts_building_time\n",
    "\n",
    "    with open(file_path, \"w\") as file:\n",
    "        json.dump(json_data, file)\n"
   ],
   "id": "8cb54e2582c97bf1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Noise Mitigation in Quantum Computing\n",
    "\n",
    "Quantum computers are inherently prone to errors due to decoherence, gate imperfections, and readout noise. These errors can significantly affect the accuracy of quantum algorithms, especially those relying on near-term noisy quantum hardware (NISQ devices). \n",
    "\n",
    "To combat these effects and extract more reliable results, **noise mitigation** techniques are employed. Unlike error correction, which requires additional qubits and complex encoding schemes, noise mitigation works at the software level and can be applied post-processing or during circuit execution.\n",
    "\n",
    "### Zero-Noise Extrapolation (ZNE)\n",
    "\n",
    "**Zero-Noise Extrapolation** is a leading noise mitigation strategy. It works by deliberately amplifying the noise in a controlled way and then using the results to extrapolate back to the zero-noise limit. Here's how it works:\n",
    "\n",
    "1. **Run the same quantum circuit multiple times**, each time artificially increasing the noise level (e.g., by folding gates to increase circuit depth).\n",
    "2. **Record the expectation values** (e.g., energy or measurement outcomes) for each noise level.\n",
    "3. **Fit a curve** (e.g., linear, quadratic) through these noisy points.\n",
    "4. **Extrapolate back** to the zero-noise value (i.e., what the result would be if there were no noise).\n",
    "\n"
   ],
   "id": "4180827a3bcca4bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def expectation_from_distribution(distribution, operator):\n",
    "    expectation = 0.0\n",
    "    for pauli, coeff in zip(operator.paulis, operator.coeffs):\n",
    "        pauli_str = pauli.to_label()\n",
    "        if any(p in pauli_str for p in \"XY\"):\n",
    "            continue\n",
    "        for bitstring, prob in distribution.items():\n",
    "            if len(pauli_str) < len(bitstring):\n",
    "                pauli_str = pauli_str.rjust(len(bitstring), \"I\")\n",
    "            parity = 1\n",
    "            for i in range(len(bitstring)):\n",
    "                p = pauli_str[-(i + 1)]\n",
    "                if p == 'Z':\n",
    "                    parity *= 1 if bitstring[-(i + 1)] == '0' else -1\n",
    "            expectation += coeff.real * prob * parity\n",
    "    return expectation.real\n",
    "\n",
    "# Setup backend and seed\n",
    "device_backend = FakeVigoV2()\n",
    "algorithm_globals.random_seed = 10598\n",
    "\n",
    "# Setup ZNE\n",
    "scale_factors = [1.0, 3.0, 5.0]\n",
    "factory = RichardsonFactory(scale_factors=scale_factors)\n",
    "scale_noise = fold_global\n",
    "\n",
    "# Main loop\n",
    "for sir_program in sir_programs:\n",
    "    print(f\"Running program: {sir_program}\")\n",
    "    file_path = f\"results/selectqaoa/fake_vigo_zne/{sir_program}-data.json\"\n",
    "    json_data = {}\n",
    "    qpu_run_times = []\n",
    "    pareto_fronts_building_times = []\n",
    "\n",
    "    for i in range(experiments):\n",
    "        final_selected_tests = []\n",
    "        cluster_dict_index = 0\n",
    "\n",
    "        for qubo in qubos_dictionary[sir_program]:\n",
    "            print(f\"Experiment {i}, QUBO {cluster_dict_index}\")\n",
    "            cluster_tests = list(clusters_dictionary[sir_program].values())[cluster_dict_index]\n",
    "            print(\"Cluster's Test Cases:\", cluster_tests)\n",
    "\n",
    "            # Convert QUBO to Ising\n",
    "            qp2qubo = QuadraticProgramToQubo()\n",
    "            qubo_problem = qp2qubo.convert(qubo)\n",
    "            hamiltonian, _ = qubo_problem.to_ising()\n",
    "            reps = sir_programs_rep_values[sir_program]\n",
    "            ansatz = QAOAAnsatz(hamiltonian, reps=reps)\n",
    "\n",
    "            def zne_expectation(params):\n",
    "                circuit = ansatz.assign_parameters(params)\n",
    "                circuit.measure_all()\n",
    "                def run_expectation(circ):\n",
    "                    sampler = BackendSampler(backend=device_backend)\n",
    "                    result = sampler.run(circ).result()\n",
    "                    dist = result.quasi_dists[0].binary_probabilities()\n",
    "                    return expectation_from_distribution(dist, hamiltonian)\n",
    "                return zne.execute_with_zne(circuit, run_expectation, scale_noise=scale_noise, factory=factory)\n",
    "\n",
    "            initial_params = np.array([0.5] * ansatz.num_parameters)\n",
    "            result = minimize(zne_expectation, initial_params, method=\"COBYLA\")\n",
    "            optimal_params = result.x\n",
    "\n",
    "            final_circuit = ansatz.assign_parameters(optimal_params)\n",
    "            final_circuit.measure_all()\n",
    "            sampler = BackendSampler(backend=device_backend)\n",
    "\n",
    "            s = time.time()\n",
    "            final_result = sampler.run(final_circuit).result()\n",
    "            e = time.time()\n",
    "            dist = final_result.quasi_dists[0].binary_probabilities()\n",
    "            best_bitstring = max(dist.items(), key=lambda x: x[1])[0]\n",
    "            qpu_run_times.append((e - s) * 1000)\n",
    "\n",
    "            selected_tests = [cluster_tests[j] for j, bit in enumerate(best_bitstring[::-1]) if bit == '1']\n",
    "            print(f\"Selected tests: {selected_tests}\")\n",
    "\n",
    "            for test in selected_tests:\n",
    "                if test not in final_selected_tests:\n",
    "                    final_selected_tests.append(test)\n",
    "\n",
    "            cluster_dict_index += 1\n",
    "\n",
    "        print(\"Final selected tests:\", final_selected_tests)\n",
    "        print(\"Number of tests selected:\", len(final_selected_tests))\n",
    "\n",
    "        start = time.time()\n",
    "        pareto_front = build_pareto_front(sir_program, final_selected_tests)\n",
    "        end = time.time()\n",
    "        pareto_fronts_building_times.append((end - start) * 1000)\n",
    "        json_data[f\"pareto_front_{i}\"] = pareto_front\n",
    "\n",
    "    json_data[\"mean_qpu_run_time(ms)\"] = statistics.mean(qpu_run_times)\n",
    "    json_data[\"stdev_qpu_run_time(ms)\"] = statistics.stdev(qpu_run_times)\n",
    "    json_data[\"all_qpu_run_times(ms)\"] = qpu_run_times\n",
    "    json_data[\"mean_pareto_fronts_building_time(ms)\"] = statistics.mean(pareto_fronts_building_times)\n",
    "\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(json_data, f, indent=4)\n",
    "\n",
    "    print(f\"Results saved to {file_path}\")\n"
   ],
   "id": "ab605d8f4023172b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Depolarizing Error Simulator\n",
    "The **depolarizing noise model** is a common quantum noise model that simulates the effect of random errors occurring in quantum computations. In a **depolarizing channel**, a qubit undergoes a transformation where it has a probability \\( p \\) of being replaced by a completely mixed state, losing all its quantum coherence.\n",
    "\n",
    "- **1% noise** simulates near-term quantum hardware with optimized error rates.\n",
    "- **2% noise** represents moderate errors in typical superconducting quantum processors.\n",
    "- **5% noise** helps study worst-case scenarios and assess robustness of quantum algorithms.\n",
    "- Comparing different noise levels allows us to evaluate how quantum circuits **degrade** under varying degrees of decoherence and to **develop error mitigation techniques**.\n"
   ],
   "id": "cee2816526058aef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "noise_dep = NoiseModel()\n",
    "\n",
    "# Define a 1% depolarizing error for single-qubit and two-qubit gates\n",
    "depolarizing_error_1q = depolarizing_error(0.01, 1)  # 1% depolarizing error for single-qubit gates\n",
    "depolarizing_error_2q = depolarizing_error(0.01, 2)  # 1% depolarizing error for two-qubit gates\n",
    "\n",
    "# Apply depolarizing noise to single-qubit gates\n",
    "noise_dep.add_all_qubit_quantum_error(depolarizing_error_1q, ['rx', 'rz', 'sx', 'x', 'h'])\n",
    "\n",
    "# Apply depolarizing noise to two-qubit gates\n",
    "noise_dep.add_all_qubit_quantum_error(depolarizing_error_2q, ['cx', 'cz', 'swap'])\n",
    "\n",
    "sim_dep = AerSimulator(noise_model=noise_dep)\n",
    "\n",
    "algorithm_globals.random_seed = 10598\n",
    "\n",
    "#I want to run the sampler 30 times to obtain different results for each sir program\n",
    "for sir_program in sir_programs:\n",
    "    qaoa_mes = QAOA(sampler=BackendSampler(backend=sim_dep), optimizer=COBYLA(100), reps=sir_programs_rep_values[sir_program])\n",
    "    qaoa = MinimumEigenOptimizer(qaoa_mes)  # using QAOA\n",
    "    #the fronts will be saved into files\n",
    "    print(sir_program)\n",
    "    file_path = \"results/selectqaoa/depolarizing_sim/01/\" + sir_program + \"-data.json\"\n",
    "    json_data = []\n",
    "    response = None\n",
    "    qpu_run_times = []\n",
    "    pareto_fronts_building_times = []\n",
    "    for i in range(experiments):\n",
    "        final_selected_tests = []\n",
    "        cluster_dict_index = 0\n",
    "        for qubo in qubos_dictionary[sir_program]:\n",
    "            print(\"Experiment Number: \" + str(i))\n",
    "            print(\"QUBO Problem: \" + str(qubo) + \"\\nNumber: \" + str(cluster_dict_index))\n",
    "            print(\"Cluster's Test Cases: \" +str(list(clusters_dictionary[sir_program].values())[cluster_dict_index]))\n",
    "            #for each iteration get the result\n",
    "            s = time.time()\n",
    "            qaoa_result = qaoa.solve(qubo)\n",
    "            print(\"RESULTS: \" + str(qaoa_result))\n",
    "            e = time.time()\n",
    "            qpu_run_times.append((e - s) * 1000)\n",
    "            #let's extract the selected tests\n",
    "            variable_values = qaoa_result.x\n",
    "            indexes_selected_tests = [index for index, value in enumerate(variable_values) if value == 1]\n",
    "            print(\"Indexes of selected tests to convert. \" + str(indexes_selected_tests))\n",
    "            selected_tests = []\n",
    "            for index in indexes_selected_tests:\n",
    "                selected_tests.append(list(clusters_dictionary[sir_program].values())[cluster_dict_index][index])\n",
    "            print(\"Selected tests: \" + str(selected_tests))\n",
    "            cluster_dict_index += 1\n",
    "            for selected_test in selected_tests:\n",
    "                if selected_test not in final_selected_tests:\n",
    "                    final_selected_tests.append(selected_test)\n",
    "        i+=1\n",
    "        #now we have to build the pareto front\n",
    "        print(\"Final Selected Test Cases: \" + str(final_selected_tests))\n",
    "        print(len(final_selected_tests))\n",
    "        start = time.time()\n",
    "        pareto_front = build_pareto_front(sir_program, final_selected_tests)\n",
    "        end = time.time()\n",
    "        json_data[\"pareto_front_\" + str(i)] = pareto_front\n",
    "        pareto_front_building_time = (end - start) * 1000\n",
    "        pareto_fronts_building_times.append(pareto_front_building_time)\n",
    "\n",
    "    #compute the average time needed for the construction of a pareto frontier and run time\n",
    "    mean_qpu_run_time = statistics.mean(qpu_run_times)\n",
    "    mean_pareto_fronts_building_time = statistics.mean(pareto_fronts_building_times)\n",
    "    json_data[\"mean_qpu_run_time(ms)\"] = mean_qpu_run_time\n",
    "    json_data[\"stdev_qpu_run_time(ms)\"] = statistics.stdev(qpu_run_times)\n",
    "    json_data[\"all_qpu_run_times(ms)\"] = qpu_run_times\n",
    "    json_data[\"mean_pareto_fronts_building_time(ms)\"] = mean_pareto_fronts_building_time\n",
    "\n",
    "    with open(file_path, \"w\") as file:\n",
    "        json.dump(json_data, file)\n"
   ],
   "id": "188a16dd83242111",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "noise_dep = NoiseModel()\n",
    "\n",
    "# Define a 2% depolarizing error for single-qubit and two-qubit gates\n",
    "depolarizing_error_1q = depolarizing_error(0.02, 1)  # 2% depolarizing error for single-qubit gates\n",
    "depolarizing_error_2q = depolarizing_error(0.02, 2)  # 2% depolarizing error for two-qubit gates\n",
    "\n",
    "# Apply depolarizing noise to single-qubit gates\n",
    "noise_dep.add_all_qubit_quantum_error(depolarizing_error_1q, ['rx', 'rz', 'sx', 'x', 'h'])\n",
    "\n",
    "# Apply depolarizing noise to two-qubit gates\n",
    "noise_dep.add_all_qubit_quantum_error(depolarizing_error_2q, ['cx', 'cz', 'swap'])\n",
    "\n",
    "sim_dep = AerSimulator(noise_model=noise_dep)\n",
    "\n",
    "algorithm_globals.random_seed = 10598\n",
    "\n",
    "#I want to run the sampler 30 times to obtain different results for each sir program\n",
    "for sir_program in sir_programs:\n",
    "    qaoa_mes = QAOA(sampler=BackendSampler(backend=sim_dep), optimizer=COBYLA(100), reps=sir_programs_rep_values[sir_program])\n",
    "    qaoa = MinimumEigenOptimizer(qaoa_mes)  # using QAOA\n",
    "    #the fronts will be saved into files\n",
    "    print(sir_program)\n",
    "    file_path = \"results/selectqaoa/depolarizing_sim/02/\" + sir_program + \"-data.json\"\n",
    "    json_data = []\n",
    "    response = None\n",
    "    qpu_run_times = []\n",
    "    pareto_fronts_building_times = []\n",
    "    for i in range(experiments):\n",
    "        final_selected_tests = []\n",
    "        cluster_dict_index = 0\n",
    "        for qubo in qubos_dictionary[sir_program]:\n",
    "            print(\"Experiment Number: \" + str(i))\n",
    "            print(\"QUBO Problem: \" + str(qubo) + \"\\nNumber: \" + str(cluster_dict_index))\n",
    "            print(\"Cluster's Test Cases: \" +str(list(clusters_dictionary[sir_program].values())[cluster_dict_index]))\n",
    "            #for each iteration get the result\n",
    "            s = time.time()\n",
    "            qaoa_result = qaoa.solve(qubo)\n",
    "            print(\"RESULTS: \" + str(qaoa_result))\n",
    "            e = time.time()\n",
    "            qpu_run_times.append((e - s) * 1000)\n",
    "            #let's extract the selected tests\n",
    "            variable_values = qaoa_result.x\n",
    "            indexes_selected_tests = [index for index, value in enumerate(variable_values) if value == 1]\n",
    "            print(\"Indexes of selected tests to convert. \" + str(indexes_selected_tests))\n",
    "            selected_tests = []\n",
    "            for index in indexes_selected_tests:\n",
    "                selected_tests.append(list(clusters_dictionary[sir_program].values())[cluster_dict_index][index])\n",
    "            print(\"Selected tests: \" + str(selected_tests))\n",
    "            cluster_dict_index += 1\n",
    "            for selected_test in selected_tests:\n",
    "                if selected_test not in final_selected_tests:\n",
    "                    final_selected_tests.append(selected_test)\n",
    "        i+=1\n",
    "        #now we have to build the pareto front\n",
    "        print(\"Final Selected Test Cases: \" + str(final_selected_tests))\n",
    "        print(len(final_selected_tests))\n",
    "        start = time.time()\n",
    "        pareto_front = build_pareto_front(sir_program, final_selected_tests)\n",
    "        end = time.time()\n",
    "        json_data[\"pareto_front_\" + str(i)] = pareto_front\n",
    "        pareto_front_building_time = (end - start) * 1000\n",
    "        pareto_fronts_building_times.append(pareto_front_building_time)\n",
    "\n",
    "    #compute the average time needed for the construction of a pareto frontier and run time\n",
    "    mean_qpu_run_time = statistics.mean(qpu_run_times)\n",
    "    mean_pareto_fronts_building_time = statistics.mean(pareto_fronts_building_times)\n",
    "    json_data[\"mean_qpu_run_time(ms)\"] = mean_qpu_run_time\n",
    "    json_data[\"stdev_qpu_run_time(ms)\"] = statistics.stdev(qpu_run_times)\n",
    "    json_data[\"all_qpu_run_times(ms)\"] = qpu_run_times\n",
    "    json_data[\"mean_pareto_fronts_building_time(ms)\"] = mean_pareto_fronts_building_time\n",
    "\n",
    "    with open(file_path, \"w\") as file:\n",
    "        json.dump(json_data, file)\n"
   ],
   "id": "5b50efcb97700f8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "noise_dep = NoiseModel()\n",
    "\n",
    "# Define a 5% depolarizing error for single-qubit and two-qubit gates\n",
    "depolarizing_error_1q = depolarizing_error(0.05, 1)  # 5% depolarizing error for single-qubit gates\n",
    "depolarizing_error_2q = depolarizing_error(0.05, 2)  # 5% depolarizing error for two-qubit gates\n",
    "\n",
    "# Apply depolarizing noise to single-qubit gates\n",
    "noise_dep.add_all_qubit_quantum_error(depolarizing_error_1q, ['rx', 'rz', 'sx', 'x', 'h'])\n",
    "\n",
    "# Apply depolarizing noise to two-qubit gates\n",
    "noise_dep.add_all_qubit_quantum_error(depolarizing_error_2q, ['cx', 'cz', 'swap'])\n",
    "\n",
    "sim_dep = AerSimulator(noise_model=noise_dep)\n",
    "\n",
    "algorithm_globals.random_seed = 10598\n",
    "\n",
    "#I want to run the sampler 30 times to obtain different results for each sir program\n",
    "for sir_program in sir_programs:\n",
    "    qaoa_mes = QAOA(sampler=BackendSampler(backend=sim_dep), optimizer=COBYLA(100), reps=sir_programs_rep_values[sir_program])\n",
    "    qaoa = MinimumEigenOptimizer(qaoa_mes)  # using QAOA\n",
    "    #the fronts will be saved into files\n",
    "    print(sir_program)\n",
    "    file_path = \"results/selectqaoa/depolarizing_sim/05/\" + sir_program + \"-data.json\"\n",
    "    json_data = []\n",
    "    response = None\n",
    "    qpu_run_times = []\n",
    "    pareto_fronts_building_times = []\n",
    "    for i in range(experiments):\n",
    "        final_selected_tests = []\n",
    "        cluster_dict_index = 0\n",
    "        for qubo in qubos_dictionary[sir_program]:\n",
    "            print(\"Experiment Number: \" + str(i))\n",
    "            print(\"QUBO Problem: \" + str(qubo) + \"\\nNumber: \" + str(cluster_dict_index))\n",
    "            print(\"Cluster's Test Cases: \" +str(list(clusters_dictionary[sir_program].values())[cluster_dict_index]))\n",
    "            #for each iteration get the result\n",
    "            s = time.time()\n",
    "            qaoa_result = qaoa.solve(qubo)\n",
    "            print(\"RESULTS: \" + str(qaoa_result))\n",
    "            e = time.time()\n",
    "            qpu_run_times.append((e - s) * 1000)\n",
    "            #let's extract the selected tests\n",
    "            variable_values = qaoa_result.x\n",
    "            indexes_selected_tests = [index for index, value in enumerate(variable_values) if value == 1]\n",
    "            print(\"Indexes of selected tests to convert. \" + str(indexes_selected_tests))\n",
    "            selected_tests = []\n",
    "            for index in indexes_selected_tests:\n",
    "                selected_tests.append(list(clusters_dictionary[sir_program].values())[cluster_dict_index][index])\n",
    "            print(\"Selected tests: \" + str(selected_tests))\n",
    "            cluster_dict_index += 1\n",
    "            for selected_test in selected_tests:\n",
    "                if selected_test not in final_selected_tests:\n",
    "                    final_selected_tests.append(selected_test)\n",
    "        i+=1\n",
    "        #now we have to build the pareto front\n",
    "        print(\"Final Selected Test Cases: \" + str(final_selected_tests))\n",
    "        print(len(final_selected_tests))\n",
    "        start = time.time()\n",
    "        pareto_front = build_pareto_front(sir_program, final_selected_tests)\n",
    "        end = time.time()\n",
    "        json_data[\"pareto_front_\" + str(i)] = pareto_front\n",
    "        pareto_front_building_time = (end - start) * 1000\n",
    "        pareto_fronts_building_times.append(pareto_front_building_time)\n",
    "\n",
    "    #compute the average time needed for the construction of a pareto frontier and run time\n",
    "    mean_qpu_run_time = statistics.mean(qpu_run_times)\n",
    "    mean_pareto_fronts_building_time = statistics.mean(pareto_fronts_building_times)\n",
    "    json_data[\"mean_qpu_run_time(ms)\"] = mean_qpu_run_time\n",
    "    json_data[\"stdev_qpu_run_time(ms)\"] = statistics.stdev(qpu_run_times)\n",
    "    json_data[\"all_qpu_run_times(ms)\"] = qpu_run_times\n",
    "    json_data[\"mean_pareto_fronts_building_time(ms)\"] = mean_pareto_fronts_building_time\n",
    "\n",
    "    with open(file_path, \"w\") as file:\n",
    "        json.dump(json_data, file)\n"
   ],
   "id": "41e0dc50b3f44546",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Effectiveness Empirical Evaluations\n",
    "The four algorithms were evaluated in terms of their **effectiveness**. In particular, we built a **reference Pareto front** made by all the **non-dominated solutions** found by all the algorithms during all ten runs. \n",
    "\n",
    "Let $P = \\{ P_1, ..., P_l \\}$ be the set of \\( $l$ \\) different Pareto fronts obtained after all the experiment runs by all the evaluated algorithms. The **reference Pareto front** $P_{\\text{ref}}$ is defined as follows:\n",
    "\n",
    "\n",
    "$P_{\\text{ref}} \\subseteq \\bigcup_{i=1}^{l} P_i$\n",
    "\n",
    "\n",
    "where $\\forall p \\in P_{\\text{ref}}, \\nexists q \\in P_{\\text{ref}}: q > p$. \n",
    "\n",
    "Given the reference front, we computed as an **effectiveness metric** the **number of non-dominated solutions**, i.e., the number of non-dominated solutions found by an algorithm selected for the final reference front.\n",
    "\n",
    "To ensure the empirical reliability of the results, we used the **Mann-Whitney U test** and the **Vargha-Delaney effect size** $\\hat{A}_{12}$. The null hypothesis, i.e., there is **no statistically significant difference** between the effectiveness of the algorithms, is rejected if p-values < 0.05.\n",
    "\n",
    "\n",
    "### The Dunn's Test\n",
    "\n",
    "The **Kruskal-Wallis H-test** is a non-parametric statistical test used to determine whether there are significant differences between three or more independent groups. However, while the test can indicate that at least one group differs, it does not specify which groups are different from each other.\n",
    "\n",
    "To address this, we apply **Dunn's Test**, a post-hoc pairwise comparison method that helps identify which specific groups differ. This test compares all possible pairs of groups to determine which ones have statistically significant differences. Since multiple comparisons increase the chance of false positives, Dunn's test is often used with a correction method such as Bonferroni or Holm to adjust p-values. If the adjusted p-value is below the significance threshold (typically 0.05), we conclude that the two compared groups are significantly different.\n",
    "\n"
   ],
   "id": "927ce932-68b7-4a3b-a8ed-edc0e7ded093"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#the other sizes are uniform\n",
    "#this static information can be easily obtained from the pareto .json files\n",
    "\n",
    "qtcs_flex_pareto_sizes = [187,187,187,187,187,187,187,187,187,187]\n",
    "divga_flex_pareto_sizes = [140,140,140,140,140,140,140,140,140,140]\n",
    "qaoa_statevector_flex_pareto_sizes = [447,434,413,409,434,455,446,410,416,427]\n",
    "qaoa_aer_flex_pareto_sizes = [517,515,518,523,516,518,505,514,518,526]\n",
    "qaoa_fakevigo_flex_pareto_sizes = [540,540,540,540,540,540,540,539,539,539]\n",
    "qaoa_noise1_flex_pareto_sizes = [539,540,540,539,540,540,539,539,539,539]\n",
    "qaoa_noise2_flex_pareto_sizes = [540,539,540,539,540,540,539,540,539,540]\n",
    "qaoa_noise5_flex_pareto_sizes = [539,539,540,540,540,539,540,540,540,539]\n",
    "greedy_flex_size = 567\n",
    "\n",
    "qtcs_grep_pareto_sizes = [229,229,230,229,230,229,230.229,230,230]\n",
    "divga_grep_pareto_sizes = [70,70,70,70,70,70,70,70,70,70]\n",
    "qaoa_statevector_grep_pareto_sizes = [485,507,504,505,489,492,489,493,489,470]\n",
    "qaoa_aer_grep_pareto_sizes = [343,345,343,341,346,345,345,346,340,348]\n",
    "qaoa_fakevigo_grep_pareto_sizes = [321,321,321,321,321,321,321,321,321,321]\n",
    "qaoa_noise1_grep_pareto_sizes = [321,321,321,321,321,321,321,321,321,321]\n",
    "qaoa_noise2_grep_pareto_sizes = [321,321,321,321,321,321,321,321,321,321]\n",
    "qaoa_noise5_grep_pareto_sizes = [321,321,321,321,321,321,321,321,321,321]\n",
    "greedy_grep_size = 802\n",
    "\n",
    "qtcs_gzip_pareto_sizes = [87,86,85,86,87,87,86,87,85,87]\n",
    "divga_gzip_pareto_sizes = [105,105,105,105,105,105,105,105,105,105]\n",
    "qaoa_statevector_gzip_pareto_sizes = [137,143,146,144,164,143,157,144,141,169]\n",
    "qaoa_aer_gzip_pareto_sizes = [147,146,145,146,151,155,155,151,154,156]\n",
    "qaoa_fakevigo_gzip_pareto_sizes = [168,168,166,169,168,167,167,168,168,167]\n",
    "qaoa_noise1_gzip_pareto_sizes = [167,167,167,166,168,167,166,166,168,166]\n",
    "qaoa_noise2_gzip_pareto_sizes = [166,166,168,167,166,166,166,167,167,166]\n",
    "qaoa_noise5_gzip_pareto_sizes = [169,167,167,169,168,168,168,165,167,167]\n",
    "greedy_gzip_size = 199\n",
    "\n",
    "qtcs_sed_pareto_sizes = [131,131,131,131,131,131,131,131,131,131]\n",
    "divga_sed_pareto_sizes = [105,62,105,105,102,105,105,97,105,105]\n",
    "qaoa_statevector_sed_pareto_sizes = [249,241,252,226,245,231,248,241,245,237]\n",
    "qaoa_aer_sed_pareto_sizes = [260,251,253,263,252,263,260,256,250,253]\n",
    "qaoa_fakevigo_sed_pareto_sizes = [235,235,236,235,236,236,235,236,236,236]\n",
    "qaoa_noise1_sed_pareto_sizes = [236,235,235,235,236,236,236,235,236,235]\n",
    "qaoa_noise2_sed_pareto_sizes = [235,235,236,235,235,236,236,236,236,236]\n",
    "qaoa_noise5_sed_pareto_sizes = [236,236,235,236,236,235,235,236,236,235]\n",
    "greedy_sed_size = 356"
   ],
   "id": "fd4f851336f7e0fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Unpack the tuples for each list\n",
    "def unpack_tuples(data):\n",
    "    \"\"\"Unpack the data\"\"\"\n",
    "    return zip(*data)\n",
    "\n",
    "json_data = {\"flex\":{\"statevector_sim\":[],\"aer_sim\":[],\"fake_vigo\":[],\"depolarizing_sim/01\":[],\"depolarizing_sim/02\":[],\"depolarizing_sim/05\":[]},\"grep\":{\"statevector_sim\":[],\"aer_sim\":[],\"fake_vigo\":[],\"depolarizing_sim/01\":[],\"depolarizing_sim/02\":[],\"depolarizing_sim/05\":[]},\"gzip\":{\"statevector_sim\":[],\"aer_sim\":[],\"fake_vigo\":[],\"depolarizing_sim/01\":[],\"depolarizing_sim/02\":[],\"depolarizing_sim/05\":[]},\"sed\":{\"statevector_sim\":[],\"aer_sim\":[],\"fake_vigo\":[],\"depolarizing_sim/01\":[],\"depolarizing_sim/02\":[],\"depolarizing_sim/05\":[]}}\n",
    "normal_dist = True\n",
    "for sir_program in sir_programs:\n",
    "    print(\"=================\"+sir_program+\"=================\")\n",
    "    with open('./results/add-greedy/'+sir_program+'_data.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    greedy_pareto_front = pareto_fronts_json['pareto_front']\n",
    "    \n",
    "    #each solution of the pareto front is a subset of the test suite\n",
    "    greedy_pareto_vectors = []\n",
    "    for front_solution in greedy_pareto_front:\n",
    "        #compute the fitness scores of each single solution\n",
    "        greedy_pareto_vectors.append((total_cost(sir_program,front_solution),total_coverage(sir_program,front_solution),total_faults(sir_program,front_solution)))\n",
    "        #here we make the same but for each of the fronts built by SelectQA and DIV-GA\n",
    "\n",
    "    #the following 2 lists will contain much tuples as solutions in each of the 10 pareto fronts\n",
    "    #each tuple represent the fitness value for each solution\n",
    "    qtcs_pareto_vectors = []\n",
    "    divga_pareto_vectors = []\n",
    "    qaoa_statevector_pareto_vectors = []\n",
    "    qaoa_aer_pareto_vectors = []\n",
    "    qaoa_fakevigo_pareto_vectors = []\n",
    "    qaoa_noise1_pareto_vectors = []\n",
    "    qaoa_noise2_pareto_vectors = []\n",
    "    qaoa_noise5_pareto_vectors = []\n",
    "    \n",
    "    for index in range(0,10):\n",
    "        # Load the JSON file\n",
    "        with open('results/selectqa/'+sir_program+'-data.json', 'r') as file:\n",
    "            pareto_fronts_json = json.load(file)\n",
    "        \n",
    "        qtcs_pareto_front = pareto_fronts_json['pareto_front_'+str(index)]\n",
    "        #a single solution is a subset of the initial test suite\n",
    "        for front_solution in qtcs_pareto_front:\n",
    "            qtcs_pareto_vectors.append((total_cost(sir_program,front_solution),total_coverage(sir_program,front_solution),total_faults(sir_program,front_solution)))\n",
    "        #print(len(qtcs_pareto_vectors))\n",
    "        \n",
    "        with open('./results/divga/'+sir_program+'_pareto_fronts_divga.json', 'r') as file:\n",
    "            pareto_fronts_json = json.load(file)\n",
    "        \n",
    "        divga_pareto_front = pareto_fronts_json[sir_program+'_pareto_front_'+str(index)]\n",
    "        for front_solution in divga_pareto_front:\n",
    "            divga_pareto_vectors.append((total_cost(sir_program,front_solution),total_coverage(sir_program,front_solution),total_faults(sir_program,front_solution)))\n",
    "        #print(len(divga_pareto_vectors))\n",
    "        \n",
    "        #the .json file extracted for the statevector simulator is the copy of the optimal one (between 1,2,4,8,16)\n",
    "        with open('results/selectqaoa/statevector_sim/'+sir_program+'-data.json', 'r') as file:\n",
    "            pareto_fronts_json = json.load(file)\n",
    "        \n",
    "        qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "        #a single solution is a subset of the initial test suite\n",
    "        for front_solution in qaoa_pareto_front:\n",
    "            qaoa_statevector_pareto_vectors.append((total_cost(sir_program,front_solution),total_coverage(sir_program,front_solution),total_faults(sir_program,front_solution)))\n",
    "        #print(len(qaoa_statevector_pareto_vectors))\n",
    "        \n",
    "        with open('results/selectqaoa/aer_sim/'+sir_program+'-data.json', 'r') as file:\n",
    "            pareto_fronts_json = json.load(file)\n",
    "        \n",
    "        qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "        #a single solution is a subset of the initial test suite\n",
    "        for front_solution in qaoa_pareto_front:\n",
    "            qaoa_aer_pareto_vectors.append((total_cost(sir_program,front_solution),total_coverage(sir_program,front_solution),total_faults(sir_program,front_solution)))\n",
    "        #print(len(qaoa_aer_pareto_vectors))\n",
    "        \n",
    "        with open('results/selectqaoa/fake_vigo/'+sir_program+'-data.json', 'r') as file:\n",
    "            pareto_fronts_json = json.load(file)\n",
    "        \n",
    "        qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "        #a single solution is a subset of the initial test suite\n",
    "        for front_solution in qaoa_pareto_front:\n",
    "            qaoa_fakevigo_pareto_vectors.append((total_cost(sir_program,front_solution),total_coverage(sir_program,front_solution),total_faults(sir_program,front_solution)))\n",
    "        #print(len(qaoa_fakevigo_pareto_vectors))\n",
    "        \n",
    "        with open('results/selectqaoa/depolarizing_sim/01/'+sir_program+'-data.json', 'r') as file:\n",
    "            pareto_fronts_json = json.load(file)\n",
    "        \n",
    "        qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "        #a single solution is a subset of the initial test suite\n",
    "        for front_solution in qaoa_pareto_front:\n",
    "            qaoa_noise1_pareto_vectors.append((total_cost(sir_program,front_solution),total_coverage(sir_program,front_solution),total_faults(sir_program,front_solution)))\n",
    "        #print(len(qaoa_noise1_pareto_vectors))\n",
    "        \n",
    "        with open('results/selectqaoa/depolarizing_sim/02/'+sir_program+'-data.json', 'r') as file:\n",
    "            pareto_fronts_json = json.load(file)\n",
    "        \n",
    "        qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "        #a single solution is a subset of the initial test suite\n",
    "        for front_solution in qaoa_pareto_front:\n",
    "            qaoa_noise2_pareto_vectors.append((total_cost(sir_program,front_solution),total_coverage(sir_program,front_solution),total_faults(sir_program,front_solution)))\n",
    "        #print(len(qaoa_noise2_pareto_vectors))\n",
    "        \n",
    "        with open('results/selectqaoa/depolarizing_sim/05/'+sir_program+'-data.json', 'r') as file:\n",
    "            pareto_fronts_json = json.load(file)\n",
    "        \n",
    "        qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "        #a single solution is a subset of the initial test suite\n",
    "        for front_solution in qaoa_pareto_front:\n",
    "            qaoa_noise5_pareto_vectors.append((total_cost(sir_program,front_solution),total_coverage(sir_program,front_solution),total_faults(sir_program,front_solution)))\n",
    "        #print(len(qaoa_noise5_pareto_vectors))\n",
    "        \n",
    "    qaoa_pareto_vectors = [qaoa_statevector_pareto_vectors,qaoa_aer_pareto_vectors,qaoa_fakevigo_pareto_vectors,qaoa_noise1_pareto_vectors,qaoa_noise2_pareto_vectors,qaoa_noise5_pareto_vectors]\n",
    "    qaoa_configs = [\"statevector_sim\",\"aer_sim\",\"fake_vigo\",\"depolarizing_sim/01\",\"depolarizing_sim/02\",\"depolarizing_sim/05\"]\n",
    "    \n",
    "    for i in range(len(qaoa_pareto_vectors)):\n",
    "        \n",
    "        total_fronts = [qtcs_pareto_vectors,divga_pareto_vectors,greedy_pareto_vectors,qaoa_pareto_vectors[i]]\n",
    "        reference_pareto = []\n",
    "        portions = [0,0,0,0]\n",
    "        \n",
    "        # get the reference frontier\n",
    "        for index, front1 in enumerate(total_fronts):\n",
    "            for front_solution1 in front1:\n",
    "                is_dominated = 0\n",
    "                other_fronts = total_fronts[:index] + total_fronts[index+1:]\n",
    "                for front2 in other_fronts:\n",
    "                    for front_solution2 in front2:\n",
    "                        if pareto_dominance(front_solution1,front_solution2):\n",
    "                            is_dominated = 1\n",
    "                            break\n",
    "                    if is_dominated:\n",
    "                        break\n",
    "                if not is_dominated:\n",
    "                    reference_pareto.append(front_solution1)\n",
    "                    portions[index] = portions[index] + 1\n",
    "        \n",
    "        #print(portions)\n",
    "        \n",
    "        qtcs_non_dominated_values = []\n",
    "        divga_non_dominated_values = []\n",
    "        qaoa_non_dominated_values = []\n",
    "        greedy_non_dominated = 0\n",
    "        \n",
    "        for index in range(0,10):\n",
    "            qtcs_non_dominated = 0\n",
    "            divga_non_dominated = 0\n",
    "            qaoa_non_dominated = 0\n",
    "            \n",
    "            # Load the JSON file\n",
    "            with open('results/selectqa/'+sir_program+'-data.json', 'r') as file:\n",
    "                pareto_fronts_json = json.load(file)\n",
    "            \n",
    "            qtcs_pareto_front = pareto_fronts_json['pareto_front_'+str(index)]\n",
    "            for front_solution in qtcs_pareto_front:\n",
    "                if (total_cost(sir_program,front_solution),total_coverage(sir_program,front_solution),total_faults(sir_program,front_solution)) in reference_pareto:\n",
    "                    qtcs_non_dominated += 1\n",
    "            \n",
    "            qtcs_non_dominated_values.append(qtcs_non_dominated)\n",
    "            #print(qtcs_non_dominated)\n",
    "            \n",
    "            with open('./results/divga/'+sir_program+'_pareto_fronts_divga.json', 'r') as file:\n",
    "                pareto_fronts_json = json.load(file)\n",
    "            \n",
    "            divga_pareto_front = pareto_fronts_json[sir_program+'_pareto_front_'+str(index)]\n",
    "            for front_solution in divga_pareto_front:\n",
    "                if (total_cost(sir_program,front_solution),total_coverage(sir_program,front_solution),total_faults(sir_program,front_solution)) in reference_pareto:\n",
    "                    divga_non_dominated += 1\n",
    "                    \n",
    "            divga_non_dominated_values.append(divga_non_dominated)\n",
    "            #print(divga_non_dominated)\n",
    "            \n",
    "            with open('results/selectqaoa/'+qaoa_configs[i]+'/'+sir_program+'-data.json', 'r') as file:\n",
    "                pareto_fronts_json = json.load(file)\n",
    "            \n",
    "            qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "            for front_solution in qaoa_pareto_front:\n",
    "                if (total_cost(sir_program,front_solution),total_coverage(sir_program,front_solution),total_faults(sir_program,front_solution)) in reference_pareto:\n",
    "                    qaoa_non_dominated += 1\n",
    "            \n",
    "            qaoa_non_dominated_values.append(qaoa_non_dominated)\n",
    "            #print(qaoa_statevector_non_dominated)\n",
    "            \n",
    "        for front_solution in greedy_pareto_front:\n",
    "            if (total_cost(sir_program,front_solution),total_coverage(sir_program,front_solution),total_faults(sir_program,front_solution)) in reference_pareto:\n",
    "                greedy_non_dominated += 1\n",
    "        #print(greedy_non_dominated)\n",
    "        \n",
    "        print(\"=================\"+sir_program+\"=================\")\n",
    "        print(\"QTCS/DIVGA/ADD GREEDY\")\n",
    "        print(\"=================\"+str(qaoa_configs[i])+\"=================\")\n",
    "        print(\"SelectQA Non Dominated Values\")\n",
    "        print(qtcs_non_dominated_values)\n",
    "        print(\"SelectQA Non Dominated Mean\")\n",
    "        print(statistics.mean(qtcs_non_dominated_values))\n",
    "        print(\"SelectQA Non Dominated StDev\")\n",
    "        print(statistics.stdev(qtcs_non_dominated_values))\n",
    "        json_data[sir_program][qaoa_configs[i]][\"selectqa\"] = (qtcs_non_dominated_values,statistics.mean(qtcs_non_dominated_values),statistics.stdev(qtcs_non_dominated_values))\n",
    "        \n",
    "        print(\"DIV-GA Non Dominated Values\")\n",
    "        print(divga_non_dominated_values)\n",
    "        print(\"DIV-GA Non Dominated Mean\")\n",
    "        print(statistics.mean(divga_non_dominated_values))\n",
    "        print(\"DIV-GA Non Dominated StDev\")\n",
    "        print(statistics.stdev(divga_non_dominated_values))\n",
    "        json_data[sir_program][qaoa_configs[i]][\"divga\"] = (divga_non_dominated_values,statistics.mean(divga_non_dominated_values),statistics.stdev(divga_non_dominated_values))\n",
    "        \n",
    "        print(\"SelectQAOA Non Dominated Values (\"+str(qaoa_configs[i])+\")\")\n",
    "        print(qaoa_non_dominated_values)\n",
    "        print(\"SelectQAOA Non Dominated Mean (\"+str(qaoa_configs[i])+\")\")\n",
    "        print(statistics.mean(qaoa_non_dominated_values))\n",
    "        print(\"SelectQAOA Non Dominated StDev (\"+str(qaoa_configs[i])+\")\")\n",
    "        print(statistics.stdev(qaoa_non_dominated_values))\n",
    "        json_data[sir_program][qaoa_configs[i]][\"qaoa\"] = (qaoa_non_dominated_values,statistics.mean(qaoa_non_dominated_values),statistics.stdev(qaoa_non_dominated_values))\n",
    "        \n",
    "        print(\"Greedy Non Dominated Values\")\n",
    "        print(greedy_non_dominated)\n",
    "        json_data[sir_program][qaoa_configs[i]][\"add_greedy\"] = (greedy_non_dominated,greedy_non_dominated,0)\n",
    "        \n",
    "        print(json_data)\n",
    "        \n",
    "        if normal_dist:\n",
    "            lists = [qtcs_non_dominated_values,divga_non_dominated_values,qaoa_non_dominated_values]\n",
    "    \n",
    "            # we are interested in knowing if the obtained sequences are normally distributed to decide what statistical test we should apply\n",
    "            for i, list in enumerate(lists, start=1):\n",
    "                stat, p_value = shapiro(list)\n",
    "                #print(f\"List {i}: Statistic = {stat:.5f}, P-value = {p_value:.5f}\")\n",
    "                \n",
    "                # check if the sequence is normally distributed (using 0.05 as threshold)\n",
    "                if p_value <= 0.05:\n",
    "                    #print(f\"List {i} NOT seems to be normally distributed (p-value = {p_value:.5f})\")\n",
    "                    normal_dist = False\n",
    "        \n",
    "        # print the plots\n",
    "        best_pareto_vectors = None\n",
    "        means = [statistics.mean(qtcs_non_dominated_values),statistics.mean(divga_non_dominated_values),statistics.mean(qaoa_non_dominated_values),greedy_non_dominated]\n",
    "        best_mean_index = means.index(max(means))\n",
    "        if best_mean_index == 0:\n",
    "            best_pareto_vectors = qtcs_pareto_vectors\n",
    "            msg = \"SelectQA\"\n",
    "        elif best_mean_index == 1:\n",
    "            best_pareto_vectors = divga_pareto_vectors\n",
    "            msg = \"DIV-GA\"\n",
    "        elif best_mean_index == 2:\n",
    "            if i == 0:\n",
    "                best_pareto_vectors = qaoa_statevector_pareto_vectors\n",
    "                msg = \"Statevector SelectQAOA\"\n",
    "            elif i == 1:\n",
    "                best_pareto_vectors = qaoa_aer_pareto_vectors\n",
    "                msg = \"Aer Simulator SelectQAOA\"\n",
    "            elif i == 2:\n",
    "                best_pareto_vectors = qaoa_fakevigo_pareto_vectors\n",
    "                msg = \"FakeVigo SelectQAOA\"\n",
    "            elif i == 3:\n",
    "                best_pareto_vectors = qaoa_noise1_pareto_vectors\n",
    "                msg = \"Dep Err 1 SelectQAOA\"\n",
    "            elif i == 4:\n",
    "                best_pareto_vectors = qaoa_noise2_pareto_vectors\n",
    "                msg = \"Dep Err 2 SelectQAOA\"\n",
    "            elif i == 5:\n",
    "                best_pareto_vectors = qaoa_noise5_pareto_vectors\n",
    "                msg = \"Dep Err 5 SelectQAOA\"\n",
    "        else:\n",
    "            best_pareto_vectors = greedy_pareto_vectors\n",
    "            msg = \"Additional Greedy\" \n",
    "        x1, y1, z1 = unpack_tuples(best_pareto_vectors)\n",
    "        x2, y2, z2 = unpack_tuples(reference_pareto)\n",
    "        \n",
    "        # Create 3D scatter plots\n",
    "        fig = plt.figure(figsize=(18,6))\n",
    "        \n",
    "        ax = fig.add_subplot(133, projection='3d')\n",
    "        ax.scatter(x1, y1, z1, c='m', marker='s', s=100, label=msg)\n",
    "        ax.scatter(x2, y2, z2, c='b', marker='o', s=50, label='Reference Pareto')\n",
    "        ax.set_xlabel('Cost')\n",
    "        ax.set_ylabel('Statement Coverage')\n",
    "        ax.set_zlabel('Past Faults',labelpad=0.2)\n",
    "        ax.set_title(msg + ' vs Reference Pareto')\n",
    "        ax.legend()\n",
    "\n",
    "        # Show the plots\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "if normal_dist:\n",
    "    print(\"Values all normally distributed\")\n",
    "else:\n",
    "    print(\"Values not normally distributed\")\n",
    "\n",
    "with open(\"multi_obj_frontiers_eval.json\", \"w\") as file:\n",
    "    json.dump(json_data, file)"
   ],
   "id": "ccfe8ff65a31b9b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Efficiency Empirical Evaluations\n",
    "Regarding the efficiency, we analyzed the algorithms’ **total execution times**. The total run time of **SelectQAOA** was computed by summing the **decomposition** and **execution times** of its run on a **Barbora (IT4Innovations supercomputer)** featuring 8 cores of the **Intel Xeon Gold 6240 CPU** with **2.60 GHz**. \n",
    "\n",
    "For **SelectQA**, executed on the **D-Wave hybrid_binary_quadratic_model_version2**, we considered the **D-Wave API \"total run time\"** metric. \n",
    "\n",
    "The other algorithms' execution times were computed on an **Apple MacBook Air** featuring an **M1 chip** and **16GB of RAM**.\n",
    "\n",
    "Finally, to ensure the empirical reliability of the results, we used the **Mann-Whitney U test** and the **Vargha-Delaney effect size** $\\hat{A}_{12}$."
   ],
   "id": "cb803100148cebcb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "versions = [\"statevector_sim\", \"aer_sim\", \"fake_vigo\", \"depolarizing_sim/01\", \"depolarizing_sim/02\", \"depolarizing_sim/05\"]\n",
    "\n",
    "def is_not_normal(data):\n",
    "    if len(data) < 3:\n",
    "        return False  # troppo pochi dati per testare\n",
    "    _, p_value = shapiro(data)\n",
    "    return p_value < 0.05\n",
    "\n",
    "def read_json_field(path, field):\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data.get(field, [])\n",
    "    except FileNotFoundError:\n",
    "        return []\n",
    "\n",
    "for sir_program in sir_programs:\n",
    "    not_normal = False\n",
    "\n",
    "    # QTCs\n",
    "    qtcs_path = f\"results/selectqa/{sir_program}-data.json\"\n",
    "    qtcs_times = read_json_field(qtcs_path, \"run_times(ms)\")\n",
    "    if is_not_normal(qtcs_times):\n",
    "        not_normal = True\n",
    "\n",
    "    # Add-greedy\n",
    "    add_greedy_path = f\"results/add-greedy/{sir_program}_data.json\"\n",
    "    add_greedy_time = read_json_field(add_greedy_path, \"resolution_time(ms)\")\n",
    "    values = add_greedy_time if isinstance(add_greedy_time, list) else [add_greedy_time]\n",
    "    if is_not_normal(values):\n",
    "        not_normal = True\n",
    "\n",
    "    # DivGA\n",
    "    divga_path = f\"results/divga/{sir_program}_pareto_fronts_divga.json\"\n",
    "    divga_times = read_json_field(divga_path, \"execution_times\")\n",
    "    if is_not_normal(divga_times):\n",
    "        not_normal = True\n",
    "\n",
    "    # QAOA\n",
    "    for version in versions:\n",
    "        qaoa_path = f\"results/selectqaoa/{version}/{sir_program}-data.json\"\n",
    "        qaoa_times = read_json_field(qaoa_path, \"all_qpu_run_times(ms)\")\n",
    "        if not qaoa_times or len(qaoa_times) < 30:\n",
    "            continue  # salta se dati insufficienti o assenti\n",
    "\n",
    "        if len(qaoa_times) % 10 != 0:\n",
    "            continue  # sicurezza: deve essere divisibile per 10\n",
    "\n",
    "        # Dividi in gruppi da 10 elementi consecutivi e calcola le medie\n",
    "        grouped_means = [\n",
    "            np.mean(qaoa_times[i:i+10])\n",
    "            for i in range(0, len(qaoa_times), 10)\n",
    "        ]\n",
    "\n",
    "        if is_not_normal(grouped_means):\n",
    "            not_normal = True\n",
    "\n",
    "    status = \"not normal\" if not_normal else \"normal\"\n",
    "    print(f\"{sir_program} -> {status}\")"
   ],
   "id": "411bce52a0ce733d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1ed58fef23cbc91f",
   "metadata": {},
   "source": [
    "## Single-Objective SelectQAOA\n",
    "Since the quantum state-of-the-art methods IGDec-QAOA, BootQA, and Single-Objective SelectQA formalize the Test Case Selection problem as a single-objective problem with different objectives than those used by the previously used Multi-Objective algorithms, we implemented the Single-Objective SelectQAOA version. \n",
    "\n",
    "### Chosen Datasets\n",
    "\n",
    "We used datasets chosen for the quantum state-of-the-art original works:\n",
    "\n",
    "- PaintControl: dataset gathered from ABB Robotics Norway;\n",
    "- GSDTSR: dataset gathered from Google;\n",
    "- IOF/ROL: industrial datasets for testing complex industrial robots from ABB Robotic;\n",
    "- ELEVATOR: dataset of tests for elevators from Orona.\n",
    "\n",
    " From datasets PaintControl, GSDTSR, and IOF/ROL we read:\n",
    " \n",
    "- \"execution time\": time needed by the test case to be executed (to minimize);\n",
    "- \"failure rate\": the ability, in percentage, of a test case to spot a failure (to maximize).\n",
    "\n",
    "From ELEVATOR, to form two TCS problems, we read:\n",
    "\n",
    "- (\"ELEVATOR\" problem) \"execution time\" to minimize and \"input diversity\" (the diversity of the test) to maximize;\n",
    "- (\"ELEVATOR2\" problem) execution time\" to minimize, \"passengers count\" (the number of passengers for that case), and \"travel distance\" (distance crossed by the elevator for the test) both to maximize."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bootqa_programs = [\"gsdtsr\",\"paintcontrol\", \"iofrol\", \"elevator\", \"elevator2\"]\n",
    "bootqa_programs_rep_values = {\"gsdtsr\":1,\"paintcontrol\":1,\"iofrol\":2, \"elevator\":4, \"elevator2\":4}\n",
    "experiments = 10"
   ],
   "id": "af2fd91994c47932",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_data(data_name):\n",
    "    \"\"\"Read the datasets\"\"\"\n",
    "    if data_name == \"elevator\":\n",
    "        data = pd.read_csv(\"datasets/quantum_sota_datasets/elevator.csv\", dtype={\"cost\": int, \"input_div\": float})\n",
    "    elif data_name == \"elevator2\":\n",
    "        data = pd.read_csv(\"datasets/quantum_sota_datasets/elevator.csv\", dtype={\"cost\": int, \"pcount\": int, \"dist\": int})\n",
    "    else:\n",
    "        data = pd.read_csv(\"datasets/quantum_sota_datasets/\" + data_name + \".csv\", dtype={\"time\": float, \"rate\": float})\n",
    "        data = data[data['rate'] > 0]\n",
    "    return data"
   ],
   "id": "d830cf456495d31a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Problem Decomposition with Clustering",
   "id": "693e010ae099c9d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bootqa_clusters = dict()\n",
    "\n",
    "for bootqa_program in bootqa_programs:\n",
    "    data = get_data(bootqa_program)\n",
    "    \n",
    "    # Total suite metrics\n",
    "    if bootqa_program == \"elevator\" or bootqa_program == \"elevator2\":\n",
    "        test_cases_costs = data[\"cost\"].tolist()\n",
    "    else:\n",
    "        test_cases_costs = data[\"time\"].tolist()\n",
    "    \n",
    "    if bootqa_program == \"elevator\":\n",
    "        test_cases_effectiveness = data[\"input_div\"].tolist()\n",
    "        print(f\"Tot suite cost: {sum(test_cases_costs)}\")\n",
    "        print(f\"Tot suite input div: {sum(test_cases_effectiveness)}\")\n",
    "    elif bootqa_program == \"elevator2\":\n",
    "        test_cases_pcount = data[\"pcount\"].tolist()\n",
    "        test_cases_dist = data[\"dist\"].tolist()\n",
    "        print(f\"Tot suite cost: {sum(test_cases_costs)}\")\n",
    "        print(f\"Tot suite pcount: {sum(test_cases_pcount)}\")\n",
    "        print(f\"Tot suite dist: {sum(test_cases_dist)}\")\n",
    "    else:\n",
    "        test_cases_effectiveness = data[\"rate\"].tolist()\n",
    "        print(f\"Tot suite cost: {sum(test_cases_costs)}\")\n",
    "        print(f\"Tot suite rate: {sum(test_cases_effectiveness)}\")\n",
    "        \n",
    "    # Normalize data\n",
    "    if bootqa_program != \"elevator2\":\n",
    "        cluster_data = np.column_stack((test_cases_costs, test_cases_effectiveness))\n",
    "    else:\n",
    "        cluster_data = np.column_stack((test_cases_costs, test_cases_pcount, test_cases_dist))\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    normalized_data = scaler.fit_transform(cluster_data)\n",
    "    \n",
    "    if bootqa_program == \"elevator\" or bootqa_program == \"elevator2\":\n",
    "        num_clusters = 90\n",
    "    if bootqa_program == \"gsdtsr\":\n",
    "        num_clusters = 50\n",
    "    if bootqa_program == \"iofrol\":\n",
    "        num_clusters = 6\n",
    "    if bootqa_program == \"paintcontrol\":\n",
    "        num_clusters = 4\n",
    "        \n",
    "    max_cluster_dim = 5\n",
    "    \n",
    "    # Step 2: Perform K-Means Clustering\n",
    "    start = time.time()\n",
    "    linkage_matrix = linkage(normalized_data, method='ward')\n",
    "    clusters = fcluster(linkage_matrix, t=num_clusters, criterion='maxclust')\n",
    "    \n",
    "    # Organize test cases by cluster\n",
    "    clustered_data = defaultdict(list)\n",
    "    for idx, cluster_id in enumerate(clusters):\n",
    "        clustered_data[cluster_id].append(idx)\n",
    "    \n",
    "    # Process clusters to ensure none exceed max_cluster_dim\n",
    "    new_cluster_id = max(clustered_data.keys()) + 1  # Start new IDs after existing ones\n",
    "    to_add = []  # Collect new smaller clusters\n",
    "    \n",
    "    for cluster_id, elements in list(clustered_data.items()):  # Avoid modifying dict during iteration\n",
    "        if len(elements) > max_cluster_dim:\n",
    "            num_splits = -(-len(elements) // max_cluster_dim)  # Ceiling division to get the required number of splits\n",
    "            split_size = -(-len(elements) // num_splits)  # Recalculate to distribute elements evenly\n",
    "            \n",
    "            # Split while keeping sizes balanced\n",
    "            parts = [elements[i:i + split_size] for i in range(0, len(elements), split_size)]\n",
    "    \n",
    "            # Ensure all new clusters are within max_cluster_dim\n",
    "            for part in parts:\n",
    "                if len(part) > max_cluster_dim:\n",
    "                    raise ValueError(f\"A split cluster still exceeds max_cluster_dim ({len(part)} > {max_cluster_dim})!\")\n",
    "    \n",
    "            # Add new parts to the new clusters\n",
    "            to_add.extend(parts)\n",
    "    \n",
    "            # Remove original large cluster\n",
    "            del clustered_data[cluster_id]\n",
    "    \n",
    "    # Assign new IDs to split parts\n",
    "    for part in to_add:\n",
    "        if part:  # Only add if the part is non-empty\n",
    "            clustered_data[new_cluster_id] = part\n",
    "            new_cluster_id += 1\n",
    "    end = time.time()\n",
    "    print(\"SelectQAOA Decomposition Time(ms): \" + str((end-start)*1000))\n",
    "    \n",
    "    bootqa_clusters[bootqa_program] = clustered_data\n",
    "    \n",
    "    # Step 3: Calculate the metrics for each refined cluster\n",
    "    cluster_metrics = {}\n",
    "    for cluster_id, members in clustered_data.items():\n",
    "        tot_cluster_costs = sum(test_cases_costs[i] for i in members)\n",
    "        if bootqa_program != \"elevator2\":\n",
    "            tot_cluster_effectiveness = sum(test_cases_effectiveness[i] for i in members)\n",
    "        else:\n",
    "            tot_cluster_pcount = sum(test_cases_pcount[i] for i in members)\n",
    "            tot_cluster_dist = sum(test_cases_dist[i] for i in members)\n",
    "        if bootqa_program != \"elevator2\":\n",
    "            cluster_metrics[cluster_id] = {\n",
    "                \"tot_cluster_cost\": tot_cluster_costs,\n",
    "                \"tot_cluster_rates\": tot_cluster_effectiveness\n",
    "            }\n",
    "        else:\n",
    "            cluster_metrics[cluster_id] = {\n",
    "                \"tot_cluster_cost\": tot_cluster_costs,\n",
    "                \"tot_cluster_pcount\": tot_cluster_pcount,\n",
    "                \"tot_cluster_dist\": tot_cluster_dist\n",
    "            }\n",
    "        print(f\"Cluster {cluster_id + 1} metrics:\")\n",
    "        print(f\"Test Cases: {members}\")\n",
    "        print(f\" - Num. Test Cases: {len(members):.2f}\")\n",
    "        print(f\" - Execution Cost: {tot_cluster_costs:.2f}\")\n",
    "        if bootqa_program != \"elevator2\":\n",
    "            print(f\" - Failure Rate: {tot_cluster_effectiveness}\")\n",
    "        else:\n",
    "            print(f\" - PCount: {tot_cluster_pcount}\")\n",
    "            print(f\" - Dist: {tot_cluster_dist}\")\n",
    "    \n",
    "    print(\"===========================================================================\")    \n",
    "    \n",
    "    for cluster_id in clustered_data.keys():\n",
    "        if len(clustered_data[cluster_id]) > max_cluster_dim:\n",
    "            print(\"Program: \" + bootqa_program)\n",
    "            print(\"Test cases of cluster \" + str(cluster_id) + \": \" + str(len(clustered_data[cluster_id])))\n",
    "    \n",
    "    # Plotting the clusters in 3D space\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    if bootqa_program != \"elevator2\":\n",
    "        ax = fig.add_subplot(111)\n",
    "    else:\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Extracting data for plotting\n",
    "    exec_costs = np.array(test_cases_costs)\n",
    "    if bootqa_program != \"elevator2\":\n",
    "        effectiveness = np.array(test_cases_effectiveness)\n",
    "    else:\n",
    "        pcounts = np.array(test_cases_pcount)\n",
    "        dists = np.array(test_cases_dist)\n",
    "    \n",
    "    # Plot each refined cluster with a different color\n",
    "    colors = plt.cm.get_cmap(\"tab10\", len(clustered_data))  # A colormap with as many colors as clusters\n",
    "    for cluster_id, members in clustered_data.items():\n",
    "        if bootqa_program != \"elevator2\":\n",
    "            ax.scatter(\n",
    "                exec_costs[members], \n",
    "                effectiveness[members], \n",
    "                color=colors(cluster_id % 10), \n",
    "                label=f\"Cluster {cluster_id + 1}\"\n",
    "            )\n",
    "        else:\n",
    "            ax.scatter(\n",
    "                exec_costs[members], \n",
    "                pcounts[members], \n",
    "                dists[members],\n",
    "                color=colors(cluster_id % 10), \n",
    "                label=f\"Cluster {cluster_id + 1}\"\n",
    "            )\n",
    "    \n",
    "    # Label the axes\n",
    "    ax.set_xlabel(\"Execution Cost\")\n",
    "    if bootqa_program != \"elevator2\":\n",
    "        ax.set_ylabel(\"Effectiveness\")\n",
    "    else:\n",
    "        ax.set_ylabel(\"Passengers Count\")\n",
    "        ax.set_zlabel(\"Travel Distance\")\n",
    "    ax.legend()\n",
    "    ax.set_title(\"Test Case Clustering Visualization for: \" + bootqa_program)\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n"
   ],
   "id": "c3f636fd6c3c18d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## QUBO Problems Definition",
   "id": "b164c120083a967e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def make_linear_terms_bootqa(cluster_test_cases, test_cases_costs, test_cases_rates, alpha):\n",
    "    \"\"\"Making the linear terms of the QUBO\"\"\"\n",
    "    max_cost = max(test_cases_costs)\n",
    "    \n",
    "    estimated_costs = []\n",
    "\n",
    "    #linear coefficients, that are the diagonal of the matrix encoding the QUBO\n",
    "    for test_case in cluster_test_cases:\n",
    "        estimated_costs.append((alpha * ((test_cases_costs[test_case])/max_cost)) - ((1-alpha)*test_cases_rates[test_case]))\n",
    "    \n",
    "    return np.array(estimated_costs)\n",
    "\n",
    "def make_linear_terms_bootqa2(cluster_test_cases, test_cases_costs, pcount, dist, alpha):\n",
    "    \"\"\"Making the linear terms of the QUBO for the elevator2 problem\"\"\"\n",
    "    max_cost = max(test_cases_costs)\n",
    "    max_pcount = max(pcount)\n",
    "    max_dist = max(dist)\n",
    "    \n",
    "    estimated_costs = []\n",
    "\n",
    "    #linear coefficients, that are the diagonal of the matrix encoding the QUBO\n",
    "    for test_case in cluster_test_cases:\n",
    "        estimated_costs.append(((alpha/3) * ((test_cases_costs[test_case])/max_cost)) - ((alpha/3) * ((pcount[test_case])/max_pcount)) - ((alpha/3) * ((dist[test_case])/max_dist)))\n",
    "    \n",
    "    return np.array(estimated_costs)"
   ],
   "id": "846f18771ca2956d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_linear_qubo(linear_terms):\n",
    "    \"\"\"This function is the one that has to encode the QUBO problem that QAOA will have to solve. The QUBO problem specifies the optimization to solve and a quadratic binary unconstrained problem\"\"\"\n",
    "    qubo = QuadraticProgram()\n",
    "    \n",
    "    for i in range(0,len(linear_terms)):\n",
    "        qubo.binary_var('x%s' % (i))\n",
    "\n",
    "    qubo.minimize(linear=linear_terms)\n",
    "\n",
    "    return qubo"
   ],
   "id": "d5209356b015e7dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Purpose of Bootstrap Sampling in QAOA Runtime Analysis\n",
    "\n",
    "Bootstrap sampling is used to estimate the variability and confidence intervals of QAOA execution times when only a limited number of runtime measurements are available. By resampling the observed data with replacement, we can generate an empirical distribution of statistics (e.g., mean or variance), enabling more robust comparisons and uncertainty quantification in performance evaluations.\n"
   ],
   "id": "f512558668c6e8d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def bootstrap_confidence_interval(data, num_samples, confidence_alpha=0.95):\n",
    "    \"\"\"This function determines the statistical range within we would expect the mean value of execution times to fall; it relies on the bootstrapping strategy, which allows the calculation of the confidence interval by repeated sampling (with replacement) from the existing data to obtain an estimate of the confidence interval.\"\"\"\n",
    "    sample_means = []\n",
    "    for _ in range(num_samples):\n",
    "        bootstrap_sample = [random.choice(data) for _ in range(len(data))]\n",
    "        sample_mean = np.mean(bootstrap_sample)\n",
    "        sample_means.append(sample_mean)\n",
    "    \n",
    "    lower_percentile = (1 - confidence_alpha) / 2 * 100\n",
    "    upper_percentile = (confidence_alpha + (1 - confidence_alpha) / 2) * 100\n",
    "    lower_bound = np.percentile(sample_means, lower_percentile)\n",
    "    upper_bound = np.percentile(sample_means, upper_percentile)\n",
    "    \n",
    "    return lower_bound, upper_bound"
   ],
   "id": "5cce857b18a69eed",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9c603089358add45",
   "metadata": {},
   "source": [
    "\n",
    "## Optimal Single-Objective Depth Configuration"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Statevector Simulator",
   "id": "95b138e553222615"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "bootqa_alphas = {\"gsdtsr\": 0.2,\"paintcontrol\": 0.80, \"iofrol\": 0.82, \"elevator\": 0.50, \"elevator2\": 2.9}\n",
    "run_times_dictionary = {\"gsdtsr\": [],\"paintcontrol\": [], \"iofrol\": [], \"elevator\": [], \"elevator2\": []}\n",
    "\n",
    "algorithm_globals.random_seed = 10598\n",
    "backend = Aer.get_backend(\"statevector_simulator\")\n",
    "\n",
    "for bootqa_program in bootqa_programs:\n",
    "    for rep in [1,2,4,8,16]:\n",
    "        qaoa_mes = QAOA(sampler=BackendSampler(backend=backend), optimizer=COBYLA(100), reps=rep)\n",
    "        qaoa = MinimumEigenOptimizer(qaoa_mes)\n",
    "        data = get_data(bootqa_program)\n",
    "        # Total suite metrics\n",
    "        if bootqa_program != \"elevator2\" and bootqa_program != \"elevator\":\n",
    "            test_cases_costs = data[\"time\"].tolist()\n",
    "        else:\n",
    "            test_cases_costs = data[\"cost\"].tolist()\n",
    "        test_cases_effectiveness = None\n",
    "        test_cases_pcount = None\n",
    "        test_cases_dist = None\n",
    "        if bootqa_program != \"elevator2\":\n",
    "            test_cases_effectiveness = data[\"input_div\"].tolist() if bootqa_program == \"elevator\" else data[\"rate\"].tolist()\n",
    "        else:\n",
    "            test_cases_pcount = data[\"pcount\"].tolist()\n",
    "            test_cases_dist = data[\"dist\"].tolist()\n",
    "        \n",
    "        final_test_suite_costs = []\n",
    "        final_effectivenesses = []\n",
    "        final_pcounts = []\n",
    "        final_dists = []\n",
    "        for i in range(experiments):\n",
    "            final_selected_cases = []\n",
    "            cluster_number = 0\n",
    "            for cluster_id in bootqa_clusters[bootqa_program]:\n",
    "                print(\"Cluster: \" + str(bootqa_clusters[bootqa_program][cluster_id]))\n",
    "                linear_terms = None\n",
    "                if bootqa_program != \"elevator2\":\n",
    "                    linear_terms = make_linear_terms_bootqa(bootqa_clusters[bootqa_program][cluster_id], test_cases_costs, test_cases_effectiveness, bootqa_alphas[bootqa_program])\n",
    "                else:\n",
    "                    linear_terms = make_linear_terms_bootqa2(bootqa_clusters[bootqa_program][cluster_id], test_cases_costs,test_cases_pcount, test_cases_dist, bootqa_alphas[bootqa_program])\n",
    "                linear_qubo = create_linear_qubo(linear_terms)\n",
    "                print(\"Linear QUBO: \" + str(linear_qubo))\n",
    "                #for each iteration get the result\n",
    "                s = time.time()\n",
    "                qaoa_result = qaoa.solve(linear_qubo)\n",
    "                e = time.time()\n",
    "                print(\"QAOA Result: \" + str(qaoa_result))\n",
    "                run_times_dictionary[bootqa_program].append((e-s)*1000)\n",
    "                \n",
    "                variable_values = qaoa_result.x\n",
    "                indexes_selected_cases = [index for index, value in enumerate(variable_values) if value == 1]\n",
    "                print(\"Indexes of selected tests to convert. \" + str(indexes_selected_cases))\n",
    "                selected_tests = []\n",
    "                for index in indexes_selected_cases:\n",
    "                    selected_tests.append(bootqa_clusters[bootqa_program][cluster_id][index])\n",
    "                print(\"Selected tests: \" + str(selected_tests))\n",
    "                for selected_test in selected_tests:\n",
    "                    if selected_test not in final_test_suite_costs:\n",
    "                        final_selected_cases.append(selected_test)\n",
    "                \n",
    "            #compute the final test suite cost\n",
    "            final_test_suite_cost = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_test_suite_cost += test_cases_costs[selected_test_case]\n",
    "            final_test_suite_costs.append(final_test_suite_cost)\n",
    "                \n",
    "            #compute the total effectiveness\n",
    "            if bootqa_program != \"elevator2\":\n",
    "                final_effectiveness = 0\n",
    "                for selected_test_case in final_selected_cases:\n",
    "                    final_effectiveness += test_cases_effectiveness[selected_test_case]\n",
    "                final_effectivenesses.append(final_effectiveness)\n",
    "            else:\n",
    "                final_pcount = 0\n",
    "                for selected_test_case in final_selected_cases:\n",
    "                    final_pcount += test_cases_pcount[selected_test_case]\n",
    "                final_pcounts.append(final_pcount)\n",
    "                \n",
    "                final_dist = 0\n",
    "                for selected_test_case in final_selected_cases:\n",
    "                    final_dist += test_cases_dist[selected_test_case]\n",
    "                final_dists.append(final_dist)\n",
    "        \n",
    "        print(\"Final Test Suite: \" + str(final_selected_cases))\n",
    "        #compute the qpu access times\n",
    "        qpu_run_times_without_zeros = []\n",
    "        for access_time in run_times_dictionary[bootqa_program]:\n",
    "          if access_time != 0:\n",
    "            qpu_run_times_without_zeros.append(access_time)\n",
    "        lower_bound, upper_bound = bootstrap_confidence_interval(qpu_run_times_without_zeros, 1000, 0.95)\n",
    "        for i in range(len(run_times_dictionary[bootqa_program])):\n",
    "          if run_times_dictionary[bootqa_program][i] == 0:\n",
    "              run_times_dictionary[bootqa_program][i] = upper_bound\n",
    "        average_qpu_access_time = statistics.mean(run_times_dictionary[bootqa_program]) \n",
    "        \n",
    "        if bootqa_program == \"elevator2\":\n",
    "            var_names = [\"final_test_suite_costs\", \"final_pcounts\", \"final_dists\",\n",
    "                     \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                     \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "            values = [final_test_suite_costs, final_pcounts, final_dists, average_qpu_access_time, \n",
    "                      statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                      lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "        else:\n",
    "            var_names = [\"final_test_suite_costs\", \"final_effectivenesses\",\n",
    "                         \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                         \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "            values = [final_test_suite_costs, final_effectivenesses, average_qpu_access_time, \n",
    "                      statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                      lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "        \n",
    "        # Ensure the directory exists\n",
    "        output_dir = \"results/selectqaoa/statevector_sim\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Path to save the file\n",
    "        file_path = os.path.join(output_dir, f\"{bootqa_program}-rep-{rep}.csv\")\n",
    "        \n",
    "        # Writing results to the file\n",
    "        with open(file_path, \"w\", newline=\"\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(var_names)\n",
    "            writer.writerow(values)\n",
    "        print(f\"Results saved to {file_path}\")"
   ],
   "id": "1dc0f4aa-da57-42e3-9dc2-de9f8089400c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bootqa_alphas = {\"gsdtsr\": 0.2,\"paintcontrol\": 0.80, \"iofrol\": 0.82, \"elevator\": 0.50, \"elevator2\": 2.9}\n",
    "min_f_vals = {\"gsdtsr\":None,\"paintcontrol\":None,\"iofrol\":None,\"elevator\":None, \"elevator2\":None}\n",
    "normalized_fvals = {}\n",
    "normality_results = {}\n",
    "\n",
    "for bootqa_program in bootqa_programs:\n",
    "    print(\"Dataset:\", bootqa_program)\n",
    "    \n",
    "    # Load files\n",
    "    file_paths = {\n",
    "        1: f\"./results/selectqaoa/statevector_sim/{bootqa_program}-rep-1.csv\",\n",
    "        2: f\"./results/selectqaoa/statevector_sim/{bootqa_program}-rep-2.csv\",\n",
    "        4: f\"./results/selectqaoa/statevector_sim/{bootqa_program}-rep-4.csv\",\n",
    "        8: f\"./results/selectqaoa/statevector_sim/{bootqa_program}-rep-8.csv\",\n",
    "        16: f\"./results/selectqaoa/statevector_sim/{bootqa_program}-rep-16.csv\"\n",
    "    }\n",
    "\n",
    "    dfs = {k: pd.read_csv(v) for k, v in file_paths.items()}\n",
    "    \n",
    "    # Parse results\n",
    "    def extract_column(col):\n",
    "        return {k: ast.literal_eval(df[col].iloc[-1]) for k, df in dfs.items()}\n",
    "    \n",
    "    cost_dict = extract_column('final_test_suite_costs')\n",
    "    \n",
    "    if bootqa_program != \"elevator2\":\n",
    "        metric_dicts = {\n",
    "            \"final_test_suite_costs\": cost_dict,\n",
    "            \"final_effectivenesses\": extract_column('final_effectivenesses')\n",
    "        }\n",
    "    else:\n",
    "        metric_dicts = {\n",
    "            \"final_test_suite_costs\": cost_dict,\n",
    "            \"final_pcounts\": extract_column('final_pcounts'),\n",
    "            \"final_dists\": extract_column('final_dists')\n",
    "        }\n",
    "    \n",
    "    alpha = bootqa_alphas[bootqa_program]\n",
    "    program_fvals = {}\n",
    "\n",
    "    all_f_vals = []\n",
    "\n",
    "    # First: compute all f_vals and store them to later get the global min\n",
    "    temp_fvals = {}\n",
    "    for version in [1, 2, 4, 8, 16]:\n",
    "        #print(\"Version: \" + str(version))\n",
    "        version_fvals = []\n",
    "        for i in range(10):\n",
    "            #print(\"Experiment: \" + str(i))\n",
    "            if bootqa_program != \"elevator2\":\n",
    "                cost = metric_dicts[\"final_test_suite_costs\"][version][i]\n",
    "                fail_rate = metric_dicts[\"final_effectivenesses\"][version][i]\n",
    "                #print(\"cost: \" + str(cost))\n",
    "                #print(\"fail rate: \" + str(fail_rate))\n",
    "                f_val = alpha * cost - (1 - alpha) * fail_rate\n",
    "                #print(\"f_val: \" + str(f_val))\n",
    "            else:\n",
    "                cost = metric_dicts[\"final_test_suite_costs\"][version][i]\n",
    "                pcount = metric_dicts[\"final_pcounts\"][version][i]\n",
    "                dist = metric_dicts[\"final_dists\"][version][i]\n",
    "                #print(\"cost: \" + str(cost))\n",
    "                #print(\"pcounts: \" + str(pcount))\n",
    "                #print(\"final_dists: \" + str(dist))\n",
    "                f_val = (alpha / 3) * (cost - pcount - dist)\n",
    "                #print(\"f_val: \" + str(f_val))\n",
    "\n",
    "            version_fvals.append(f_val)\n",
    "            all_f_vals.append(f_val)\n",
    "        temp_fvals[version] = version_fvals\n",
    "\n",
    "    min_f_val = min(all_f_vals)\n",
    "    print(\"MIN:\" + str(min_f_val))\n",
    "    \n",
    "    program_normality = {}\n",
    "    # Now normalize\n",
    "    for version in [1, 2, 4, 8, 16]:\n",
    "        program_fvals[str(version)] = {}\n",
    "        norm_vals = []\n",
    "        for i, f_val in enumerate(temp_fvals[version]):\n",
    "            normalized = f_val / min_f_val\n",
    "            program_fvals[str(version)][str(i)] = normalized\n",
    "            norm_vals.append(normalized)\n",
    "            \n",
    "        # Test di normalità\n",
    "        #print(norm_vals)\n",
    "        stat, p_value = shapiro(norm_vals)\n",
    "        is_normal = p_value > 0.05\n",
    "        program_normality[str(version)] = {\n",
    "            \"p_value\": round(p_value, 4),\n",
    "            \"is_normal\": is_normal\n",
    "        }\n",
    "        print(f\"{bootqa_program} | rep={version} | p={p_value:.4f} | Normal? {'Yes' if is_normal else 'No'}\")\n",
    "\n",
    "    normalized_fvals[bootqa_program] = program_fvals\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"normalized_fvals.json\", \"w\") as f:\n",
    "    json.dump(normalized_fvals, f, indent=2)\n"
   ],
   "id": "b3596b36f9770ce3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "GSDTSR BEST rep = 1\n",
    "PaintControl BEST rep = 1\n",
    "IOF/ROL BEST rep = 2!\n",
    "ELEVATOR BEST rep = 4!\n",
    "ELEVATOR2 BEST rep = 4!\n",
    "\"\"\""
   ],
   "id": "5ed4509200ef0601",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Aer Simulator",
   "id": "b09a4c3dcbabe428"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bootqa_alphas = {\"gsdtsr\": 0.2,\"paintcontrol\": 0.80, \"iofrol\": 0.82, \"elevator\": 0.50, \"elevator2\": 2.9}\n",
    "run_times_dictionary = {\"gsdtsr\": [],\"paintcontrol\": [], \"iofrol\": [], \"elevator\": [], \"elevator2\": []}\n",
    "\n",
    "sim_ideal = AerSimulator()\n",
    "algorithm_globals.random_seed = 10598\n",
    "\n",
    "for bootqa_program in bootqa_programs:\n",
    "    qaoa_mes = QAOA(sampler=BackendSampler(backend=sim_ideal), optimizer=COBYLA(100), reps=bootqa_programs_rep_values[bootqa_program])\n",
    "    qaoa = MinimumEigenOptimizer(qaoa_mes)  # using QAOA\n",
    "    data = get_data(bootqa_program)\n",
    "    # Total suite metrics\n",
    "    if bootqa_program != \"elevator2\" and bootqa_program != \"elevator\":\n",
    "        test_cases_costs = data[\"time\"].tolist()\n",
    "    else:\n",
    "        test_cases_costs = data[\"cost\"].tolist()\n",
    "    test_cases_rates = None\n",
    "    test_cases_pcount = None\n",
    "    test_cases_dist = None\n",
    "    if bootqa_program != \"elevator2\":\n",
    "        test_cases_rates = data[\"input_div\"].tolist() if bootqa_program == \"elevator\" else data[\"rate\"].tolist()\n",
    "    else:\n",
    "        test_cases_pcount = data[\"pcount\"].tolist()\n",
    "        test_cases_dist = data[\"dist\"].tolist()\n",
    "    \n",
    "    final_test_suite_costs = []\n",
    "    final_effectivenesses = []\n",
    "    final_pcounts = []\n",
    "    final_dists = []\n",
    "    for i in range(experiments):\n",
    "        final_selected_cases = []\n",
    "        cluster_number = 0\n",
    "        for cluster_id in bootqa_clusters[bootqa_program]:\n",
    "            print(\"Cluster: \" + str(bootqa_clusters[bootqa_program][cluster_id]))\n",
    "            linear_terms = None\n",
    "            if bootqa_program != \"elevator2\":\n",
    "                linear_terms = make_linear_terms_bootqa(bootqa_clusters[bootqa_program][cluster_id], test_cases_costs, test_cases_rates, bootqa_alphas[bootqa_program])\n",
    "            else:\n",
    "                linear_terms = make_linear_terms_bootqa2(bootqa_clusters[bootqa_program][cluster_id], test_cases_costs,test_cases_pcount, test_cases_dist, bootqa_alphas[bootqa_program])\n",
    "            linear_qubo = create_linear_qubo(linear_terms)\n",
    "            print(\"Linear QUBO: \" + str(linear_qubo))\n",
    "            #for each iteration get the result\n",
    "            s = time.time()\n",
    "            qaoa_result = qaoa.solve(linear_qubo)\n",
    "            e = time.time()\n",
    "            print(\"QAOA Result: \" + str(qaoa_result))\n",
    "            run_times_dictionary[bootqa_program].append((e-s)*1000)\n",
    "            \n",
    "            variable_values = qaoa_result.x\n",
    "            indexes_selected_cases = [index for index, value in enumerate(variable_values) if value == 1]\n",
    "            print(\"Indexes of selected tests to convert. \" + str(indexes_selected_cases))\n",
    "            selected_tests = []\n",
    "            for index in indexes_selected_cases:\n",
    "                selected_tests.append(bootqa_clusters[bootqa_program][cluster_id][index])\n",
    "            print(\"Selected tests: \" + str(selected_tests))\n",
    "            for selected_test in selected_tests:\n",
    "                if selected_test not in final_test_suite_costs:\n",
    "                    final_selected_cases.append(selected_test)\n",
    "            \n",
    "        #compute the final test suite cost\n",
    "        final_test_suite_cost = 0\n",
    "        for selected_test_case in final_selected_cases:\n",
    "            final_test_suite_cost += test_cases_costs[selected_test_case]\n",
    "        final_test_suite_costs.append(final_test_suite_cost)\n",
    "            \n",
    "        #compute the total failure rate\n",
    "        if bootqa_program != \"elevator2\":\n",
    "            final_effectiveness = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_effectiveness += test_cases_rates[selected_test_case]\n",
    "            final_effectivenesses.append(final_effectiveness)\n",
    "        else:\n",
    "            final_pcount = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_pcount += test_cases_pcount[selected_test_case]\n",
    "            final_pcounts.append(final_pcount)\n",
    "            \n",
    "            final_dist = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_dist += test_cases_dist[selected_test_case]\n",
    "            final_dists.append(final_dist)\n",
    "    \n",
    "    print(\"Final Test Suite: \" + str(final_selected_cases))\n",
    "    #compute the qpu access times\n",
    "    qpu_run_times_without_zeros = []\n",
    "    for access_time in run_times_dictionary[bootqa_program]:\n",
    "      if access_time != 0:\n",
    "        qpu_run_times_without_zeros.append(access_time)\n",
    "    lower_bound, upper_bound = bootstrap_confidence_interval(qpu_run_times_without_zeros, 1000, 0.95)\n",
    "    for i in range(len(run_times_dictionary[bootqa_program])):\n",
    "      if run_times_dictionary[bootqa_program][i] == 0:\n",
    "          run_times_dictionary[bootqa_program][i] = upper_bound\n",
    "    average_qpu_access_time = statistics.mean(run_times_dictionary[bootqa_program]) \n",
    "    \n",
    "    if bootqa_program == \"elevator2\":\n",
    "        var_names = [\"final_test_suite_costs\", \"final_pcounts\", \"final_dists\",\n",
    "                 \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                 \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_pcounts, final_dists, average_qpu_access_time, \n",
    "                  statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                  lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "    else:\n",
    "        var_names = [\"final_test_suite_costs\", \"final_effectivenesses\",\n",
    "                     \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                     \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_effectivenesses, average_qpu_access_time, \n",
    "                  statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                  lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    output_dir = \"results/selectqaoa/aer_sim\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Path to save the file\n",
    "    file_path = os.path.join(output_dir, f\"{bootqa_program}.csv\")\n",
    "    \n",
    "    # Writing results to the file\n",
    "    with open(file_path, \"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(var_names)\n",
    "        writer.writerow(values)\n",
    "    print(f\"Results saved to {file_path}\")"
   ],
   "id": "12bc23d05ffe997c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "61af3d3afde081b",
   "metadata": {},
   "source": [
    "## Fake Vigo Noise Simulator"
   ]
  },
  {
   "cell_type": "code",
   "id": "edab03cf4dfcc5f6",
   "metadata": {},
   "source": [
    "bootqa_alphas = {\"gsdtsr\": 0.2,\"paintcontrol\": 0.80, \"iofrol\": 0.82, \"elevator\": 0.50, \"elevator2\": 2.9}\n",
    "run_times_dictionary = {\"gsdtsr\": [],\"paintcontrol\": [], \"iofrol\": [], \"elevator\": [], \"elevator2\": []}\n",
    "\n",
    "device_backend = FakeVigoV2()\n",
    "\n",
    "algorithm_globals.random_seed = 10598\n",
    "\n",
    "for bootqa_program in bootqa_programs:\n",
    "    qaoa_mes = QAOA(sampler=BackendSampler(backend=device_backend), optimizer=COBYLA(100), reps=bootqa_programs_rep_values[bootqa_program])\n",
    "    qaoa = MinimumEigenOptimizer(qaoa_mes)  # using QAOA\n",
    "    data = get_data(bootqa_program)\n",
    "    # Total suite metrics\n",
    "    if bootqa_program != \"elevator2\" and bootqa_program != \"elevator\":\n",
    "        test_cases_costs = data[\"time\"].tolist()\n",
    "    else:\n",
    "        test_cases_costs = data[\"cost\"].tolist()\n",
    "    test_cases_rates = None\n",
    "    test_cases_pcount = None\n",
    "    test_cases_dist = None\n",
    "    if bootqa_program != \"elevator2\":\n",
    "        test_cases_rates = data[\"input_div\"].tolist() if bootqa_program == \"elevator\" else data[\"rate\"].tolist()\n",
    "    else:\n",
    "        test_cases_pcount = data[\"pcount\"].tolist()\n",
    "        test_cases_dist = data[\"dist\"].tolist()\n",
    "    \n",
    "    final_test_suite_costs = []\n",
    "    final_effectivenesses = []\n",
    "    final_pcounts = []\n",
    "    final_dists = []\n",
    "    for i in range(experiments):\n",
    "        final_selected_cases = []\n",
    "        cluster_number = 0\n",
    "        for cluster_id in bootqa_clusters[bootqa_program]:\n",
    "            print(\"Cluster: \" + str(bootqa_clusters[bootqa_program][cluster_id]))\n",
    "            linear_terms = None\n",
    "            if bootqa_program != \"elevator2\":\n",
    "                linear_terms = make_linear_terms_bootqa(bootqa_clusters[bootqa_program][cluster_id], test_cases_costs, test_cases_rates, bootqa_alphas[bootqa_program])\n",
    "            else:\n",
    "                linear_terms = make_linear_terms_bootqa2(bootqa_clusters[bootqa_program][cluster_id], test_cases_costs,test_cases_pcount, test_cases_dist, bootqa_alphas[bootqa_program])\n",
    "            linear_qubo = create_linear_qubo(linear_terms)\n",
    "            print(\"Linear QUBO: \" + str(linear_qubo))\n",
    "            #for each iteration get the result\n",
    "            s = time.time()\n",
    "            qaoa_result = qaoa.solve(linear_qubo)\n",
    "            e = time.time()\n",
    "            print(\"Program: \" + str(bootqa_program) + \". It: \" + str(i))\n",
    "            print(\"QAOA Result: \" + str(qaoa_result))\n",
    "            run_times_dictionary[bootqa_program].append((e-s)*1000)\n",
    "            \n",
    "            variable_values = qaoa_result.x\n",
    "            indexes_selected_cases = [index for index, value in enumerate(variable_values) if value == 1]\n",
    "            print(\"Indexes of selected tests to convert. \" + str(indexes_selected_cases))\n",
    "            selected_tests = []\n",
    "            for index in indexes_selected_cases:\n",
    "                selected_tests.append(bootqa_clusters[bootqa_program][cluster_id][index])\n",
    "            print(\"Selected tests: \" + str(selected_tests))\n",
    "            for selected_test in selected_tests:\n",
    "                if selected_test not in final_test_suite_costs:\n",
    "                    final_selected_cases.append(selected_test)\n",
    "            \n",
    "        #compute the final test suite cost\n",
    "        final_test_suite_cost = 0\n",
    "        for selected_test_case in final_selected_cases:\n",
    "            final_test_suite_cost += test_cases_costs[selected_test_case]\n",
    "        final_test_suite_costs.append(final_test_suite_cost)\n",
    "            \n",
    "        #compute the total failure rate\n",
    "        if bootqa_program != \"elevator2\":\n",
    "            final_effectiveness = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_effectiveness += test_cases_rates[selected_test_case]\n",
    "            final_effectivenesses.append(final_effectiveness)\n",
    "        else:\n",
    "            final_pcount = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_pcount += test_cases_pcount[selected_test_case]\n",
    "            final_pcounts.append(final_pcount)\n",
    "            \n",
    "            final_dist = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_dist += test_cases_dist[selected_test_case]\n",
    "            final_dists.append(final_dist)\n",
    "    \n",
    "    print(\"Final Test Suite: \" + str(final_selected_cases))\n",
    "    #compute the qpu access times\n",
    "    qpu_run_times_without_zeros = []\n",
    "    for access_time in run_times_dictionary[bootqa_program]:\n",
    "      if access_time != 0:\n",
    "        qpu_run_times_without_zeros.append(access_time)\n",
    "    lower_bound, upper_bound = bootstrap_confidence_interval(qpu_run_times_without_zeros, 1000, 0.95)\n",
    "    for i in range(len(run_times_dictionary[bootqa_program])):\n",
    "      if run_times_dictionary[bootqa_program][i] == 0:\n",
    "          run_times_dictionary[bootqa_program][i] = upper_bound\n",
    "    average_qpu_access_time = statistics.mean(run_times_dictionary[bootqa_program]) \n",
    "    \n",
    "    if bootqa_program == \"elevator2\":\n",
    "        var_names = [\"final_test_suite_costs\", \"final_pcounts\", \"final_dists\",\n",
    "                 \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                 \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_pcounts, final_dists, average_qpu_access_time, \n",
    "                  statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                  lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "    else:\n",
    "        var_names = [\"final_test_suite_costs\", \"final_effectivenesses\",\n",
    "                     \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                     \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_effectivenesses, average_qpu_access_time, \n",
    "                  statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                  lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    output_dir = \"results/selectqaoa/fake_vigo\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Path to save the file\n",
    "    file_path = os.path.join(output_dir, f\"{bootqa_program}.csv\")\n",
    "    \n",
    "    # Writing results to the file\n",
    "    with open(file_path, \"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(var_names)\n",
    "        writer.writerow(values)\n",
    "    print(f\"Results saved to {file_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Noise Mitigation in Quantum Computing\n",
    "\n",
    "### Zero-Noise Extrapolation (ZNE)"
   ],
   "id": "174895b87be84b7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def expectation_from_distribution(distribution, operator):\n",
    "    \"\"\"Calcola ⟨H⟩ da distribuzione e SparsePauliOp (solo Z/I).\"\"\"\n",
    "    expectation = 0.0\n",
    "    for pauli, coeff in zip(operator.paulis, operator.coeffs):\n",
    "        pauli_str = pauli.to_label()\n",
    "        if any(p in pauli_str for p in \"XY\"):  # ignora se contiene X o Y\n",
    "            continue\n",
    "        for bitstring, prob in distribution.items():\n",
    "            if len(pauli_str) < len(bitstring):\n",
    "                pauli_str = pauli_str.rjust(len(bitstring), \"I\")\n",
    "            parity = 1\n",
    "            for i in range(len(bitstring)):\n",
    "                p = pauli_str[-(i + 1)]  # allineato da destra (qubit 0 a dx)\n",
    "                if p == 'Z':\n",
    "                    parity *= 1 if bitstring[-(i + 1)] == '0' else -1\n",
    "            expectation += coeff.real * prob * parity\n",
    "    return expectation.real\n",
    "\n",
    "# Config\n",
    "bootqa_alphas = {\"gsdtsr\": 0.2, \"paintcontrol\": 0.80, \"iofrol\": 0.82, \"elevator\": 0.50, \"elevator2\": 2.9}\n",
    "run_times_dictionary = {k: [] for k in bootqa_alphas}\n",
    "algorithm_globals.random_seed = 10598\n",
    "\n",
    "scale_factors = [1.0, 3.0, 5.0]\n",
    "factory = RichardsonFactory(scale_factors=scale_factors)\n",
    "\n",
    "for bootqa_program in bootqa_programs:\n",
    "    data = get_data(bootqa_program)\n",
    "    test_cases_costs = data[\"time\"].tolist() if bootqa_program not in [\"elevator\", \"elevator2\"] else data[\"cost\"].tolist()\n",
    "    if bootqa_program == \"elevator\":\n",
    "        test_cases_rates = data[\"input_div\"].tolist()\n",
    "    elif bootqa_program != \"elevator2\":\n",
    "        test_cases_rates = data[\"rate\"].tolist()\n",
    "    else:\n",
    "        test_cases_pcount = data[\"pcount\"].tolist()\n",
    "        test_cases_dist = data[\"dist\"].tolist()\n",
    "\n",
    "    final_test_suite_costs = []\n",
    "    final_effectivenesses = []\n",
    "    final_pcounts = []\n",
    "    final_dists = []\n",
    "\n",
    "    for i in range(experiments):\n",
    "        final_selected_cases = []\n",
    "\n",
    "        for cluster_id in bootqa_clusters[bootqa_program]:\n",
    "            cluster = bootqa_clusters[bootqa_program][cluster_id]\n",
    "            print(\"Cluster: \" + str(bootqa_clusters[bootqa_program][cluster_id]))\n",
    "\n",
    "            # QUBO creation\n",
    "            if bootqa_program != \"elevator2\":\n",
    "                linear_terms = make_linear_terms_bootqa(cluster, test_cases_costs, test_cases_rates, bootqa_alphas[bootqa_program])\n",
    "            else:\n",
    "                linear_terms = make_linear_terms_bootqa2(cluster, test_cases_costs, test_cases_pcount, test_cases_dist, bootqa_alphas[bootqa_program])\n",
    "            linear_qubo = create_linear_qubo(linear_terms)\n",
    "            print(\"Linear QUBO: \" + str(linear_qubo))\n",
    "\n",
    "            # QAOA to build the circuit needs an Ising Hamiltonian as input\n",
    "            # Since the problem is a QuadraticProgram object, we must convert it into an Ising\n",
    "            qp2qubo = QuadraticProgramToQubo()\n",
    "            qubo_problem = qp2qubo.convert(linear_qubo)\n",
    "            hamiltonian, _ = qubo_problem.to_ising()\n",
    "            backend = FakeVigoV2()\n",
    "\n",
    "            # QAOAAnsatz builds a circuit with defined repetitions \n",
    "            reps = bootqa_programs_rep_values[bootqa_program]\n",
    "            ansatz = QAOAAnsatz(hamiltonian, reps=reps)\n",
    "\n",
    "            # Returns the 0 noise expected value of H with given params\n",
    "            def zne_expectation(params):\n",
    "                # Assigns params to γ and β (2*reps params)\n",
    "                circuit = ansatz.assign_parameters(params)\n",
    "                circuit.measure_all() # the circuit will produce a result in terms of bitstrings\n",
    "            \n",
    "                # Running the circuit on FakeVigo, getting the expected value for Hamiltonian \n",
    "                def run_expectation(circ):\n",
    "                    sampler = BackendSampler(backend=device_backend)\n",
    "                    result = sampler.run(circ).result()\n",
    "                    dist = result.quasi_dists[0].binary_probabilities()\n",
    "                    return expectation_from_distribution(dist, hamiltonian)\n",
    "            \n",
    "                # Running the the circuit at different noise levels, to extrapolate 0 noise H exp value\n",
    "                factory = RichardsonFactory(scale_factors=[1.0, 3.0, 5.0])\n",
    "                return zne.execute_with_zne(circuit, run_expectation, scale_noise=fold_global, factory=factory)\n",
    "            \n",
    "            # Initialize params\n",
    "            initial_params = np.array([0.5] * ansatz.num_parameters)\n",
    "            # Tries different params, calling for any trial zne_expectation, updates params based on H expected value retuned any time, until it converges\n",
    "            result = minimize(zne_expectation, initial_params, method=\"COBYLA\")\n",
    "            \n",
    "            # Get the optimal params\n",
    "            optimal_params = result.x\n",
    "                \n",
    "            # Build the final circuit with the optimal params\n",
    "            final_circuit = ansatz.assign_parameters(optimal_params)\n",
    "            final_circuit.measure_all()\n",
    "            \n",
    "            # Execute the final circuite to get the final result\n",
    "            sampler = BackendSampler(backend=device_backend)\n",
    "            start = time.time()\n",
    "            final_result = sampler.run(final_circuit).result()\n",
    "            end = time.time()\n",
    "            counts = final_result.quasi_dists[0].binary_probabilities()\n",
    "            best_bitstring = max(counts.items(), key=lambda x: x[1])[0]\n",
    "            \n",
    "            # Ora puoi estrarre gli indici dei test selezionati\n",
    "            selected_tests = [cluster[i] for i, b in enumerate(best_bitstring[::-1]) if b == \"1\"]\n",
    "            print(f\"[{bootqa_program}] Iter {i} - Best bitstring: {best_bitstring} -> {selected_tests}\")\n",
    "\n",
    "            for test in selected_tests:\n",
    "                if test not in final_selected_cases:\n",
    "                    final_selected_cases.append(test)\n",
    "\n",
    "            run_times_dictionary[bootqa_program].append((end-start)*1000)\n",
    "\n",
    "        cost = sum(test_cases_costs[t] for t in final_selected_cases)\n",
    "        final_test_suite_costs.append(cost)\n",
    "\n",
    "        if bootqa_program != \"elevator2\":\n",
    "            effectiveness = sum(test_cases_rates[t] for t in final_selected_cases)\n",
    "            final_effectivenesses.append(effectiveness)\n",
    "        else:\n",
    "            pcount = sum(test_cases_pcount[t] for t in final_selected_cases)\n",
    "            dist = sum(test_cases_dist[t] for t in final_selected_cases)\n",
    "            final_pcounts.append(pcount)\n",
    "            final_dists.append(dist)\n",
    "\n",
    "    avg_time = statistics.mean(run_times_dictionary[bootqa_program]) \n",
    "    std_time = statistics.stdev(run_times_dictionary[bootqa_program]) \n",
    "\n",
    "    # Export results\n",
    "    if bootqa_program == \"elevator2\":\n",
    "        var_names = [\"final_test_suite_costs\", \"final_pcounts\", \"final_dists\",\n",
    "                     \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\",\n",
    "                     \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_pcounts, final_dists, avg_time,\n",
    "                  std_time, run_times_dictionary[bootqa_program]]\n",
    "    else:\n",
    "        var_names = [\"final_test_suite_costs\", \"final_effectivenesses\",\n",
    "                     \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\",\n",
    "                     \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_effectivenesses, avg_time,\n",
    "                  std_time, run_times_dictionary[bootqa_program]]\n",
    "\n",
    "    os.makedirs(\"results/selectqaoa/fake_vigo_zne\", exist_ok=True)\n",
    "    file_path = f\"results/selectqaoa/fake_vigo_zne/{bootqa_program}.csv\"\n",
    "    with open(file_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(var_names)\n",
    "        writer.writerow(values)\n",
    "\n",
    "    print(f\"Results saved to {file_path}\")\n"
   ],
   "id": "38b0e91658ed978b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bdaac2eb4df95a77",
   "metadata": {},
   "source": [
    "## Depolarizing Error Simulator"
   ]
  },
  {
   "cell_type": "code",
   "id": "59773f7ef37eb27f",
   "metadata": {},
   "source": [
    "bootqa_alphas = {\"gsdtsr\": 0.2,\"paintcontrol\": 0.80, \"iofrol\": 0.82, \"elevator\": 0.50, \"elevator2\": 2.9}\n",
    "run_times_dictionary = {\"gsdtsr\": [],\"paintcontrol\": [], \"iofrol\": [], \"elevator\": [], \"elevator2\": []}\n",
    "\n",
    "noise_dep = NoiseModel()\n",
    "\n",
    "# Define a 1% depolarizing error for single-qubit and two-qubit gates\n",
    "depolarizing_error_1q = depolarizing_error(0.01, 1)  # 1% depolarizing error for single-qubit gates\n",
    "depolarizing_error_2q = depolarizing_error(0.01, 2)  # 1% depolarizing error for two-qubit gates\n",
    "\n",
    "# Apply depolarizing noise to single-qubit gates\n",
    "noise_dep.add_all_qubit_quantum_error(depolarizing_error_1q, ['rx', 'rz', 'sx', 'x', 'h'])\n",
    "\n",
    "# Apply depolarizing noise to two-qubit gates\n",
    "noise_dep.add_all_qubit_quantum_error(depolarizing_error_2q, ['cx', 'cz', 'swap'])\n",
    "\n",
    "sim_dep = AerSimulator(noise_model=noise_dep)\n",
    "\n",
    "algorithm_globals.random_seed = 10598\n",
    "\n",
    "for bootqa_program in bootqa_programs:\n",
    "    qaoa_mes = QAOA(sampler=BackendSampler(backend=sim_dep), optimizer=COBYLA(100), reps=bootqa_programs_rep_values[bootqa_program])\n",
    "    qaoa = MinimumEigenOptimizer(qaoa_mes)  # using QAOA\n",
    "    data = get_data(bootqa_program)\n",
    "    # Total suite metrics\n",
    "    if bootqa_program != \"elevator2\" and bootqa_program != \"elevator\":\n",
    "        test_cases_costs = data[\"time\"].tolist()\n",
    "    else:\n",
    "        test_cases_costs = data[\"cost\"].tolist()\n",
    "    test_cases_rates = None\n",
    "    test_cases_pcount = None\n",
    "    test_cases_dist = None\n",
    "    if bootqa_program != \"elevator2\":\n",
    "        test_cases_rates = data[\"input_div\"].tolist() if bootqa_program == \"elevator\" else data[\"rate\"].tolist()\n",
    "    else:\n",
    "        test_cases_pcount = data[\"pcount\"].tolist()\n",
    "        test_cases_dist = data[\"dist\"].tolist()\n",
    "    \n",
    "    final_test_suite_costs = []\n",
    "    final_effectivenesses = []\n",
    "    final_pcounts = []\n",
    "    final_dists = []\n",
    "    for i in range(experiments):\n",
    "        final_selected_cases = []\n",
    "        cluster_number = 0\n",
    "        for cluster_id in bootqa_clusters[bootqa_program]:\n",
    "            print(\"Cluster: \" + str(bootqa_clusters[bootqa_program][cluster_id]))\n",
    "            linear_terms = None\n",
    "            if bootqa_program != \"elevator2\":\n",
    "                linear_terms = make_linear_terms_bootqa(bootqa_clusters[bootqa_program][cluster_id], test_cases_costs, test_cases_rates, bootqa_alphas[bootqa_program])\n",
    "            else:\n",
    "                linear_terms = make_linear_terms_bootqa2(bootqa_clusters[bootqa_program][cluster_id], test_cases_costs,test_cases_pcount, test_cases_dist, bootqa_alphas[bootqa_program])\n",
    "            linear_qubo = create_linear_qubo(linear_terms)\n",
    "            print(\"Linear QUBO: \" + str(linear_qubo))\n",
    "            #for each iteration get the result\n",
    "            s = time.time()\n",
    "            qaoa_result = qaoa.solve(linear_qubo)\n",
    "            e = time.time()\n",
    "            print(\"Program: \" + str(bootqa_program) + \". It: \" + str(i))\n",
    "            print(\"QAOA Result: \" + str(qaoa_result))\n",
    "            run_times_dictionary[bootqa_program].append((e-s)*1000)\n",
    "            \n",
    "            variable_values = qaoa_result.x\n",
    "            indexes_selected_cases = [index for index, value in enumerate(variable_values) if value == 1]\n",
    "            print(\"Indexes of selected tests to convert. \" + str(indexes_selected_cases))\n",
    "            selected_tests = []\n",
    "            for index in indexes_selected_cases:\n",
    "                selected_tests.append(bootqa_clusters[bootqa_program][cluster_id][index])\n",
    "            print(\"Selected tests: \" + str(selected_tests))\n",
    "            for selected_test in selected_tests:\n",
    "                if selected_test not in final_test_suite_costs:\n",
    "                    final_selected_cases.append(selected_test)\n",
    "            \n",
    "        #compute the final test suite cost\n",
    "        final_test_suite_cost = 0\n",
    "        for selected_test_case in final_selected_cases:\n",
    "            final_test_suite_cost += test_cases_costs[selected_test_case]\n",
    "        final_test_suite_costs.append(final_test_suite_cost)\n",
    "            \n",
    "        #compute the total failure rate\n",
    "        if bootqa_program != \"elevator2\":\n",
    "            final_effectiveness = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_effectiveness += test_cases_rates[selected_test_case]\n",
    "            final_effectivenesses.append(final_effectiveness)\n",
    "        else:\n",
    "            final_pcount = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_pcount += test_cases_pcount[selected_test_case]\n",
    "            final_pcounts.append(final_pcount)\n",
    "            \n",
    "            final_dist = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_dist += test_cases_dist[selected_test_case]\n",
    "            final_dists.append(final_dist)\n",
    "    \n",
    "    print(\"Final Test Suite: \" + str(final_selected_cases))\n",
    "    #compute the qpu access times\n",
    "    qpu_run_times_without_zeros = []\n",
    "    for access_time in run_times_dictionary[bootqa_program]:\n",
    "      if access_time != 0:\n",
    "        qpu_run_times_without_zeros.append(access_time)\n",
    "    lower_bound, upper_bound = bootstrap_confidence_interval(qpu_run_times_without_zeros, 1000, 0.95)\n",
    "    for i in range(len(run_times_dictionary[bootqa_program])):\n",
    "      if run_times_dictionary[bootqa_program][i] == 0:\n",
    "          run_times_dictionary[bootqa_program][i] = upper_bound\n",
    "    average_qpu_access_time = statistics.mean(run_times_dictionary[bootqa_program]) \n",
    "    \n",
    "    if bootqa_program == \"elevator2\":\n",
    "        var_names = [\"final_test_suite_costs\", \"final_pcounts\", \"final_dists\",\n",
    "                 \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                 \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_pcounts, final_dists, average_qpu_access_time, \n",
    "                  statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                  lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "    else:\n",
    "        var_names = [\"final_test_suite_costs\", \"final_effectivenesses\",\n",
    "                     \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                     \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_effectivenesses, average_qpu_access_time, \n",
    "                  statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                  lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    output_dir = \"results/selectqaoa/depolarizing_sim/01\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Path to save the file\n",
    "    file_path = os.path.join(output_dir, f\"{bootqa_program}.csv\")\n",
    "    \n",
    "    # Writing results to the file\n",
    "    with open(file_path, \"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(var_names)\n",
    "        writer.writerow(values)\n",
    "    print(f\"Results saved to {file_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1866e5d1144d73d7",
   "metadata": {},
   "source": [
    "bootqa_alphas = {\"gsdtsr\": 0.2,\"paintcontrol\": 0.80, \"iofrol\": 0.82, \"elevator\": 0.50, \"elevator2\": 2.9}\n",
    "run_times_dictionary = {\"gsdtsr\": [],\"paintcontrol\": [], \"iofrol\": [], \"elevator\": [], \"elevator2\": []}\n",
    "\n",
    "noise_dep = NoiseModel()\n",
    "\n",
    "# Define a 2% depolarizing error for single-qubit and two-qubit gates\n",
    "depolarizing_error_1q = depolarizing_error(0.02, 1)  # 2% depolarizing error for single-qubit gates\n",
    "depolarizing_error_2q = depolarizing_error(0.02, 2)  # 2% depolarizing error for two-qubit gates\n",
    "\n",
    "# Apply depolarizing noise to single-qubit gates\n",
    "noise_dep.add_all_qubit_quantum_error(depolarizing_error_1q, ['rx', 'rz', 'sx', 'x', 'h'])\n",
    "\n",
    "# Apply depolarizing noise to two-qubit gates\n",
    "noise_dep.add_all_qubit_quantum_error(depolarizing_error_2q, ['cx', 'cz', 'swap'])\n",
    "\n",
    "sim_dep = AerSimulator(noise_model=noise_dep)\n",
    "\n",
    "algorithm_globals.random_seed = 10598\n",
    "\n",
    "for bootqa_program in bootqa_programs:\n",
    "    qaoa_mes = QAOA(sampler=BackendSampler(backend=sim_dep), optimizer=COBYLA(100), reps=bootqa_programs_rep_values[bootqa_program])\n",
    "    qaoa = MinimumEigenOptimizer(qaoa_mes)  # using QAOA\n",
    "    data = get_data(bootqa_program)\n",
    "    # Total suite metrics\n",
    "    if bootqa_program != \"elevator2\" and bootqa_program != \"elevator\":\n",
    "        test_cases_costs = data[\"time\"].tolist()\n",
    "    else:\n",
    "        test_cases_costs = data[\"cost\"].tolist()\n",
    "    test_cases_rates = None\n",
    "    test_cases_pcount = None\n",
    "    test_cases_dist = None\n",
    "    if bootqa_program != \"elevator2\":\n",
    "        test_cases_rates = data[\"input_div\"].tolist() if bootqa_program == \"elevator\" else data[\"rate\"].tolist()\n",
    "    else:\n",
    "        test_cases_pcount = data[\"pcount\"].tolist()\n",
    "        test_cases_dist = data[\"dist\"].tolist()\n",
    "    \n",
    "    final_test_suite_costs = []\n",
    "    final_effectivenesses = []\n",
    "    final_pcounts = []\n",
    "    final_dists = []\n",
    "    for i in range(experiments):\n",
    "        final_selected_cases = []\n",
    "        cluster_number = 0\n",
    "        for cluster_id in bootqa_clusters[bootqa_program]:\n",
    "            print(\"Cluster: \" + str(bootqa_clusters[bootqa_program][cluster_id]))\n",
    "            linear_terms = None\n",
    "            if bootqa_program != \"elevator2\":\n",
    "                linear_terms = make_linear_terms_bootqa(bootqa_clusters[bootqa_program][cluster_id], test_cases_costs, test_cases_rates, bootqa_alphas[bootqa_program])\n",
    "            else:\n",
    "                linear_terms = make_linear_terms_bootqa2(bootqa_clusters[bootqa_program][cluster_id], test_cases_costs,test_cases_pcount, test_cases_dist, bootqa_alphas[bootqa_program])\n",
    "            linear_qubo = create_linear_qubo(linear_terms)\n",
    "            print(\"Linear QUBO: \" + str(linear_qubo))\n",
    "            #for each iteration get the result\n",
    "            s = time.time()\n",
    "            qaoa_result = qaoa.solve(linear_qubo)\n",
    "            e = time.time()\n",
    "            print(\"Program: \" + str(bootqa_program) + \". It: \" + str(i))\n",
    "            print(\"QAOA Result: \" + str(qaoa_result))\n",
    "            run_times_dictionary[bootqa_program].append((e-s)*1000)\n",
    "            \n",
    "            variable_values = qaoa_result.x\n",
    "            indexes_selected_cases = [index for index, value in enumerate(variable_values) if value == 1]\n",
    "            print(\"Indexes of selected tests to convert. \" + str(indexes_selected_cases))\n",
    "            selected_tests = []\n",
    "            for index in indexes_selected_cases:\n",
    "                selected_tests.append(bootqa_clusters[bootqa_program][cluster_id][index])\n",
    "            print(\"Selected tests: \" + str(selected_tests))\n",
    "            for selected_test in selected_tests:\n",
    "                if selected_test not in final_test_suite_costs:\n",
    "                    final_selected_cases.append(selected_test)\n",
    "            \n",
    "        #compute the final test suite cost\n",
    "        final_test_suite_cost = 0\n",
    "        for selected_test_case in final_selected_cases:\n",
    "            final_test_suite_cost += test_cases_costs[selected_test_case]\n",
    "        final_test_suite_costs.append(final_test_suite_cost)\n",
    "            \n",
    "        #compute the total failure rate\n",
    "        if bootqa_program != \"elevator2\":\n",
    "            final_effectiveness = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_effectiveness += test_cases_rates[selected_test_case]\n",
    "            final_effectivenesses.append(final_effectiveness)\n",
    "        else:\n",
    "            final_pcount = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_pcount += test_cases_pcount[selected_test_case]\n",
    "            final_pcounts.append(final_pcount)\n",
    "            \n",
    "            final_dist = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_dist += test_cases_dist[selected_test_case]\n",
    "            final_dists.append(final_dist)\n",
    "    \n",
    "    print(\"Final Test Suite: \" + str(final_selected_cases))\n",
    "    #compute the qpu access times\n",
    "    qpu_run_times_without_zeros = []\n",
    "    for access_time in run_times_dictionary[bootqa_program]:\n",
    "      if access_time != 0:\n",
    "        qpu_run_times_without_zeros.append(access_time)\n",
    "    lower_bound, upper_bound = bootstrap_confidence_interval(qpu_run_times_without_zeros, 1000, 0.95)\n",
    "    for i in range(len(run_times_dictionary[bootqa_program])):\n",
    "      if run_times_dictionary[bootqa_program][i] == 0:\n",
    "          run_times_dictionary[bootqa_program][i] = upper_bound\n",
    "    average_qpu_access_time = statistics.mean(run_times_dictionary[bootqa_program]) \n",
    "    \n",
    "    if bootqa_program == \"elevator2\":\n",
    "        var_names = [\"final_test_suite_costs\", \"final_pcounts\", \"final_dists\",\n",
    "                 \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                 \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_pcounts, final_dists, average_qpu_access_time, \n",
    "                  statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                  lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "    else:\n",
    "        var_names = [\"final_test_suite_costs\", \"final_effectivenesses\",\n",
    "                     \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                     \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_effectivenesses, average_qpu_access_time, \n",
    "                  statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                  lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    output_dir = \"results/selectqaoa/depolarizing_sim/02\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Path to save the file\n",
    "    file_path = os.path.join(output_dir, f\"{bootqa_program}.csv\")\n",
    "    \n",
    "    # Writing results to the file\n",
    "    with open(file_path, \"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(var_names)\n",
    "        writer.writerow(values)\n",
    "    print(f\"Results saved to {file_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3831e7f5a8bae7e2",
   "metadata": {},
   "source": [
    "bootqa_alphas = {\"gsdtsr\": 0.2,\"paintcontrol\": 0.80, \"iofrol\": 0.82, \"elevator\": 0.50, \"elevator2\": 2.9}\n",
    "run_times_dictionary = {\"gsdtsr\": [],\"paintcontrol\": [], \"iofrol\": [], \"elevator\": [], \"elevator2\": []}\n",
    "\n",
    "noise_dep = NoiseModel()\n",
    "\n",
    "# Define a 5% depolarizing error for single-qubit and two-qubit gates\n",
    "depolarizing_error_1q = depolarizing_error(0.05, 1)  # 5% depolarizing error for single-qubit gates\n",
    "depolarizing_error_2q = depolarizing_error(0.05, 2)  # 5% depolarizing error for two-qubit gates\n",
    "\n",
    "# Apply depolarizing noise to single-qubit gates\n",
    "noise_dep.add_all_qubit_quantum_error(depolarizing_error_1q, ['rx', 'rz', 'sx', 'x', 'h'])\n",
    "\n",
    "# Apply depolarizing noise to two-qubit gates\n",
    "noise_dep.add_all_qubit_quantum_error(depolarizing_error_2q, ['cx', 'cz', 'swap'])\n",
    "\n",
    "sim_dep = AerSimulator(noise_model=noise_dep)\n",
    "\n",
    "algorithm_globals.random_seed = 10598\n",
    "\n",
    "for bootqa_program in bootqa_programs:\n",
    "    qaoa_mes = QAOA(sampler=BackendSampler(backend=sim_dep), optimizer=COBYLA(100), reps=bootqa_programs_rep_values[bootqa_program])\n",
    "    qaoa = MinimumEigenOptimizer(qaoa_mes)  # using QAOA\n",
    "    data = get_data(bootqa_program)\n",
    "    # Total suite metrics\n",
    "    if bootqa_program != \"elevator2\" and bootqa_program != \"elevator\":\n",
    "        test_cases_costs = data[\"time\"].tolist()\n",
    "    else:\n",
    "        test_cases_costs = data[\"cost\"].tolist()\n",
    "    test_cases_rates = None\n",
    "    test_cases_pcount = None\n",
    "    test_cases_dist = None\n",
    "    if bootqa_program != \"elevator2\":\n",
    "        test_cases_rates = data[\"input_div\"].tolist() if bootqa_program == \"elevator\" else data[\"rate\"].tolist()\n",
    "    else:\n",
    "        test_cases_pcount = data[\"pcount\"].tolist()\n",
    "        test_cases_dist = data[\"dist\"].tolist()\n",
    "    \n",
    "    final_test_suite_costs = []\n",
    "    final_effectivenesses = []\n",
    "    final_pcounts = []\n",
    "    final_dists = []\n",
    "    for i in range(experiments):\n",
    "        final_selected_cases = []\n",
    "        cluster_number = 0\n",
    "        for cluster_id in bootqa_clusters[bootqa_program]:\n",
    "            print(\"Cluster: \" + str(bootqa_clusters[bootqa_program][cluster_id]))\n",
    "            linear_terms = None\n",
    "            if bootqa_program != \"elevator2\":\n",
    "                linear_terms = make_linear_terms_bootqa(bootqa_clusters[bootqa_program][cluster_id], test_cases_costs, test_cases_rates, bootqa_alphas[bootqa_program])\n",
    "            else:\n",
    "                linear_terms = make_linear_terms_bootqa2(bootqa_clusters[bootqa_program][cluster_id], test_cases_costs,test_cases_pcount, test_cases_dist, bootqa_alphas[bootqa_program])\n",
    "            linear_qubo = create_linear_qubo(linear_terms)\n",
    "            print(\"Linear QUBO: \" + str(linear_qubo))\n",
    "            #for each iteration get the result\n",
    "            s = time.time()\n",
    "            qaoa_result = qaoa.solve(linear_qubo)\n",
    "            e = time.time()\n",
    "            print(\"Program: \" + str(bootqa_program) + \". It: \" + str(i))\n",
    "            print(\"QAOA Result: \" + str(qaoa_result))\n",
    "            run_times_dictionary[bootqa_program].append((e-s)*1000)\n",
    "            \n",
    "            variable_values = qaoa_result.x\n",
    "            indexes_selected_cases = [index for index, value in enumerate(variable_values) if value == 1]\n",
    "            print(\"Indexes of selected tests to convert. \" + str(indexes_selected_cases))\n",
    "            selected_tests = []\n",
    "            for index in indexes_selected_cases:\n",
    "                selected_tests.append(bootqa_clusters[bootqa_program][cluster_id][index])\n",
    "            print(\"Selected tests: \" + str(selected_tests))\n",
    "            for selected_test in selected_tests:\n",
    "                if selected_test not in final_test_suite_costs:\n",
    "                    final_selected_cases.append(selected_test)\n",
    "            \n",
    "        #compute the final test suite cost\n",
    "        final_test_suite_cost = 0\n",
    "        for selected_test_case in final_selected_cases:\n",
    "            final_test_suite_cost += test_cases_costs[selected_test_case]\n",
    "        final_test_suite_costs.append(final_test_suite_cost)\n",
    "            \n",
    "        #compute the total failure rate\n",
    "        if bootqa_program != \"elevator2\":\n",
    "            final_effectiveness = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_effectiveness += test_cases_rates[selected_test_case]\n",
    "            final_effectivenesses.append(final_effectiveness)\n",
    "        else:\n",
    "            final_pcount = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_pcount += test_cases_pcount[selected_test_case]\n",
    "            final_pcounts.append(final_pcount)\n",
    "            \n",
    "            final_dist = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_dist += test_cases_dist[selected_test_case]\n",
    "            final_dists.append(final_dist)\n",
    "    \n",
    "    print(\"Final Test Suite: \" + str(final_selected_cases))\n",
    "    #compute the qpu access times\n",
    "    qpu_run_times_without_zeros = []\n",
    "    for access_time in run_times_dictionary[bootqa_program]:\n",
    "      if access_time != 0:\n",
    "        qpu_run_times_without_zeros.append(access_time)\n",
    "    lower_bound, upper_bound = bootstrap_confidence_interval(qpu_run_times_without_zeros, 1000, 0.95)\n",
    "    for i in range(len(run_times_dictionary[bootqa_program])):\n",
    "      if run_times_dictionary[bootqa_program][i] == 0:\n",
    "          run_times_dictionary[bootqa_program][i] = upper_bound\n",
    "    average_qpu_access_time = statistics.mean(run_times_dictionary[bootqa_program]) \n",
    "    \n",
    "    if bootqa_program == \"elevator2\":\n",
    "        var_names = [\"final_test_suite_costs\", \"final_pcounts\", \"final_dists\",\n",
    "                 \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                 \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_pcounts, final_dists, average_qpu_access_time, \n",
    "                  statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                  lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "    else:\n",
    "        var_names = [\"final_test_suite_costs\", \"final_effectivenesses\",\n",
    "                     \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                     \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_effectivenesses, average_qpu_access_time, \n",
    "                  statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                  lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    output_dir = \"results/selectqaoa/depolarizing_sim/05\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Path to save the file\n",
    "    file_path = os.path.join(output_dir, f\"{bootqa_program}.csv\")\n",
    "    \n",
    "    # Writing results to the file\n",
    "    with open(file_path, \"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(var_names)\n",
    "        writer.writerow(values)\n",
    "    print(f\"Results saved to {file_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Effectiveness Empirical Evaluations\n",
    "\n",
    "Two compare the Single-Objective SelectQAOA with SelectQA, BootQA, and IGDec-QAOA in terms of effectiveness, we compared the solutions by evaluating the **execution times** and **failure rates** of the test suites obtained by the four strategies.\n",
    "\n",
    "We statistically analyzed the results obtained over ten independent executions by applying the **Mann-Whitney U test** with a **significance level** set at **0.05**. The null hypothesis represents a non-relevant difference between the two approaches.\n",
    "\n",
    "In contrast, if the null hypothesis is rejected, the magnitude of the difference is computed using the **Vargha-Delaney effect size** $\\hat{A}_{12}$ (**we reject the null hypothesis for** p-values $< 0.05$.\n"
   ],
   "id": "5424b574a835f2d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def a12effs(lst1,lst2,rev=True):\n",
    "  \"Computing the Vargha-Delaney's A12 Effect Size\"\n",
    "  more = same = 0.0\n",
    "  for x in lst1:\n",
    "    for y in lst2:\n",
    "      if x==y : same += 1\n",
    "      elif rev and x > y : more += 1\n",
    "      elif not rev and x < y : more += 1\n",
    "  return (more + 0.5*same)  / (len(lst1)*len(lst2))\n",
    "\n",
    "def stat_test(app1, app2, alt):\n",
    "    \"\"\"Computing the Mann-Whitney's U test and the Vargha-Delaney's A12 Effect Size\"\"\"\n",
    "    statistic, pvalue = mannwhitneyu(app1, app2, alternative=alt)\n",
    "\n",
    "    # Calculate the A12 effect size using Vargha and Delaney's formula\n",
    "    a12_effect_size = a12effs(app1, app2)\n",
    "\n",
    "    return pvalue, a12_effect_size\n",
    "\n",
    "def complete_plotter(data_name, bootqa_costs, qaoa_costs, qtcs_costs, bootqa_rates, qaoa_rates, qtcs_rates):\n",
    "    \"\"\"This funcrtion plots the resultying subtestsuites of the compared algorithms\"\"\"\n",
    "    # Create a new figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot isolated points for each dataset\n",
    "    plt.scatter(bootqa_costs, bootqa_rates, label='BootQA', marker='o', color='b', s=50)\n",
    "    plt.scatter(qaoa_costs, qaoa_rates, label='SelectQAOA', marker='x', color='g', s=200)\n",
    "    plt.scatter(qtcs_costs, qtcs_rates, label='SelectQA', marker='s', color='r', s=50)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Cost')\n",
    "    plt.ylabel('Rate')\n",
    "    plt.title(f'Cost vs Rate for {data_name}')\n",
    "\n",
    "    # Add a legend\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "id": "702e2261e791a190",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "from matplotlib import cm\n",
    "\n",
    "data_names = [\"gsdtsr\", \"paintcontrol\", \"iofrol\", \"elevator\", \"elevator2\"]\n",
    "selectqaoa_configs = [\"statevector_sim\", \"aer_sim\", \"fake_vigo\", \"depolarizing_sim/01\", \"depolarizing_sim/02\", \"depolarizing_sim/05\"]\n",
    "igdec_qaoa_elevator2_configs = [\"ideal/qaoa_1/elevator_three\", \"noise/qaoa_1/elevator_one\"]\n",
    "igdec_qaoa_configs = [\"ideal\", \"noise\"]\n",
    "\n",
    "def read_field_as_list(filepath, field):\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        return [ast.literal_eval(x) for x in df[field].dropna()]\n",
    "    except Exception as e:\n",
    "        print(f\"Errore leggendo {field} da {filepath}: {e}\")\n",
    "        return []\n",
    "\n",
    "def test_normality(data_lists, label):\n",
    "    for i, lst in enumerate(data_lists):\n",
    "        if len(lst) > 2:\n",
    "            flat = np.hstack(lst) if isinstance(lst[0], list) else lst\n",
    "            stat, p = shapiro(flat)\n",
    "            mean = np.mean(flat)\n",
    "            std = np.std(flat)\n",
    "            print(f\"{label} - Media: {mean:.3f}, Std: {std:.3f} - {'Normale' if p > 0.05 else 'NON normale'} (p={p:.3f})\")\n",
    "        else:\n",
    "            print(f\"{label} - Non testabile, lista troppo corta\")\n",
    "\n",
    "def get_color_map(labels):\n",
    "    unique_labels = list(set(labels))\n",
    "    cmap = cm.get_cmap('tab10', len(unique_labels))\n",
    "    return {label: cmap(i) for i, label in enumerate(unique_labels)}\n",
    "\n",
    "def analyze_and_plot(data_name):\n",
    "    costs_list = []\n",
    "    y_values_list = []\n",
    "    dists_list = []\n",
    "    labels = []\n",
    "\n",
    "    if data_name == \"elevator2\":\n",
    "        print(\"==================\"+data_name+\"==================\")\n",
    "        for config in selectqaoa_configs:\n",
    "            print(\"================== SelectQAOA ==================\")\n",
    "            path = f\"./results/selectqaoa/{config}/{data_name}.csv\"\n",
    "            costs = read_field_as_list(path, \"final_test_suite_costs\")\n",
    "            pcounts = read_field_as_list(path, \"final_pcounts\")\n",
    "            dists = read_field_as_list(path, \"final_dists\")\n",
    "            dists_list.extend(dists)\n",
    "            costs_list.extend(costs)\n",
    "            y_values_list.extend(pcounts)\n",
    "            labels.extend([f\"selectqaoa-{config}\"] * len(costs))\n",
    "            test_normality(costs, f\"{config} costs\")\n",
    "            test_normality(pcounts, f\"{config} pcounts\")\n",
    "            test_normality(dists, f\"{config} dists\")\n",
    "\n",
    "        for config in igdec_qaoa_elevator2_configs:\n",
    "            print(\"================== IgDec_QAOA ==================\")\n",
    "            path = f\"./results/igdec_qaoa/{config}/size_7/10/solution.csv\"\n",
    "            costs = read_field_as_list(path, \"final_test_suite_costs\")\n",
    "            pcounts = read_field_as_list(path, \"final_suite_pcounts\")\n",
    "            dists = read_field_as_list(path, \"final_suite_dists\")\n",
    "            dists_list.extend(dists)\n",
    "            costs_list.extend(costs)\n",
    "            y_values_list.extend(pcounts)\n",
    "            labels.extend([f\"igdecqaoa-{config}\"] * len(costs))\n",
    "            test_normality(costs, f\"{config} costs\")\n",
    "            test_normality(pcounts, f\"{config} suite_pcounts\")\n",
    "            test_normality(dists, f\"{config} suite_dists\")\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 7))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        color_map = get_color_map(labels)\n",
    "        \n",
    "        for cost, pcount, dist, label in zip(costs_list, y_values_list, dists_list, labels):\n",
    "            if len(cost) == len(pcount) == len(dist):\n",
    "                ax.scatter(cost, pcount, dist, color=color_map[label], label=label)\n",
    "        \n",
    "        # Etichette assi\n",
    "        ax.set_xlabel(\"Cost\")\n",
    "        ax.set_ylabel(\"PCount\")\n",
    "        ax.set_zlabel(\"Dist\", labelpad=10)  # Maggiore padding sull'asse Z\n",
    "        \n",
    "        # Migliora visibilità della legenda\n",
    "        handles, legend_labels = ax.get_legend_handles_labels()\n",
    "        unique = dict(zip(legend_labels, handles))\n",
    "        ax.legend(unique.values(), unique.keys(), fontsize='small', loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n",
    "        \n",
    "        plt.title(\"3D Plot - elevator2\", pad=20)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(\"==================\"+data_name+\"==================\")\n",
    "        for config in selectqaoa_configs:\n",
    "            print(\"================== SelectQAOA ==================\")\n",
    "            path = f\"./results/selectqaoa/{config}/{data_name}.csv\"\n",
    "            costs = read_field_as_list(path, \"final_test_suite_costs\")\n",
    "            if data_name == \"elevator\":\n",
    "                effect = read_field_as_list(path, \"final_effectivenesses\")\n",
    "            else:\n",
    "                effect = read_field_as_list(path, \"final_effectivenesses\")\n",
    "            costs_list.extend(costs)\n",
    "            y_values_list.extend(effect)\n",
    "            labels.extend([f\"selectqaoa-{config}\"] * len(costs))\n",
    "            test_normality(costs, f\"{config} costs\")\n",
    "            test_normality(effect, f\"{config} effectiveness\")\n",
    "\n",
    "        for config in igdec_qaoa_configs:\n",
    "            print(\"================== IgDec_QAOA ==================\")\n",
    "            if data_name == \"elevator\":\n",
    "                subpath = f\"{config}/qaoa_1/elevator_two\"\n",
    "                field = \"final_suite_input_divs\"\n",
    "            elif data_name == \"iofrol\":\n",
    "                subpath = f\"{config}/qaoa_1/iofrol\"\n",
    "                field = \"final_failure_rates\"\n",
    "            else:\n",
    "                subpath = f\"{config}/qaoa_1/{data_name}\"\n",
    "                field = \"final_failure_rates\"\n",
    "            path = f\"./results/igdec_qaoa/{subpath}/size_7/10/solution.csv\"\n",
    "            costs = read_field_as_list(path, \"final_test_suite_costs\")\n",
    "            effectiveness = read_field_as_list(path, field)\n",
    "            costs_list.extend(costs)\n",
    "            y_values_list.extend(effectiveness)\n",
    "            labels.extend([f\"igdecqaoa-{config}\"] * len(costs))\n",
    "            test_normality(costs, f\"{config} costs\")\n",
    "            test_normality(effectiveness, f\"{config} {field}\")\n",
    "\n",
    "        if data_name not in [\"elevator2\", \"elevator\", \"iofrol\"]:\n",
    "            for algo in [\"selectqa\", \"bootqa\"]:\n",
    "                print(\"==================\"+ algo +\"==================\")\n",
    "                path = f\"./results/{algo}/{data_name}.csv\"\n",
    "                costs = read_field_as_list(path, \"final_test_suite_costs\")\n",
    "                rates = read_field_as_list(path, \"final_failure_rates\")\n",
    "                costs_list.extend(costs)\n",
    "                y_values_list.extend(rates)\n",
    "                labels.extend([f\"{algo}\"] * len(costs))\n",
    "                test_normality(costs, f\"{algo} costs\")\n",
    "                test_normality(rates, f\"{algo} failure_rates\")\n",
    "\n",
    "        # Plot 2D\n",
    "        plt.figure()\n",
    "        color_map = get_color_map(labels)\n",
    "        \n",
    "        for cost, yval, label in zip(costs_list, y_values_list, labels):\n",
    "            color = color_map[label]\n",
    "            if len(cost) == len(yval):\n",
    "                plt.scatter(cost, yval, color=color, label=label)\n",
    "        \n",
    "        # Elimina label duplicate nella legenda\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        unique = dict(zip(labels, handles))\n",
    "        plt.legend(unique.values(), unique.keys(), fontsize='small')\n",
    "        \n",
    "        plt.title(f\"2D Plot - {data_name}\")\n",
    "        plt.xlabel(\"Cost\")\n",
    "        plt.ylabel(\"Effectiveness or Rate\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Esempio d'uso\n",
    "for data_name in data_names:\n",
    "    analyze_and_plot(data_name)\n"
   ],
   "id": "a0461055d6116517",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Efficiency Empirical Evaluation\n",
    "To compare the efficiency of **SelectQAOA**, **IGDec-QAOA**, **BootQA**, and **SelectQA**, we analyzed their **total run times**.\n",
    "\n",
    "- **BootQA** executes a **local decomposition** through **bootstrap sampling** (in this work, executed on a **MacBook Air with an M1 Chip and 16GB of RAM**) and directly runs the **Advanced System QPU**.\n",
    "- **SelectQA** relies solely on the **hybrid_binary_quadratic_model_version2** to handle highly complex optimization problems.\n",
    "\n",
    "For **BootQA**, we considered the **total run time** as the sum of the **bootstrap sampling process** and **Advanced System QPU execution time**. \n",
    "\n",
    "For **SelectQA**, we computed the **total run time** of the **hybrid_binary_quadratic_model_version2**.\n",
    "\n",
    "**IGDec-QAOA** was runned on a noisy simulator based of **ibm_brisbane**.\n",
    "\n",
    "The total run time of **SelectQAOA** was computed by summing the **decomposition** and **execution times** of its run on a **Barbora (IT4Innovations supercomputer)** featuring 8 cores of the **Intel Xeon Gold 6240 CPU** with **2.60 GHz**. \n",
    "\n",
    "The **empirical reliability** of the findings was statistically validated by analyzing the distribution of the total run times obtained over ten independent runs by each algorithm, using the **Mann-Whitney U test**. The magnitude of the differences between the sequences was quantified using the **Vargha-Delaney effect size** (\\(\\hat{A}_{12}\\))."
   ],
   "id": "90207514746f8ce5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from scipy.stats import shapiro\n",
    "import numpy as np\n",
    "\n",
    "# Dataset e configurazioni\n",
    "datasets = [\"gsdtsr\", \"paintcontrol\", \"iofrol\", \"elevator\", \"elevator2\"]\n",
    "selectqaoa_configs = [\n",
    "    \"statevector_sim\", \"aer_sim\", \"fake_vigo\",\n",
    "    \"depolarizing_sim/01\", \"depolarizing_sim/02\", \"depolarizing_sim/05\"\n",
    "]\n",
    "igdec_qaoa_configs = [\"ideal/qaoa_1/\", \"noise/qaoa_1/\"]\n",
    "igdec_qaoa_elevator2_configs = [\n",
    "    \"ideal/qaoa_1/elevator_three\", \"noise/qaoa_1/elevator_one\"\n",
    "]\n",
    "\n",
    "# Funzione per leggere un campo da CSV e convertirlo in lista di float\n",
    "def read_list_field(path, field):\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        value = df[field].iloc[0]\n",
    "        return eval(value) if isinstance(value, str) else [value]\n",
    "    except Exception as e:\n",
    "        print(f\"Errore leggendo {field} da {path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Funzione per fare il test di Shapiro e stampare statistiche\n",
    "def analyze_and_report_normality(data_lists, labels, dataset):\n",
    "    print(f\"\\n=== Dataset: {dataset} ===\")\n",
    "    for data, label in zip(data_lists, labels):\n",
    "        if len(data) < 3:\n",
    "            print(f\"{label}: Dati insufficienti per il test di Shapiro.\")\n",
    "            continue\n",
    "        try:\n",
    "            stat, p = shapiro(data)\n",
    "            mean = np.mean(data)\n",
    "            std = np.std(data, ddof=1)\n",
    "            status = \"Normale\" if p > 0.05 else \"Non normale\"\n",
    "            print(f\"{label}:\\n  Media = {mean:.2f} ms\\n  Deviazione standard = {std:.2f} ms\\n  Normalità = {status} (p = {p:.4f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"{label}: Errore nel test di Shapiro: {e}\")\n",
    "\n",
    "# Loop principale\n",
    "for dataset in datasets:\n",
    "    data_lists = []\n",
    "    labels = []\n",
    "\n",
    "    # Lettura dati SelectQAOA\n",
    "    for config in selectqaoa_configs:\n",
    "        path = f\"./results/selectqaoa/{config}/{dataset}.csv\"\n",
    "        times = read_list_field(path, \"qpu_run_times(ms)\")\n",
    "        if times:\n",
    "            data_lists.append(times)\n",
    "            labels.append(f\"SelectQAOA - {config}\")\n",
    "\n",
    "    # Lettura dati IgDec_QAOA\n",
    "    if dataset == \"elevator2\":\n",
    "        for config in igdec_qaoa_elevator2_configs:\n",
    "            path = f\"./results/igdec_qaoa/{config}/size_7/10/solution.csv\"\n",
    "            times = read_list_field(path, \"execution_times\")\n",
    "            if times:\n",
    "                data_lists.append(times)\n",
    "                labels.append(f\"IgDec_QAOA - {config}\")\n",
    "    elif dataset in [\"elevator\", \"iofrol\", \"gsdtsr\", \"paintcontrol\"]:\n",
    "        for config in igdec_qaoa_configs:\n",
    "            if dataset == \"elevator\":\n",
    "                path = f\"./results/igdec_qaoa/{config}elevator_two/size_7/10/solution.csv\"\n",
    "            else:\n",
    "                path = f\"./results/igdec_qaoa/{config}{dataset}/size_7/10/solution.csv\"\n",
    "            times = read_list_field(path, \"execution_times\")\n",
    "            if times:\n",
    "                data_lists.append(times)\n",
    "                labels.append(f\"IgDec_QAOA - {config.strip('/')}\")\n",
    "\n",
    "    # Dati extra solo per gsdtsr e paintcontrol\n",
    "    if dataset in [\"gsdtsr\", \"paintcontrol\"]:\n",
    "        # SelectQA\n",
    "        path = f\"./results/selectqa/{dataset}.csv\"\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "            val = float(df[\"average_qpu_access_time(ms)\"].iloc[0])\n",
    "            data_lists.append([val])\n",
    "            labels.append(\"SelectQA - average_qpu_access_time(ms)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Errore leggendo SelectQA per {dataset}: {e}\")\n",
    "\n",
    "        # BootQA\n",
    "        path = f\"./results/bootqa/{dataset}.csv\"\n",
    "        times = read_list_field(path, \"exectution_times(ms)\")\n",
    "        if times:\n",
    "            data_lists.append(times)\n",
    "            labels.append(\"BootQA - exectution_times(ms)\")\n",
    "\n",
    "    # Test e statistiche\n",
    "    analyze_and_report_normality(data_lists, labels, dataset)\n"
   ],
   "id": "b519ad76e01eb05e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "89c93af6ab4c2b43",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
